"""
çŸ¥è¯†å›¾è°±èåˆæ¨¡å—

å°†æ–‡æœ¬çŸ¥è¯†å›¾è°±ä¸å›¾åƒçŸ¥è¯†å›¾è°±è¿›è¡Œå¯¹é½å’Œèåˆã€‚

ä¸»è¦æµç¨‹:
1. å›¾åƒå®ä½“å¯¹é½ - å°†å›¾åƒå®ä½“ä¸æ–‡æœ¬å®ä½“åŒ¹é…
2. å›¾åƒçŸ¥è¯†å›¾è°±å¢å¼º - ä½¿ç”¨æ–‡æœ¬ä¿¡æ¯å¢å¼ºå›¾åƒå›¾è°±
3. å›¾è°±èåˆ - åˆå¹¶å›¾åƒå’Œæ–‡æœ¬çŸ¥è¯†å›¾è°±
"""
import math
import os
# è®¾ç½®TOKENIZERS_PARALLELISMç¯å¢ƒå˜é‡ä»¥ç¦ç”¨è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import base64
import xml.etree.ElementTree as ET

import networkx as nx
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from tqdm import tqdm

from ..core.base import logger, load_json, ensure_quoted
from ..core.prompt import PROMPTS, GRAPH_FIELD_SEP
from ..llm import get_llm_response, get_mmllm_response, normalize_to_json, normalize_to_json_list
from ..parameter import EMBED_MODEL, WORKING_DIR
from .. import parameter


# ============================================================================
# æ•°æ®åŠ è½½å‡½æ•°
# ============================================================================

def _get_json_path(filename: str) -> str:
    """è·å–å·¥ä½œç›®å½•ä¸‹çš„JSONæ–‡ä»¶è·¯å¾„"""
    return os.path.join(parameter.WORKING_DIR, filename)


def get_image_data() -> dict:
    """åŠ è½½å›¾åƒæ•°æ®"""
    return load_json(_get_json_path("kv_store_image_data.json")) or {}


def get_chunk_knowledge_graph() -> dict:
    """åŠ è½½æ–‡æœ¬å—çŸ¥è¯†å›¾è°±"""
    return load_json(_get_json_path("kv_store_chunk_knowledge_graph.json")) or {}


def get_text_chunks() -> dict:
    """åŠ è½½æ–‡æœ¬å—"""
    return load_json(_get_json_path("kv_store_text_chunks.json")) or {}


# ============================================================================
# ä¸Šä¸‹æ–‡è·å–å‡½æ•°
# ============================================================================

def get_nearby_chunks(data: dict, index: int) -> list[str]:
    """è·å–æŒ‡å®šç´¢å¼•é™„è¿‘çš„æ–‡æœ¬å—å†…å®¹"""
    indices = range(max(0, index - 1), min(len(data), index + 2))
    return [
        v.get("content") for v in data.values()
        if v.get("chunk_order_index") in indices
    ]


def get_nearby_entities(data: dict, index: int) -> list[dict]:
    """è·å–æŒ‡å®šç´¢å¼•é™„è¿‘çš„å®ä½“åˆ—è¡¨"""
    indices = range(max(0, index - 1), min(len(data), index + 2))
    entities = []
    for i in indices:
        chunk_data = data.get(str(i), {})
        for entity in chunk_data.get("entities", []):
            # å¤åˆ¶å¹¶ç§»é™¤source_id
            entity_copy = {k: v for k, v in entity.items() if k != "source_id"}
            entities.append(entity_copy)
    return entities


def get_nearby_relationships(data: dict, index: int) -> list[dict]:
    """è·å–æŒ‡å®šç´¢å¼•é™„è¿‘çš„å…³ç³»åˆ—è¡¨"""
    indices = range(max(0, index - 1), min(len(data), index + 2))
    relationships = []
    for i in indices:
        chunk_data = data.get(str(i), {})
        for rel in chunk_data.get("relationships", []):
            rel_copy = {k: v for k, v in rel.items() if k != "source_id"}
            relationships.append(rel_copy)
    return relationships


def _sanitize_embeddings(embeddings: np.ndarray) -> np.ndarray:
    """
    æ¸…æ´—å’Œæ ‡å‡†åŒ–åµŒå…¥çŸ©é˜µï¼Œé˜²æ­¢sklearnè®¡ç®—é”™è¯¯ã€‚
    1. æ›¿æ¢éæ³•å€¼ (NaN, Inf)
    2. å½’ä¸€åŒ– (Handling zero vectors)
    """
    if embeddings.size == 0:
        return embeddings

    # 1. åˆå§‹æ›¿æ¢ï¼šæ›¿æ¢ NaN å’Œ Inf
    embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=1e6, neginf=-1e6)
    
    # 2. æ•°å€¼è£å‰ªï¼šé™åˆ¶åµŒå…¥å€¼åœ¨åˆç†èŒƒå›´å†…ï¼Œé¿å…æå¤§å€¼å¯¼è‡´æº¢å‡º
    embeddings = np.clip(embeddings, -10.0, 10.0)  # æ ¹æ®åµŒå…¥æ¨¡å‹çš„ç‰¹æ€§è°ƒæ•´é˜ˆå€¼
    
    # 3. æ£€æŸ¥å¹¶å¤„ç†é›¶å‘é‡
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    zero_mask = (norms < 1e-8)
    
    if zero_mask.any():
        logger.warning(f"âš ï¸ å‘ç° {zero_mask.sum()} ä¸ªé›¶/æ¥è¿‘é›¶å‘é‡åµŒå…¥")
        # èµ‹äºˆé›¶å‘é‡ä¸€ä¸ªå¾®å°çš„éšæœºå€¼ä»¥é¿å…é™¤é›¶
        embeddings[zero_mask.flatten()] = np.random.normal(0, 1e-6, 
            size=(zero_mask.sum(), embeddings.shape[1]))
        # é‡æ–°è®¡ç®—èŒƒæ•°
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    
    # 4. å®‰å…¨å½’ä¸€åŒ–
    # ä½¿ç”¨ sklearn çš„ normalize æ–¹æ³•å¯ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
    normalized = embeddings / np.maximum(norms, 1e-8)
    
    # 5. æœ€ç»ˆéªŒè¯ (å¹¶å¼ºåˆ¶è½¬æ¢ä¸º float64 ä»¥æé«˜ matmul ç²¾åº¦)
    normalized = np.nan_to_num(normalized, nan=0.0, posinf=1.0, neginf=-1.0)
    return normalized.astype(np.float64)

# ============================================================================
# è°±èšç±»æ ¸å¿ƒå‡½æ•°
# ============================================================================

def _compute_spectral_labels(
    embeddings: np.ndarray,
    entity_names: list[str],
    relationships: list[dict]
) -> list[int]:
    """
    ä½¿ç”¨è°±èšç±»è®¡ç®—å®ä½“æ ‡ç­¾ã€‚
    
    Args:
        embeddings: å®ä½“åµŒå…¥çŸ©é˜µ
        entity_names: å®ä½“åç§°åˆ—è¡¨
        relationships: å…³ç³»åˆ—è¡¨ï¼ˆç”¨äºè°ƒæ•´ç›¸ä¼¼åº¦çŸ©é˜µï¼‰
    
    Returns:
        èšç±»æ ‡ç­¾åˆ—è¡¨
    """
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ (èŒƒå›´ [-1, 1])
    # æ‰‹åŠ¨è®¡ç®— cosine similarity ä»¥é¿å¼€ sklearn çš„ bug
    normalized_embeddings = _sanitize_embeddings(embeddings)
    # æ—¢ç„¶å·²ç»å½’ä¸€åŒ–ï¼Œdot product å°±æ˜¯ cosine similarity
    raw_similarity = np.dot(normalized_embeddings, normalized_embeddings.T)
    
    # 1. æ˜ å°„åˆ° [0, 1] åŒºé—´ï¼Œä¿è¯éè´Ÿæ€§ (Spectral Clustering è¦æ±‚)
    similarity_matrix = (raw_similarity + 1.0) / 2.0
    
    # æ¸…ç†ç›¸ä¼¼åº¦çŸ©é˜µä¸­çš„ NaN å’Œ Inf
    similarity_matrix = np.nan_to_num(similarity_matrix, nan=0.0, posinf=1.0, neginf=0.0)
    
    # æ ¹æ®å…³ç³»æƒé‡è°ƒæ•´ç›¸ä¼¼åº¦
    relationships = sorted(relationships, key=lambda x: x.get('weight', 0), reverse=True)
    for rel in relationships:
        src, tgt = rel.get("src_id"), rel.get("tgt_id")
        if src not in entity_names or tgt not in entity_names:
            continue
        
        weight_raw = rel.get("weight")
        if weight_raw is None:
            continue

        try:
            # 2. æƒé‡é€»è¾‘ï¼šåŠ æ³•å¢å¼º
            # åŸå§‹æƒé‡å‡è®¾ 1-100ï¼Œå°†å…¶æ˜ å°„åˆ° [0, 0.5] çš„ç›¸ä¼¼åº¦å¢ç›Š
            w_val = float(weight_raw)
            boost = min(w_val, 50.0) / 100.0  # max 0.5 boost
        except (ValueError, TypeError):
            continue
        
        if not np.isfinite(boost):
            continue
        
        src_idx = entity_names.index(src)
        tgt_idx = entity_names.index(tgt)
        
        # åŠ æ³•å¢å¼ºï¼šæœ‰å…³ç³»çš„å®ä½“æ›´ç›¸ä¼¼
        similarity_matrix[src_idx, tgt_idx] += boost
        similarity_matrix[tgt_idx, src_idx] += boost
    
    # å†æ¬¡æˆªæ–­åˆ° [0, 1] (å°½ç®¡åŠ æ³•å¯èƒ½ç¨å¾®æº¢å‡ºï¼Œä½†é™åˆ¶ä¸€ä¸‹æ›´å®‰å…¨ï¼Œæˆ–è€…å…è®¸ >1 ä¹Ÿå¯ä»¥)
    similarity_matrix = np.clip(similarity_matrix, 0.0, 1.5)
    
    # å¼ºåˆ¶å¯¹ç§°åŒ–ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
    similarity_matrix = (similarity_matrix + similarity_matrix.T) / 2.0
    
    # è®¡ç®—æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
    degree_matrix = np.diag(np.sum(similarity_matrix, axis=1))
    degree_matrix = np.clip(degree_matrix, 0, 1e6)  # é™åˆ¶åº¦çŸ©é˜µçš„æœ€å¤§å€¼
    laplacian_matrix = degree_matrix - similarity_matrix
    laplacian_matrix = np.nan_to_num(laplacian_matrix, nan=0.0, posinf=0.0, neginf=0.0)
    
    # å¯¹ç§°åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼ˆè¿›ä¸€æ­¥ç¡®ä¿ç¨³å®šæ€§ï¼‰
    laplacian_matrix = (laplacian_matrix + laplacian_matrix.T) / 2.0
    
    # ç‰¹å¾åˆ†è§£ (ä½¿ç”¨ eigh é’ˆå¯¹å¯¹ç§°/HermitiançŸ©é˜µ)
    try:
        eigvals, eigvecs = np.linalg.eigh(laplacian_matrix)
    except np.linalg.LinAlgError:
        logger.warning("âš ï¸ æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç‰¹å¾åˆ†è§£å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šeig")
        eigvals, eigvecs = np.linalg.eig(laplacian_matrix)
        
    # å–å®éƒ¨å¹¶æ’åºï¼ˆä»å°åˆ°å¤§ï¼‰
    eigvals = np.real(eigvals)
    eigvecs = np.real(eigvecs)
    
    # æ’åº
    idx = np.argsort(eigvals)
    eigvals = eigvals[idx]
    eigvecs = eigvecs[:, idx]
    
    # é€‰æ‹©å‰kä¸ªç‰¹å¾å‘é‡ (è·³è¿‡ç¬¬ä¸€ä¸ªå¸¸æ•°ç‰¹å¾å‘é‡ if connected)
    k = max(2, math.ceil(math.sqrt(len(entity_names))))
    eigvecs_selected = eigvecs[:, :k]
    eigvecs_selected = _sanitize_embeddings(eigvecs_selected)
    
    # DBSCAN èšç±»
    min_samples = max(1, math.ceil(len(entity_names) / 10))
    dbscan = DBSCAN(eps=0.5, min_samples=min_samples)
    return dbscan.fit_predict(eigvecs_selected).tolist()

def _classify_by_nearest_neighbor(
    input_embeddings: np.ndarray,
    reference_embeddings: np.ndarray,
    labels: list[int],
    n_neighbors: int = 1
) -> list[int]:
    """ä½¿ç”¨æœ€è¿‘é‚»å°†è¾“å…¥åµŒå…¥åˆ†ç±»åˆ°å·²æœ‰æ ‡ç­¾"""
    # ç¡®ä¿è¾“å…¥å’Œå‚è€ƒåµŒå…¥éƒ½ç»è¿‡æ¸…æ´—
    input_embeddings = _sanitize_embeddings(input_embeddings)
    reference_embeddings = _sanitize_embeddings(reference_embeddings)
        
    # æ‰‹åŠ¨å®ç°æœ€è¿‘é‚»æœç´¢ï¼Œé¿å¼€ sklearn çš„ bug
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦: input (M, D) @ ref.T (D, N) -> (M, N)
    sims = np.dot(input_embeddings, reference_embeddings.T)
    
    # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨ [-1, 1] èŒƒå›´å†… (å¤„ç†æµ®ç‚¹è¯¯å·®)
    sims = np.clip(sims, -1.0, 1.0)
    
    result_labels = []
    # å¯¹æ¯ä¸ªè¾“å…¥æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ top-k
    for i in range(len(input_embeddings)):
        # argsort è¿”å›ä»å°åˆ°å¤§çš„ç´¢å¼•ï¼Œå–æœ€å n_neighbors ä¸ªï¼Œç„¶ååè½¬å¾—åˆ°ä»å¤§åˆ°å°
        top_indices = np.argsort(sims[i])[-n_neighbors:][::-1]
        
        # è¿™é‡Œçš„é€»è¾‘æ˜¯ç®€å•çš„ KNN åˆ†ç±»ï¼ŸåŸå§‹ä»£ç ä¼¼ä¹åªå–äº†ç¬¬ä¸€ä¸ªé‚»å±…çš„æ ‡ç­¾ï¼Ÿ
        # åŸå§‹ä»£ç : result_labels.append(labels[indices[0][0]])
        # è¿™é‡Œ indices[0][0] æ˜¯æœ€è¿‘çš„ä¸€ä¸ªé‚»å±…
        
        nearest_idx = top_indices[0]
        result_labels.append(labels[nearest_idx])
        
    return result_labels


# ============================================================================
# å®ä½“å¯¹é½å‡½æ•°
# ============================================================================

def _prepare_and_cluster_entities(
    nearby_text_entities: list[dict],
    nearby_relationships: list[dict]
) -> tuple[np.ndarray, list[int]]:
    """
    Prepare embeddings and compute cluster labels for text entities.
    Returns: (embeddings, labels)
    """
    if not nearby_text_entities:
        return np.array([]), []
        
    descriptions = [e["description"] for e in nearby_text_entities]
    entity_names = [e["entity_name"] for e in nearby_text_entities]
    
    embeddings = EMBED_MODEL.encode(descriptions)
    embeddings = np.array(embeddings)
    embeddings = _sanitize_embeddings(embeddings)
    
    labels = _compute_spectral_labels(embeddings, entity_names, nearby_relationships)
    return embeddings, labels


def align_single_image_entity(img_entity_name: str, text_chunks: dict) -> dict:
    """å¯¹é½å•ä¸ªå›¾åƒå®ä½“"""
    image_data = get_image_data()
    entity_info = image_data.get(img_entity_name, {})
    
    image_path = entity_info.get("image_path")
    description = entity_info.get("description", "")
    chunk_index = entity_info.get("chunk_order_index", 0)
    
    nearby_chunks = get_nearby_chunks(text_chunks, chunk_index)
    entity_types = [t.upper() for t in PROMPTS["DEFAULT_ENTITY_TYPES"]]
    
    with open(image_path, "rb") as f:
        img_base = base64.b64encode(f.read()).decode("utf-8")
    
    prompt = PROMPTS["image_entity_alignment_user"].format(
        entity_type=entity_types,
        img_entity=img_entity_name,
        img_entity_description=description,
        chunk_text=nearby_chunks
    )
    result = get_mmllm_response(prompt, PROMPTS["image_entity_alignment_system"], img_base)
    return normalize_to_json(result)


def get_possible_entities_image_clustering(
    image_entity_description: str,
    nearby_text_entities: list[dict],
    nearby_relationships: list[dict]
) -> list[dict]:
    """é€šè¿‡èšç±»æ‰¾åˆ°å¯èƒ½åŒ¹é…çš„æ–‡æœ¬å®ä½“"""
    if not nearby_text_entities:
        return []
    
    # è·å–èšç±»æ ‡ç­¾å’ŒåµŒå…¥
    embeddings, labels = _prepare_and_cluster_entities(nearby_text_entities, nearby_relationships)
    if embeddings.size == 0:
        return []
    
    # åˆ†ç±»å›¾åƒå®ä½“
    input_embedding = EMBED_MODEL.encode([image_entity_description])
    target_label = _classify_by_nearest_neighbor(input_embedding, embeddings, labels, n_neighbors=3)[0]
    
    # è¿”å›åŒä¸€ç±»åˆ«çš„å®ä½“
    return [e for e, label in zip(nearby_text_entities, labels) if label == target_label]


def get_possible_entities_text_clustering(
    filtered_image_entities: list[dict],
    nearby_text_entities: list[dict],
    nearby_relationships: list[dict]
) -> tuple[list[dict], list[dict]]:
    """
    å¯¹æ–‡æœ¬å®ä½“è¿›è¡Œèšç±»ï¼Œå¹¶å°†å›¾åƒå®ä½“åˆ†é…åˆ°å¯¹åº”ç±»åˆ«ã€‚
    
    Returns:
        (image_entity_with_labels, text_clustering_results)
    """
    if not nearby_text_entities:
        return [], []
    
    # è·å–èšç±»æ ‡ç­¾å’ŒåµŒå…¥
    embeddings, labels = _prepare_and_cluster_entities(nearby_text_entities, nearby_relationships)
    if embeddings.size == 0:
        return [], []
    
    # åˆ†ç±»å›¾åƒå®ä½“
    image_entity_with_labels = []
    if filtered_image_entities:
        img_embeddings = EMBED_MODEL.encode([e["description"] for e in filtered_image_entities])
        img_labels = _classify_by_nearest_neighbor(img_embeddings, embeddings, labels)
        
        for entity, label in zip(filtered_image_entities, img_labels):
            image_entity_with_labels.append({
                "entity_name": entity["entity_name"],
                "label": label,
                "description": entity["description"],
                "entity_type": entity.get("entity_type", "image")
            })
    
    # ç”Ÿæˆèšç±»ç»“æœ
    text_clustering_results = []
    for label in set(labels):
        cluster_entities = [
            {
                "entity_name": e["entity_name"],
                "entity_type": e["entity_type"],
                "description": e["description"]
            }
            for e, l in zip(nearby_text_entities, labels) if l == label
        ]
        text_clustering_results.append({"label": label, "entities": cluster_entities})
    
    return image_entity_with_labels, text_clustering_results


def judge_image_entity_alignment(
    image_entity_name: str,
    image_entity_description: str,
    possible_entities: list[dict],
    nearby_chunks: list[str]
) -> str:
    """ä½¿ç”¨LLMåˆ¤æ–­å›¾åƒå®ä½“ä¸æ–‡æœ¬å®ä½“çš„åŒ¹é…"""
    prompt = PROMPTS["image_entity_judgement_user"].format(
        img_entity=image_entity_name,
        img_entity_description=image_entity_description,
        possible_matched_entities=possible_entities,
        chunk_text=nearby_chunks
    )
    matched_entity_name = get_llm_response(prompt, PROMPTS["image_entity_judgement_system"])
    return matched_entity_name


def judge_text_entity_alignment_clustering(
    image_entity_with_labels: list[dict],
    text_clustering_results: list[dict]
) -> list[dict]:
    """ä½¿ç”¨LLMåˆ¤æ–­å¹¶èåˆå®ä½“"""
    clusters_info = []
    for cluster in text_clustering_results:
        clusters_info.append({
            "label": cluster["label"],
            "text_entities": [
                {
                    "entity_name": entity["entity_name"],
                    "entity_type": entity["entity_type"],
                    "description": entity["description"],
                }
                for entity in cluster["entities"]
            ]
        })

    # æ„å»ºè¾“å…¥ prompt
    prompt_user = f"""
You are tasked with aligning image entities and text entities based on their labels and descriptions. Below are the clusters and the entities they contain.

Clusters information:
{{
    "clusters": [
        {", ".join([f'{{"label": {c["label"]}, "text_entities": {c["text_entities"]}}}' for c in clusters_info])}
    ]
}}

Image entities with labels:
{[
    {
        "entity_name": e["entity_name"],
        "label": e["label"],
        "description": e["description"],
        "entity_type": e["entity_type"]
    }
    for e in image_entity_with_labels
]}

Instruction:
1. For each image entity, look at the corresponding cluster (same label).
2. Compare the description and type of the image entity with the text entities in the same cluster.
3. Identify matching entities between the image entities and text entities within the same cluster (same label).
4. For each match, create a new unified entity by merging the descriptions and including the source entities under "source_image_entities" and "source_text_entities".
5. Output a JSON list where each item represents a merged entity with the following structure:
    {{
        "entity_name": "Newly merged entity name",
        "entity_type": "Type of the merged entity",
        "description": "Merged description of the entity",
        "source_image_entities": ["List of matched image entity names"],
        "source_text_entities": ["List of matched text entity names"]
    }}
Include only one JSON list as the output, strictly following the structure above.
"""
    prompt_system = """You are an AI assistant skilled in aligning entities based on semantic descriptions and cluster information. Use the provided instructions to merge entities accurately."""

    # è°ƒç”¨ LLM è·å–èåˆç»“æœ
    merged_entities = get_llm_response(cur_prompt=prompt_user, system_content=prompt_system)
    normalized_merged_entities = normalize_to_json_list(merged_entities)
    return [
        item for item in normalized_merged_entities 
        if item.get("source_image_entities") and item.get("source_text_entities")
    ]


# ============================================================================
# å›¾åƒå®ä½“æå–
# ============================================================================

def extract_image_entities(img_entity_name: str) -> list[dict]:
    """ä»GraphMLæ–‡ä»¶æå–å›¾åƒå®ä½“"""
    path = os.path.join(
        parameter.WORKING_DIR, 
        f"images/{img_entity_name}/graph_{img_entity_name}_entity_relation.graphml"
    )
    
    if not os.path.exists(path):
        logger.warning(f"âš ï¸  æœªæ‰¾åˆ°GraphMLæ–‡ä»¶: {path}")
        return []
    
    tree = ET.parse(path)
    root = tree.getroot()
    ns = {'graphml': 'http://graphml.graphdrawing.org/xmlns'}
    
    entities = []
    for node in root.findall('graphml:graph/graphml:node', ns):
        entity_name = (node.get('id') or "").strip('"')
        entity_type = "UNKNOWN"
        description = ""
        
        for data in node.findall('graphml:data', ns):
            key = data.get('key')
            text = (data.text or "").strip('"')
            if key == 'd0':
                entity_type = text
            elif key == 'd1':
                description = text
        
        entities.append({
            "entity_name": entity_name,
            "entity_type": entity_type,
            "description": description
        })
    
    return entities


def enhance_image_entities(image_entities: list[dict], nearby_chunks: list[str]) -> list[dict]:
    """ä½¿ç”¨LLMå¢å¼ºå›¾åƒå®ä½“æè¿°"""
    prompt = PROMPTS["enhance_image_entity_user"].format(
        enhanced_image_entity_list=image_entities,
        chunk_text=nearby_chunks
    )
    result = get_llm_response(prompt, PROMPTS["enhance_image_entity_system"])
    return normalize_to_json_list(result)


# ============================================================================
# çŸ¥è¯†å›¾è°±æ“ä½œ
# ============================================================================

def image_knowledge_graph_alignment(image_entity_name: str) -> list[dict]:
    """å¯¹é½å›¾åƒçŸ¥è¯†å›¾è°±ä¸æ–‡æœ¬å®ä½“"""
    image_data = get_image_data()
    chunk_kg = get_chunk_knowledge_graph()
    
    chunk_index = image_data[image_entity_name].get("chunk_order_index", 0)
    
    image_entities = extract_image_entities(image_entity_name)
    filtered = [e for e in image_entities if e['entity_type'] not in ["ORI_IMG", "IMG"]]
    
    nearby_entities = get_nearby_entities(chunk_kg, chunk_index)
    nearby_rels = get_nearby_relationships(chunk_kg, chunk_index)
    
    img_with_labels, text_clusters = get_possible_entities_text_clustering(
        filtered, nearby_entities, nearby_rels
    )
    
    return judge_text_entity_alignment_clustering(img_with_labels, text_clusters)


def enhanced_image_knowledge_graph(aligned_entities: list[dict], image_entity_name: str) -> str:
    """å¢å¼ºå›¾åƒçŸ¥è¯†å›¾è°±"""
    image_data = get_image_data()
    text_chunks = get_text_chunks()
    
    img_kg_path = os.path.join(
        parameter.WORKING_DIR,
        f'images/{image_entity_name}/graph_{image_entity_name}_entity_relation.graphml'
    )
    enhanced_path = os.path.join(
        parameter.WORKING_DIR,
        f'images/{image_entity_name}/enhanced_graph_{image_entity_name}_entity_relation.graphml'
    )
    
    image_entities = extract_image_entities(image_entity_name)
    filtered = [e for e in image_entities if e['entity_type'] not in ["ORI_IMG", "IMG"]]
    
    chunk_index = image_data[image_entity_name].get("chunk_order_index", 0)
    nearby_chunks = get_nearby_chunks(text_chunks, chunk_index)
    
    # è·å–å·²å¯¹é½çš„å›¾åƒå®ä½“
    aligned_image_names = []
    for entity in aligned_entities:
        src_imgs = entity.get('source_image_entities', [])
        if src_imgs:
            aligned_image_names.append(src_imgs[0])
    
    # è¿‡æ»¤å‡ºæœªå¯¹é½çš„å®ä½“è¿›è¡Œå¢å¼º
    to_enhance = [e for e in filtered if e['entity_name'] not in aligned_image_names]
    enhanced = enhance_image_entities(to_enhance, nearby_chunks)
    
    # æ›´æ–°å›¾è°±
    G = nx.read_graphml(img_kg_path)
    
    for entity in enhanced:
        original_name = entity.get('original_name')
        if not original_name or 'description' not in entity:
            continue
        
        new_name = ensure_quoted(entity['entity_name'])
        
        for node_id in list(G.nodes()):
            if node_id.strip('"') == original_name:
                G = nx.relabel_nodes(G, {node_id: new_name})
                G.nodes[new_name]['description'] = entity['description']
                break
    
    nx.write_graphml(G, enhanced_path)
    return enhanced_path


def image_knowledge_graph_update(enhanced_path: str, image_entity_name: str) -> str:
    """æ›´æ–°å›¾åƒçŸ¥è¯†å›¾è°±ï¼Œæ·»åŠ ä¸æ–‡æœ¬å®ä½“çš„è¿æ¥"""
    image_data = get_image_data()
    text_chunks = get_text_chunks()
    chunk_kg = get_chunk_knowledge_graph()
    
    new_path = os.path.join(
        parameter.WORKING_DIR,
        f'images/{image_entity_name}/new_graph_{image_entity_name}_entity_relation.graphml'
    )
    
    image_entity = align_single_image_entity(image_entity_name, text_chunks)
    chunk_index = image_data[image_entity_name].get("chunk_order_index", 0)
    nearby_chunks = get_nearby_chunks(text_chunks, chunk_index)
    nearby_entities = get_nearby_entities(chunk_kg, chunk_index)
    nearby_rels = get_nearby_relationships(chunk_kg, chunk_index)
    
    if not image_entity:
        return enhanced_path
    
    entity_name = image_entity.get("entity_name", "no_match")
    entity_desc = image_entity.get("description", "")
    
    if entity_name.lower().replace(" ", "") in ["no_match", "nomatch"]:
        return enhanced_path
    
    possible_matches = get_possible_entities_image_clustering(entity_desc, nearby_entities, nearby_rels)
    matched_name = judge_image_entity_alignment(entity_name, entity_desc, possible_matches, nearby_chunks)
    
    if not matched_name or not matched_name.strip():
        logger.warning(f"âš ï¸  æœªèƒ½åŒ¹é…å›¾åƒå®ä½“: {entity_name}")
        return enhanced_path
    
    matched_normalized = matched_name.strip().replace(" ", "").replace("\\", "").lower()
    
    G = nx.read_graphml(enhanced_path)
    
    # æŸ¥æ‰¾ORI_IMGèŠ‚ç‚¹
    source_node = None
    for node, data in G.nodes(data=True):
        etype = data.get('entity_type', '')
        if etype in ['"ORI_IMG"', '"UNKNOWN"', 'ORI_IMG', 'UNKNOWN']:
            source_node = node
            break
    
    if source_node is None:
        logger.warning("æœªæ‰¾åˆ°ORI_IMGèŠ‚ç‚¹")
        return enhanced_path
    
    # è·å–è¾¹å±æ€§æ¨¡æ¿
    edges = list(G.edges(data=True))
    if edges:
        edge_data = edges[0][2]
        source_id = edge_data.get("source_id", "")
        order = edge_data.get("order", 1)
    else:
        source_id = G.nodes[source_node].get('source_id', '')
        order = 1
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ–‡æœ¬å®ä½“
    matched = False
    for entity in nearby_entities:
        ename = entity.get("entity_name", "")
        if ename.strip().replace(" ", "").replace("\\", "").lower() == matched_normalized:
            matched = True
            quoted_name = ensure_quoted(ename)
            
            G.add_node(quoted_name,
                       entity_type=entity["entity_type"],
                       description=entity["description"],
                       source_id=source_id)
            
            G.add_edge(source_node, quoted_name,
                       weight=10.0,
                       description=f"{source_node} is the image of {ename}.",
                       source_id=source_id,
                       order=order)
            break
    
    if not matched:
        # æ·»åŠ æ–°çš„å›¾åƒå®ä½“èŠ‚ç‚¹
        G.add_node(entity_name,
                   entity_type="IMG_ENTITY",
                   description=entity_desc,
                   source_id=source_id)
        
        G.add_edge(source_node, entity_name,
                   weight=10.0,
                   description=f"{source_node} is the image of {entity_name}.",
                   source_id=source_id,
                   order=order)
    
    nx.write_graphml(G, new_path)
    return new_path


def merge_graphs(
    image_graph_path: str,
    text_graph_path: str,
    aligned_entities: list[dict],
    image_entity_name: str
) -> str:
    """åˆå¹¶å›¾åƒå’Œæ–‡æœ¬çŸ¥è¯†å›¾è°±"""
    merged_path = os.path.join(parameter.WORKING_DIR, f'graph_merged_{image_entity_name}.graphml')
    
    image_graph = nx.read_graphml(image_graph_path)
    text_graph = nx.read_graphml(text_graph_path)
    
    if image_graph is None or text_graph is None:
        logger.error("âŒ åŠ è½½å›¾è°±å¤±è´¥")
        return text_graph_path
    
    merged = nx.compose(image_graph, text_graph)
    
    for entity_info in aligned_entities:
        required_keys = ['entity_name', 'entity_type', 'description', 'source_image_entities', 'source_text_entities']
        if not all(k in entity_info for k in required_keys):
            continue
        
        src_image = entity_info['source_image_entities']
        src_text = entity_info['source_text_entities']
        
        if not src_image or not src_text:
            continue
        
        target = ensure_quoted(src_image[0])
        
        # è·å–source_id
        src_id_img = image_graph.nodes.get(target, {}).get('source_id', '')
        src_id_txt = text_graph.nodes.get(ensure_quoted(src_text[0]), {}).get('source_id', '')
        combined_source_id = GRAPH_FIELD_SEP.join(filter(None, [src_id_img, src_id_txt]))
        
        # åˆå¹¶å®ä½“
        all_entities = list(set(src_image + src_text))
        
        for entity in all_entities:
            entity = ensure_quoted(entity)
            if entity == target or entity not in merged.nodes:
                continue
                
            if entity in merged.nodes: 
                neighbors = list(merged.neighbors(entity))  # è·å–å½“å‰å®ä½“çš„é‚»å±…
                for neighbor in neighbors:
                    if not merged.has_edge(target, neighbor):
                        merged.add_edge(target, neighbor)  # å°†é‚»å±…ä¸ç›®æ ‡å®ä½“è¿æ¥
                    # å°†è¾¹çš„å±æ€§åˆå¹¶åˆ°ç›®æ ‡èŠ‚ç‚¹çš„è¾¹
                    edge_data = merged.get_edge_data(entity, neighbor)
                    target_edge_data = merged.get_edge_data(target, neighbor)
                    
                    # åˆå¹¶è¾¹çš„å±æ€§ï¼ˆä¼ é€’æ‰€æœ‰å±æ€§ï¼Œå¦‚ weightã€descriptionã€source_id å’Œ orderï¼‰
                    if target_edge_data:
                        # å¦‚æœè¾¹å·²ç»å­˜åœ¨ï¼Œåˆå¹¶ç°æœ‰çš„å±æ€§
                        for key in edge_data:
                            if key in ['weight', 'description', 'source_id', 'order']:
                                # åˆå¹¶è¾¹çš„å±æ€§ï¼Œå¦‚æœå·²æœ‰å°±æ·»åŠ æ–°å€¼ï¼Œæˆ–è€…ä¿æŒå·²æœ‰çš„
                                target_edge_data[key] = edge_data.get(key, target_edge_data.get(key))
                    else:
                        # å¦‚æœè¾¹ä¸å­˜åœ¨ï¼Œå°±æ·»åŠ æ–°çš„è¾¹å±æ€§
                        merged[target][neighbor].update(edge_data)

                merged.remove_node(entity)  # åˆ é™¤å·²ç»åˆå¹¶çš„å®ä½“èŠ‚ç‚¹
        
        # æ›´æ–°ç›®æ ‡èŠ‚ç‚¹å±æ€§
        if target not in merged.nodes:
            merged.add_node(target)
        
        merged.nodes[target].update({
            'entity_type': entity_info['entity_type'],
            'description': entity_info['description'],
            'source_id': combined_source_id
        })
        
        new_name = ensure_quoted(entity_info['entity_name'])
        if new_name != target:
            merged = nx.relabel_nodes(merged, {target: new_name})
    
    nx.write_graphml(merged, merged_path)
    logger.info(f"ğŸ”— å›¾è°±èåˆå®Œæˆ: {merged_path}")
    return merged_path


# ============================================================================
# ä¸»å…¥å£
# ============================================================================

async def fusion(img_ids: list[str]) -> str:
    """
    èåˆæ‰€æœ‰å›¾åƒçš„çŸ¥è¯†å›¾è°±ã€‚
    
    Args:
        img_ids: å›¾åƒå®ä½“åç§°åˆ—è¡¨
    
    Returns:
        æœ€ç»ˆåˆå¹¶å›¾è°±çš„è·¯å¾„
    """
    graph_path = os.path.join(parameter.WORKING_DIR, 'graph_chunk_entity_relation.graphml')
    
    if not img_ids:
        return graph_path
    
    for image_name in tqdm(img_ids, desc="ğŸ”— å›¾è°±èåˆ", unit="å¼ "):
        merged_path = os.path.join(parameter.WORKING_DIR, f'graph_merged_{image_name}.graphml')
        
        if os.path.exists(merged_path):
            graph_path = merged_path
            continue
        
        aligned = image_knowledge_graph_alignment(image_name)
        enhanced_path = enhanced_image_knowledge_graph(aligned, image_name)
        updated_path = image_knowledge_graph_update(enhanced_path, image_name)
        graph_path = merge_graphs(updated_path, graph_path, aligned, image_name)
    
    return graph_path