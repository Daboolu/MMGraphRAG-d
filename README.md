<h3 align="center">ğŸ‰ MMGraphRAG</h3>

<p align="center">
  <b>âœ¨ A Multi-Modal Knowledge Graph RAG Framework âœ¨</b>
</p>

<p align="center">
  <i>From documents to multi-modal knowledge graphs â€” an all-in-one MMGraphRAG solution</i>
</p>

<p align="center">
  <a href="README_zh.md">ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–‡æ¡£</a>
</p>

---

## ğŸŒŸ Key Features

<table>
<tr>
<td width="50%">

### ğŸ“Š Multi-Modal Knowledge Graph
- **Text + Image** unified modeling
- YOLO-based intelligent image segmentation
- Multi-modal entity fusion (spectral clustering)

</td>
<td width="50%">

### ğŸ” Intelligent RAG Retrieval
- Semantic similarity entity retrieval
- Multi-modal context-enhanced answers
- Supports chart/table-related Q&A

</td>
</tr>
<tr>
<td width="50%">

### ğŸ–¼ï¸ Interactive Visualization
- **Built-in Web visualization server**
- Force-directed graph browsing
- Real-time search & subgraph highlighting
- Click to view entity details

</td>
<td width="50%">

### âš¡ Flexible & Easy to Use
- One-command CLI build
- Dual engine support: MinerU / PyMuPDF
- LLM caching for faster re-runs

</td>
</tr>
</table>

---

## ğŸ“– About The Project

![MMGraphRAG Framework](examples/paper/framework.png)

This diagram illustrates the complete workflow of MMGraphRAG.

This project is based on modifications to nano-graphrag to support multi-modal inputs (community-related code removed). The image processing component uses YOLO and Multi-modal Large Language Models (MLLM) to convert images into scene graphs. The fusion component then uses spectral clustering to select candidate entities, combining the textual knowledge graph and the image knowledge graph to construct a multi-modal knowledge graph.

Our Cross-Modal Entity Linking (CMEL) dataset is available here:

https://github.com/wanxueyao/CMEL-dataset

---

## ğŸ”§ Environment Setup

### Dependencies Installation

#### Core Dependencies

```bash
pip install openai                    # LLM API calls
pip install sentence-transformers     # Text embeddings
pip install networkx                  # Graph storage
pip install numpy                     # Numerical computation
pip install scikit-learn              # Vector similarity calculation
pip install Pillow                    # Image processing
pip install tqdm                      # Progress bar
pip install tiktoken                  # Text chunking token calculation
pip install ultralytics               # YOLO image segmentation
pip install opencv-python             # Image processing (cv2)
```

#### Visualization Server Dependencies

```bash
pip install flask                     # Web server framework
pip install flask-cors                # Cross-origin support
```

#### PDF Parsing Dependencies

This project supports two PDF parsing options. **Install at least one**:

| Option | Installation Command | Features |
|--------|---------------------|----------|
| **MinerU** (Recommended) | `pip install -U "mineru[all]"` | Higher parsing quality, supports complex layouts, better image context extraction |
| **PyMuPDF** | `pip install pymupdf` | Lightweight, easy installation, suitable for simple PDFs |

> **Switching**: Set `USE_MINERU = True/False` in `src/parameter.py`
>
> **Fallback**: If MinerU is unavailable, the system automatically falls back to PyMuPDF

### Model Configuration

This project requires **three types of models**, all configured in `src/parameter.py`:

#### 1. Text LLM (Required)
Used for text entity extraction, relationship building, etc. Requires an OpenAI-compatible API:

```python
API_KEY = "your-api-key"
API_BASE = "https://your-api-endpoint/v1"
MODEL_NAME = "qwen3-max"  # or other text models
```

#### 2. Multi-Modal LLM (Required)
Used for image understanding, visual entity extraction, etc. Requires an API that supports image input:

```python
MM_API_KEY = "your-api-key"
MM_API_BASE = "https://your-api-endpoint/v1"
MM_MODEL_NAME = "qwen-vl-max"  # or other multi-modal models
```

#### 3. Embedding Model (Required)
Used for entity vectorization and semantic retrieval. Configure in `src/parameter.py`:

```python
EMBEDDING_MODEL_DIR = './models/all-MiniLM-L6-v2'
EMBED_MODEL = SentenceTransformer(EMBEDDING_MODEL_DIR, device="cpu")
```

> **Tip**: The embedding model can be auto-downloaded using the model name, or manually downloaded and configured with a local path.

### MinerU Configuration

If you choose to use MinerU:

1. **Install**: `pip install -U "mineru[all]"`
2. **Configure**: See [MinerU official documentation](https://github.com/opendatalab/MinerU) for model file downloads
3. **Verify**: Ensure MinerU runs independently before proceeding

---

## âš™ï¸ Parameter Configuration

All core parameters are defined in `src/parameter.py`:

### Directory Configuration

| Parameter | Description | Default |
|-----------|-------------|---------|
| `INPUT_PDF_PATH` | Input PDF file path | - |
| `CACHE_PATH` | LLM response cache directory | `cache` |
| `WORKING_DIR` | Intermediate processing files directory | `working` |
| `OUTPUT_DIR` | Final graph output directory | `output` |
| `MMKG_NAME` | Output graph name | `mmkg_timestamp` |

### Processing Configuration

| Parameter | Description | Default |
|-----------|-------------|---------|
| `USE_MINERU` | Whether to use MinerU for PDF preprocessing | `True` |
| `ENTITY_EXTRACT_MAX_GLEANING` | Max iterations for text entity extraction | `0` |
| `ENTITY_SUMMARY_MAX_TOKENS` | Max tokens for entity summary | `500` |
| `SUMMARY_CONTEXT_MAX_TOKENS` | Max tokens for summary context | `10000` |

### RAG Retrieval Configuration

| Parameter | Description | Default |
|-----------|-------------|---------|
| `QueryParam.top_k` | Number of entities to retrieve | `5` |
| `QueryParam.response_type` | Response style type | `Detailed System-like Response` |
| `QueryParam.local_max_token_for_local_context` | Max tokens for local context | `4000` |
| `QueryParam.number_of_mmentities` | Number of multi-modal entities | `3` |
| `QueryParam.local_max_token_for_text_unit` | Max tokens for text unit | `4000` |
| `RETRIEVAL_THRESHOLD` | Retrieval similarity threshold | `0.2` |

---

## ğŸš€ Usage

### Quick Start

```bash
# 1ï¸âƒ£ Build knowledge graph
python main.py -i path/to/your/document.pdf

# 2ï¸âƒ£ Query
python main.py -q "Your question"

# 3ï¸âƒ£ Launch visualization âœ¨
python main.py -s
# ğŸŒ Visit http://localhost:8080 to explore the interactive graph
```

### Building Knowledge Graph

```bash
# Build graph from specified PDF file
python main.py -i path/to/your/document.pdf

# Specify working and output directories
python main.py -i document.pdf -w ./working -o ./output

# Use PyMuPDF for PDF processing (instead of MinerU)
python main.py -i document.pdf -m pymupdf

# Force rebuild (clear working directory)
python main.py -i document.pdf -f

# Show verbose debug logs
python main.py -i document.pdf -v
```

### RAG Query

```bash
# Query the built graph
python main.py -q "Your question"

# Specify retrieval parameters
python main.py -q "Your question" --top_k 10 --response_type "Concise answer"

# If graph doesn't exist, it will be built first
python main.py -i document.pdf -q "Your question"
```

### ğŸ–¼ï¸ Visualization Server

The built-in Web visualization server lets you intuitively explore the knowledge graph:

```bash
# Start knowledge graph visualization server
python main.py -s

# Specify port and graph file
python main.py -s --port 8888 --graph path/to/graph.graphml
```

**Visualization Highlights**:
- ğŸ”® **Force-Directed Layout**: Automatically optimizes node positions for clear graph structure
- ğŸ” **Real-Time Search**: Quickly locate entities of interest
- ğŸ¯ **Subgraph Highlighting**: Enter a question to highlight relevant entities and connections
- ğŸ“‹ **Details Panel**: Click nodes to view entity descriptions, types, and more
- ğŸ¨ **Type Coloring**: Different entity types use different colors for easy identification

### Command Line Arguments

| Argument | Short | Description |
|----------|-------|-------------|
| `--input` | `-i` | PDF file path |
| `--working` | `-w` | Intermediate working directory |
| `--output` | `-o` | Final output directory |
| `--method` | `-m` | PDF preprocessing method (`mineru`/`pymupdf`) |
| `--force` | `-f` | Force clear working directory and rebuild |
| `--verbose` | `-v` | Show verbose debug logs |
| `--query` | `-q` | Execute RAG query |
| `--top_k` | - | Number of entities to retrieve |
| `--response_type` | - | Response style |
| `--server` | `-s` | Start visualization server |
| `--port` | - | Server port (default: 8080) |
| `--graph` | - | Specify graph file path |

---

## ğŸ“ Example Files

The `examples/` directory contains complete usage examples, demonstrating the full workflow from PDF input to knowledge graph construction and Q&A evaluation:

### Directory Structure

```
examples/
â”œâ”€â”€ example_input/          # ğŸ“¥ Input files
â”‚   â”œâ”€â”€ 2020.acl-main.45.pdf   # Sample PDF: An NLP academic paper
â”‚   â””â”€â”€ 13_qa.jsonl            # Q&A dataset: 13 questions (Text/Multimodal) with ground truth
â”‚
â”œâ”€â”€ example_working/        # âš™ï¸ Intermediate results (auto-generated)
â”‚   â”œâ”€â”€ 2020.acl-main.45/      # PDF preprocessing output (Markdown, layout info)
â”‚   â”œâ”€â”€ images/                # Extracted images from PDF
â”‚   â”œâ”€â”€ graph_*.graphml        # Intermediate graphs (text graph, image graph)
â”‚   â””â”€â”€ kv_store_*.json        # Key-value storage (Text Chunks, Image Descriptions, etc.)
â”‚
â”œâ”€â”€ example_output/         # ğŸ“¤ Final output
â”‚   â”œâ”€â”€ example_mmkg.graphml   # Final fused multi-modal knowledge graph
â”‚   â”œâ”€â”€ example_mmkg_emb.npy   # Graph node embeddings
â”‚   â”œâ”€â”€ example_mmkg_report.md # Build statistics report (node count, entity distribution)
â”‚   â””â”€â”€ retrieval_log.md       # RAG query detailed logs
â”‚
â”œâ”€â”€ cache/                  # ğŸ’¾ Cache data
â”‚   â””â”€â”€ *.json                 # LLM API response cache for faster re-runs
â”‚
â”œâ”€â”€ paper/                  # ğŸ“„ Project materials
â”‚   â”œâ”€â”€ framework.png          # System architecture diagram
â”‚   â””â”€â”€ mmgraphrag.pdf         # Project-related paper/documentation
â”‚
â”œâ”€â”€ docqa_example.py        # ğŸ§ª Q&A evaluation script
â””â”€â”€ docqa_results.md        # ğŸ“Š Evaluation results report
```

### Sample Document & Evaluation

- **Sample Document** (`2020.acl-main.45.pdf`): Demonstrates the system's ability to process academic papers with rich text and charts.
- **Evaluation Script** (`docqa_example.py`): A one-click evaluation tool that:
    1. Automatically reads the sample PDF and builds a knowledge graph
    2. Loads questions from `13_qa.jsonl` (covering text-only and multi-modal chart Q&A)
    3. Performs RAG retrieval and answering using the built graph
    4. Generates a detailed evaluation report `docqa_results.md`, comparing model answers with ground truth

Run evaluation:
```bash
python examples/docqa_example.py
```

---

<p align="center">
  <i>Letting Hues Quietly weave through knowledge graph ğŸ¨</i><br>
  <i>a small graph with big dreams âœ¨</i>
</p>
