<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d6" for="edge" attr.name="order" attr.type="long"/>
<key id="d5" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d4" for="edge" attr.name="description" attr.type="string"/>
<key id="d3" for="edge" attr.name="weight" attr.type="double"/>
<key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;IMAGE_18&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image is a structured table labeled 'Table 5: Experimental results for NER task,' presenting comparative performance metrics of various named entity recognition (NER) models across four different datasets: English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA, and Chinese OntoNotes 4.0. The table is divided into four main sections, each corresponding to one dataset. Within each section, the rows list different models, and the columns report their performance in terms of Precision (Prec.), Recall (Rec.), and F1 score. The models include ELMo (Peters et al., 2018), CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), and its variants with additional loss functions: BERT-MRC+FL (Focal Loss), BERT-MRC+DL (Dice Loss), and BERT-MRC+DSC (Dice-Similarity Coefficient Loss). For each model, the exact numerical values are provided. Notably, in the English CoNLL 2003 dataset, BERT-MRC achieves an F1 score of 93.04, while BERT-MRC+DSC improves it to 93.33 (+0.29). In English OntoNotes 5.0, BERT-MRC+DSC reaches an F1 of 92.07 (+0.96) compared to the base BERT-MRC. On Chinese MSRA, BERT-MRC+DSC achieves 96.72 (+0.97), and on Chinese OntoNotes 4.0, it scores 84.47 (+2.36), significantly outperforming the baseline BERT-MRC. The table highlights that the DSC loss consistently provides substantial gains over other losses like FL and DL, especially in Chinese datasets. The context emphasizes that these results demonstrate state-of-the-art (SOTA) performance, particularly due to the effectiveness of the DSC loss in handling data imbalance issues."</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;ENGLISH CONLL 2003&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"A benchmark dataset for named entity recognition in English, used to evaluate the performance of various models."</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;ENGLISH ONTONOTES 5.0&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"A multilingual corpus for semantic role labeling and other NLP tasks, specifically the English version with rich annotations."</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;CHINESE MSRA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"A Chinese named entity recognition dataset developed by Microsoft Research Asia, commonly used for evaluating Chinese NLP models."</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;CHINESE ONTONOTES 4.0&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"The Chinese version of the OntoNotes corpus, used for evaluating syntactic and semantic parsing models on Chinese text."</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;ELMO (PETERS ET AL., 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A language model based on deep bidirectional LSTM networks that provides context-sensitive word representations.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;CVT (CLARK ET AL., 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A contextualized vector representation model designed for named entity recognition tasks.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;BERT-TAGGER (DEVLIN ET AL., 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A BERT-based model adapted for sequence tagging tasks such as named entity recognition.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;BERT-MRC (LI ET AL., 2019)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A reading comprehension-based approach using BERT to perform named entity recognition.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;BERT-MRC+FL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"An enhanced version of BERT-MRC incorporating a fine-tuning strategy for improved performance.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;BERT-MRC+DL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"An extension of BERT-MRC with a dual learning framework to improve generalization.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;BERT-MRC+DSC&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A variant of BERT-MRC that uses dynamic self-criticism to refine predictions during training.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A lattice-based LSTM model designed for handling Chinese characters and their morphological variations.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;GLYCE-BERT (WU ET AL., 2019)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A BERT-based model that incorporates character-level information for better Chinese language understanding.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;PREC.&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">"Precision metric indicating the proportion of correctly identified entities among all predicted entities.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;REC.&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">"Recall metric measuring the proportion of actual entities correctly identified by the model.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<node id="&quot;F1&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">"Harmonic mean of precision and recall, providing a balanced measure of model performance.""</data>
  <data key="d2">examples/example_working/images/image_18.jpg</data>
</node>
<edge source="&quot;IMAGE_18&quot;" target="&quot;ENGLISH CONLL 2003&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"English CoNLL 2003是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;ENGLISH ONTONOTES 5.0&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"English OntoNotes 5.0是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;CHINESE MSRA&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Chinese MSRA是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;CHINESE ONTONOTES 4.0&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Chinese OntoNotes 4.0是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;ELMO (PETERS ET AL., 2018)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"ELMo (Peters et al., 2018)是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;CVT (CLARK ET AL., 2018)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CVT (Clark et al., 2018)是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;BERT-TAGGER (DEVLIN ET AL., 2018)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"BERT-Tagger (Devlin et al., 2018)是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;BERT-MRC (LI ET AL., 2019)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"BERT-MRC (Li et al., 2019)是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;BERT-MRC+FL&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"BERT-MRC+FL是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;BERT-MRC+DL&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"BERT-MRC+DL是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;BERT-MRC+DSC&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"BERT-MRC+DSC是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Lattice-LSTM (Zhang and Yang, 2018)是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;GLYCE-BERT (WU ET AL., 2019)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Glyce-BERT (Wu et al., 2019)是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;PREC.&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Prec.是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;REC.&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Rec.是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_18&quot;" target="&quot;F1&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"F1是从image_18中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH CONLL 2003&quot;" target="&quot;ELMO (PETERS ET AL., 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"ELMo is evaluated on the English CoNLL 2003 dataset, achieving an F1 score of 92.22."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH CONLL 2003&quot;" target="&quot;CVT (CLARK ET AL., 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"CVT is evaluated on the English CoNLL 2003 dataset, achieving an F1 score of 92.6."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH CONLL 2003&quot;" target="&quot;BERT-TAGGER (DEVLIN ET AL., 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERT-Tagger is evaluated on the English CoNLL 2003 dataset, achieving an F1 score of 92.8."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH CONLL 2003&quot;" target="&quot;BERT-MRC (LI ET AL., 2019)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT-MRC achieves the highest F1 score of 93.04 on the English CoNLL 2003 dataset."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH CONLL 2003&quot;" target="&quot;BERT-MRC+FL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT-MRC+FL performs slightly worse than BERT-MRC on English CoNLL 2003 with an F1 of 93.11."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH CONLL 2003&quot;" target="&quot;BERT-MRC+DL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT-MRC+DL achieves an F1 of 93.17 on English CoNLL 2003, showing marginal improvement over baseline."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH CONLL 2003&quot;" target="&quot;BERT-MRC+DSC&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT-MRC+DSC achieves the best result on English CoNLL 2003 with an F1 of 93.33."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH ONTONOTES 5.0&quot;" target="&quot;CVT (CLARK ET AL., 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"CVT is evaluated on English OntoNotes 5.0, achieving an F1 score of 88.8."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH ONTONOTES 5.0&quot;" target="&quot;BERT-TAGGER (DEVLIN ET AL., 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERT-Tagger is evaluated on English OntoNotes 5.0, achieving an F1 of 89.16."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH ONTONOTES 5.0&quot;" target="&quot;BERT-MRC (LI ET AL., 2019)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT-MRC achieves an F1 of 91.11 on English OntoNotes 5.0, outperforming earlier models."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH ONTONOTES 5.0&quot;" target="&quot;BERT-MRC+FL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT-MRC+FL improves upon BERT-MRC with an F1 of 91.22 on English OntoNotes 5.0."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH ONTONOTES 5.0&quot;" target="&quot;BERT-MRC+DL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT-MRC+DL achieves an F1 of 91.88 on English OntoNotes 5.0, showing consistent gains."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ENGLISH ONTONOTES 5.0&quot;" target="&quot;BERT-MRC+DSC&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT-MRC+DSC achieves the highest F1 of 92.07 on English OntoNotes 5.0."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE MSRA&quot;" target="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Lattice-LSTM is evaluated on Chinese MSRA, achieving an F1 of 93.18."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE MSRA&quot;" target="&quot;BERT-TAGGER (DEVLIN ET AL., 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERT-Tagger achieves an F1 of 94.80 on Chinese MSRA."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE MSRA&quot;" target="&quot;GLYCE-BERT (WU ET AL., 2019)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Glyce-BERT achieves an F1 of 95.54 on Chinese MSRA."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE MSRA&quot;" target="&quot;BERT-MRC (LI ET AL., 2019)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT-MRC achieves an F1 of 95.75 on Chinese MSRA, the highest among baseline models."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE MSRA&quot;" target="&quot;BERT-MRC+FL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERT-MRC+FL slightly underperforms with an F1 of 95.67 on Chinese MSRA."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE MSRA&quot;" target="&quot;BERT-MRC+DL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT-MRC+DL achieves an F1 of 96.44 on Chinese MSRA, showing strong improvement."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE MSRA&quot;" target="&quot;BERT-MRC+DSC&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT-MRC+DSC achieves the best performance on Chinese MSRA with an F1 of 96.72."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE ONTONOTES 4.0&quot;" target="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Lattice-LSTM is evaluated on Chinese OntoNotes 4.0, achieving an F1 of 73.88."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE ONTONOTES 4.0&quot;" target="&quot;BERT-TAGGER (DEVLIN ET AL., 2018)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"BERT-Tagger achieves an F1 of 79.16 on Chinese OntoNotes 4.0."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE ONTONOTES 4.0&quot;" target="&quot;GLYCE-BERT (WU ET AL., 2019)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Glyce-BERT achieves an F1 of 80.62 on Chinese OntoNotes 4.0."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE ONTONOTES 4.0&quot;" target="&quot;BERT-MRC (LI ET AL., 2019)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT-MRC achieves an F1 of 82.11 on Chinese OntoNotes 4.0."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE ONTONOTES 4.0&quot;" target="&quot;BERT-MRC+FL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT-MRC+FL improves to an F1 of 83.30 on Chinese OntoNotes 4.0."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE ONTONOTES 4.0&quot;" target="&quot;BERT-MRC+DL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT-MRC+DL achieves an F1 of 84.01 on Chinese OntoNotes 4.0."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHINESE ONTONOTES 4.0&quot;" target="&quot;BERT-MRC+DSC&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT-MRC+DSC achieves the highest F1 of 84.47 on Chinese OntoNotes 4.0."</data>
  <data key="d5">examples/example_working/images/image_18.jpg</data>
  <data key="d6">1</data>
</edge>
</graph></graphml>