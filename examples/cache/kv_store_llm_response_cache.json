{
  "7b386a292f7f1e491117333e369fdeea": {
    "return": "(\"entity\"<|>\"Table 3\"<|>\"event\"<|>\"Table 3 presents experimental results for Chinese POS (Part-of-Speech) tagging datasets, specifically CTB5, CTB6, and UD1.4, showcasing performance metrics of various models.\")##\n(\"entity\"<|>\"Table 4\"<|>\"event\"<|>\"Table 4 presents experimental results for English POS tagging datasets, including evaluations on English WSJ and English Tweets corpora.\")##\n(\"entity\"<|>\"Shao et al., 2017\"<|>\"person\"<|>\"Shao et al. (2017) are researchers who developed the Joint-POS models (Sig and Ens variants) evaluated in the experiments.\")##\n(\"entity\"<|>\"Zhang and Yang, 2018\"<|>\"person\"<|>\"Zhang and Yang (2018) are researchers who proposed the Lattice-LSTM model for Chinese POS tagging, cited in the results.\")##\n(\"entity\"<|>\"Devlin et al., 2018\"<|>\"person\"<|>\"Devlin et al. (2018) are the authors of the BERT model; their BERT-Tagger is used as a baseline in multiple experiments across Chinese and English datasets.\")##\n(\"entity\"<|>\"Bohnet et al., 2018\"<|>\"person\"<|>\"Bohnet et al. (2018) are researchers who developed the Meta BiLSTM model evaluated on the English WSJ dataset.\")##\n(\"entity\"<|>\"Godin, 2019\"<|>\"person\"<|>\"Godin (2019) is a researcher associated with the FastText+CNN+CRF model evaluated on English Tweets.\")##\n(\"relationship\"<|>\"Table 3\"<|>\"Shao et al., 2017\"<|>\"The Joint-POS models by Shao et al. (2017) are reported in Table 3 as part of the experimental results for Chinese POS datasets.\"<|>8)##\n(\"relationship\"<|>\"Table 3\"<|>\"Zhang and Yang, 2018\"<|>\"The Lattice-LSTM model by Zhang and Yang (2018) is included in Table 3’s results for Chinese POS tagging.\"<|>8)##\n(\"relationship\"<|>\"Table 3\"<|>\"Devlin et al., 2018\"<|>\"The BERT-Tagger by Devlin et al. (2018) appears in Table 3 as a baseline model for Chinese POS tagging experiments.\"<|>8)##\n(\"relationship\"<|>\"Table 4\"<|>\"Bohnet et al., 2018\"<|>\"The Meta BiLSTM model by Bohnet et al. (2018) is listed in Table 4 under results for the English WSJ dataset.\"<|>8)##\n(\"relationship\"<|>\"Table 4\"<|>\"Devlin et al., 2018\"<|>\"The BERT-Tagger and its variants by Devlin et al. (2018) are central to the results presented in Table 4 for English POS datasets.\"<|>9)##\n(\"relationship\"<|>\"Table 4\"<|>\"Godin, 2019\"<|>\"The FastText+CNN+CRF model by Godin (2019) is referenced in Table 4 in the context of English Tweets POS tagging results.\"<|>7)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "f9ca363b81231518a0f281c72935e839": {
    "return": "(\"entity\"<|>\"DICE COEFFICIENT\"<|>\"concept\"<|>\"The Dice Coefficient (DSC) is an F1-oriented statistic used to measure the similarity between two sets, commonly applied in evaluating model predictions against ground truth in machine learning contexts. It is mathematically defined and discussed with variants including smoothing and loss formulations.\")##\n(\"entity\"<|>\"TABLE 2\"<|>\"table\"<|>\"Table 2 presents a comparison of different loss functions and their mathematical formulas, including CE, WCE, DL, TL, and DSC FL, with notes on smoothing and positivity adjustments.\")##\n(\"entity\"<|>\"MILLETARI ET AL. (2016)\"<|>\"person\"<|>\"Milletari et al. (2016) refers to a group of researchers who proposed a modification to the Dice Loss denominator using a squared form to achieve faster convergence in optimization.\")##\n(\"entity\"<|>\"DICE LOSS (DL)\"<|>\"concept\"<|>\"Dice Loss (DL) is a loss function derived from the Dice Coefficient, used in training machine learning models, particularly in segmentation tasks. It exists in individual-sample and set-level formulations and often includes a smoothing factor γ.\")##\n(\"entity\"<|>\"TVERSKY INDEX (TI)\"<|>\"concept\"<|>\"The Tversky Index (TI) is a generalization of the Dice Coefficient that approximates the Fβ score, allowing for asymmetric penalization of false positives and false negatives.\")##\n(\"relationship\"<|>\"DICE COEFFICIENT\"<|>\"DICE LOSS (DL)\"<|>\"Dice Loss is directly derived from the Dice Coefficient, adapting it into a differentiable loss function suitable for model training.\"<|>9)##\n(\"relationship\"<|>\"MILLETARI ET AL. (2016)\"<|>\"DICE LOSS (DL)\"<|>\"Milletari et al. (2016) proposed a specific squared-form modification to the Dice Loss denominator to improve convergence speed.\"<|>8)##\n(\"relationship\"<|>\"TABLE 2\"<|>\"DICE LOSS (DL)\"<|>\"Table 2 includes the formula for Dice Loss (DL) among other loss functions, providing a concise reference for its mathematical expression.\"<|>7)##\n(\"relationship\"<|>\"DICE COEFFICIENT\"<|>\"TVERSKY INDEX (TI)\"<|>\"The Tversky Index extends the Dice Coefficient to a more general case, functioning as an approximation of the Fβ score.\"<|>8)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "9516d2c9400f0922839029978c948a4a": {
    "return": "(\"entity\"<|>\"Table 9\"<|>\"event\"<|>\"Table 9 presents experimental results on the effect of DL and DSC on sentiment classification tasks, specifically involving BERT fine-tuned with cross-entropy.\")##\n(\"entity\"<|>\"Table 10\"<|>\"event\"<|>\"Table 10 displays the effect of hyperparameters (α and β) in the Tversky Index on performance metrics across two datasets: Chinese OntoNotes4.0 and English QuoRef MRC.\")##\n(\"entity\"<|>\"Chinese OntoNotes4.0 NER dataset\"<|>\"geo\"<|>\"Chinese OntoNotes4.0 is a named entity recognition dataset focused on Chinese language data, used in experiments evaluating the Tversky Index hyperparameters.\")##\n(\"entity\"<|>\"English QuoRef MRC dataset\"<|>\"geo\"<|>\"English QuoRef MRC is a machine reading comprehension dataset in English, used alongside the Chinese OntoNotes4.0 dataset to test hyperparameter effects in the Tversky Index.\")##\n(\"entity\"<|>\"Qinghong Han\"<|>\"person\"<|>\"Qinghong Han is acknowledged for providing comments and suggestions on the research work.\")##\n(\"entity\"<|>\"Wei Wu\"<|>\"person\"<|>\"Wei Wu is acknowledged for providing comments and suggestions on the research work.\")##\n(\"entity\"<|>\"Jiawei Wu\"<|>\"person\"<|>\"Jiawei Wu is acknowledged for providing comments and suggestions on the research work.\")##\n(\"entity\"<|>\"National Natural Science Foundation of China\"<|>\"organization\"<|>\"The National Natural Science Foundation of China (NSFC) provided funding support for this research under grant numbers 61625107 and 61751209.\")##\n(\"relationship\"<|>\"Table 9\"<|>\"BERT + CE\"<|>\"Table 9 references 'BERT + CE' as a method involving fine-tuning BERT with cross-entropy, though 'BERT + CE' itself is not an entity type listed; thus, no explicit relationship is formed per allowed types.\"<|>0)##\n(\"relationship\"<|>\"Table 10\"<|>\"Chinese OntoNotes4.0 NER dataset\"<|>\"Table 10 contains experimental results specifically derived from the Chinese OntoNotes4.0 NER dataset, showing F1 scores at varying α values.\"<|>9)##\n(\"relationship\"<|>\"Table 10\"<|>\"English QuoRef MRC dataset\"<|>\"Table 10 includes performance results on the English QuoRef MRC dataset across different α settings in the Tversky Index.\"<|>9)##\n(\"relationship\"<|>\"Qinghong Han\"<|>\"National Natural Science Foundation of China\"<|>\"While both are mentioned in the Acknowledgement section, there is no direct indication that Qinghong Han is affiliated with NSFC; the relationship is not clearly established.\"<|>0)##\n(\"relationship\"<|>\"Wei Wu\"<|>\"National Natural Science Foundation of China\"<|>\"No explicit link is provided between Wei Wu and the NSFC beyond both being mentioned in the Acknowledgement section.\"<|>0)##\n(\"relationship\"<|>\"Jiawei Wu\"<|>\"National Natural Science Foundation of China\"<|>\"There is no clear evidence in the text linking Jiawei Wu directly to the NSFC funding body.\"<|>0)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "7ab94702f0af514df3c1dc11a22e0476": {
    "return": "(\"entity\"<|>\"SPACY1\"<|>\"organization\"<|>\"Spacy1 refers to the spaCy natural language processing library, used in the text for named entity recognition and linking mentions to DBpedia entities.\")##\n(\"entity\"<|>\"DBPEDIA\"<|>\"organization\"<|>\"DBpedia is a large-scale, multilingual knowledge base extracted from Wikipedia, used here as a reference for linking entity mentions.\")##\n(\"entity\"<|>\"BERT\"<|>\"organization\"<|>\"BERT (Bidirectional Encoder Representations from Transformers) is a language model used in the experiments described, serving as the base architecture for various training objectives.\")##\n(\"entity\"<|>\"STANFORD SENTIMENT TREEBANK (SST)\"<|>\"organization\"<|>\"The Stanford Sentiment Treebank is a benchmark dataset for sentiment analysis, comprising SST-2 and SST-5 variants, used to evaluate model performance on accuracy-oriented tasks.\")##\n(\"entity\"<|>\"TABLE 8\"<|>\"event\"<|>\"Table 8 presents experimental results comparing different data augmentation strategies for the Quora Question Pairs (QQP) dataset in terms of F1-score across multiple BERT-based model variants.\")##\n(\"entity\"<|>\"TABLE 9\"<|>\"event\"<|>\"Table 9 displays the performance of BERT models with different loss functions (CE, DL, DSC) on the Stanford Sentiment Treebank datasets (SST-2 and SST-5), measured by accuracy.\")##\n(\"entity\"<|>\"QQP\"<|>\"event\"<|>\"QQP refers to the Quora Question Pairs dataset, an evaluation benchmark used in the experiments to assess data augmentation techniques via F1-score.\")##\n(\"entity\"<|>\"SST-2\"<|>\"event\"<|>\"SST-2 is a binary sentiment classification task derived from the Stanford Sentiment Treebank, used to test model accuracy under different training objectives.\")##\n(\"entity\"<|>\"SST-5\"<|>\"event\"<|>\"SST-5 is a fine-grained five-class sentiment classification task from the Stanford Sentiment Treebank, evaluated for accuracy in the described experiments.\")##\n(\"relationship\"<|>\"SPACY1\"<|>\"DBPEDIA\"<|>\"Spacy1 is used to retrieve entity mentions and link them to corresponding entities in DBpedia for data augmentation.\"<|>8)##\n(\"relationship\"<|>\"BERT\"<|>\"TABLE 8\"<|>\"BERT and its variants (e.g., BERT+FL, BERT+DL) are evaluated in Table 8 using different data augmentation methods on the QQP dataset.\"<|>9)##\n(\"relationship\"<|>\"BERT\"<|>\"TABLE 9\"<|>\"BERT models with different loss functions (CE, DL, DSC) are evaluated in Table 9 on SST-2 and SST-5 for accuracy.\"<|>9)##\n(\"relationship\"<|>\"TABLE 8\"<|>\"QQP\"<|>\"Table 8 specifically reports results for data augmentation experiments conducted on the QQP dataset.\"<|>10)##\n(\"relationship\"<|>\"TABLE 9\"<|>\"STANFORD SENTIMENT TREEBANK (SST)\"<|>\"Table 9 presents results from experiments on the Stanford Sentiment Treebank datasets, including SST-2 and SST-5.\"<|>10)##\n(\"relationship\"<|>\"STANFORD SENTIMENT TREEBANK (SST)\"<|>\"SST-2\"<|>\"SST-2 is a subset of the Stanford Sentiment Treebank used for binary sentiment classification.\"<|>9)##\n(\"relationship\"<|>\"STANFORD SENTIMENT TREEBANK (SST)\"<|>\"SST-5\"<|>\"SST-5 is a subset of the Stanford Sentiment Treebank used for fine-grained five-class sentiment classification.\"<|>9)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "c0fb11f1988c4b8b22b8194a1e9cf3bf": {
    "return": "(\"entity\"<|>\"Figure 1\"<|>\"event\"<|>\"Figure 1 is an illustration depicting the derivatives of four loss functions, highlighting how different losses behave with respect to prediction probability p, particularly around the threshold of 0.5.\")##\n(\"entity\"<|>\"Table\"<|>\"event\"<|>\"The Table presents evaluation metrics (Precision, Recall, F1) for different models across datasets CTB5, CTB6, and UD1.4, including results for 'Joint-POS(Sig)(Shao et al., 2017)'.\")##\n(\"entity\"<|>\"Tversky Index\"<|>\"event\"<|>\"Tversky Index (TI) is a generalization of the Dice coefficient used to compute similarity between two sets, offering control over false-negative and false-positive tradeoffs via parameters alpha and beta.\")##\n(\"entity\"<|>\"Tversky Loss\"<|>\"event\"<|>\"Tversky Loss (TL) is a loss function derived from the Tversky Index, designed to improve model training by adjusting penalties for false positives and false negatives.\")##\n(\"entity\"<|>\"Self-adjusting Dice Loss\"<|>\"event\"<|>\"Self-adjusting Dice Loss is a proposed adaptive variant of the Dice Similarity Coefficient that introduces a decaying factor (1 - p) to reduce the influence of easy examples during training.\")##\n(\"entity\"<|>\"Focal Loss\"<|>\"event\"<|>\"Focal Loss (FL) is a loss function originally proposed by Lin et al. (2017) for object detection to mitigate class imbalance by down-weighting well-classified examples using a (1 - p)^γ factor.\")##\n(\"entity\"<|>\"Dice Similarity Coefficient\"<|>\"event\"<|>\"Dice Similarity Coefficient (DSC) is a measure of overlap between two sets, referenced as a special case of the Tversky Index when α = β = 0.5, and serves as a basis for several loss functions discussed.\")##\n(\"entity\"<|>\"Lin et al., 2017\"<|>\"person\"<|>\"Lin et al., 2017 refers to the authors who introduced Focal Loss in the context of one-stage object detection in computer vision.\")##\n(\"entity\"<|>\"Shao et al., 2017\"<|>\"person\"<|>\"Shao et al., 2017 are researchers cited for their 'Joint-POS(Sig)' model, which achieved specific performance metrics on Chinese Treebank and Universal Dependencies datasets.\")##\n(\"relationship\"<|>\"Tversky Index\"<|>\"Tversky Loss\"<|>\"Tversky Loss is directly derived from the Tversky Index by transforming it into a differentiable loss function for optimization.\"<|>9)##\n(\"relationship\"<|>\"Tversky Index\"<|>\"Dice Similarity Coefficient\"<|>\"The Dice Similarity Coefficient is a special case of the Tversky Index when α = β = 0.5.\"<|>8)##\n(\"relationship\"<|>\"Self-adjusting Dice Loss\"<|>\"Dice Similarity Coefficient\"<|>\"Self-adjusting Dice Loss modifies the standard DSC by introducing a weighting factor (1 - p) to adaptively focus on hard examples.\"<|>9)##\n(\"relationship\"<|>\"Self-adjusting Dice Loss\"<|>\"Focal Loss\"<|>\"The Self-adjusting Dice Loss mimics the core idea of Focal Loss by down-weighting easy examples during training, similar to how Focal Loss uses (1 - p)^γ.\"<|>7)##\n(\"relationship\"<|>\"Focal Loss\"<|>\"Lin et al., 2017\"<|>\"Lin et al., 2017 are the original proposers of Focal Loss in the domain of object detection.\"<|>10)##\n(\"relationship\"<|>\"Table\"<|>\"Shao et al., 2017\"<|>\"The Table includes performance results for the model proposed by Shao et al., 2017, specifically 'Joint-POS(Sig)'.<|>8)##\n(\"relationship\"<|>\"Figure 1\"<|>\"Dice Similarity Coefficient\"<|>\"Figure 1 illustrates the derivative behavior of DSC compared to other loss functions, showing how DSC stops providing gradient once p > 0.5.\"<|>8)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "e667f499d5869d7e3b5551f5e0e70f3d": {
    "return": "(\"entity\"<|>\"Table 6\"<|>\"organization\"<|>\"Table 6 is a data table presenting experimental results for the Machine Reading Comprehension (MRC) task, comparing models like BERT and XLNet with and without the proposed DSC loss.\")##\n(\"entity\"<|>\"Table 7\"<|>\"organization\"<|>\"Table 7 is a data table showing experimental results for the Paraphrase Identification (PI) task on MRPC and QQP datasets, using BERT and XLNet as base models with various training objectives.\")##\n(\"entity\"<|>\"BERT\"<|>\"organization\"<|>\"BERT refers to the Bidirectional Encoder Representations from Transformers model introduced by Devlin et al. in 2018, used as a baseline in paraphrase identification and MRC tasks.\")##\n(\"entity\"<|>\"XLNet\"<|>\"organization\"<|>\"XLNet is a language model introduced by Yang et al. in 2019, used as a baseline in experiments for both MRC and PI tasks, noted for enabling learning of bidirectional contexts.\")##\n(\"entity\"<|>\"DSC\"<|>\"organization\"<|>\"DSC refers to a proposed loss function or training objective that enhances model performance in both MRC and PI tasks when combined with BERT or XLNet.\")##\n(\"entity\"<|>\"MRPC\"<|>\"event\"<|>\"MRPC (Microsoft Research Paraphrase Corpus) is a dataset and associated evaluation task for determining if two sentences are paraphrases, used in the PI experiments.\")##\n(\"entity\"<|>\"QQP\"<|>\"event\"<|>\"QQP (Quora Question Pairs) is a dataset and associated evaluation task for paraphrase identification, featuring imbalanced class distribution with 37% positive and 63% negative examples.\")##\n(\"entity\"<|>\"SQuADv1.1\"<|>\"event\"<|>\"SQuADv1.1 is a Machine Reading Comprehension dataset used in evaluating model performance, referenced in the experimental results with EM and F1 scores.\")##\n(\"entity\"<|>\"SQuAD v2.0\"<|>\"event\"<|>\"SQuAD v2.0 is an extended version of the SQuAD dataset that includes unanswerable questions, used in MRC evaluations.\")##\n(\"entity\"<|>\"QuoRef\"<|>\"event\"<|>\"QuoRef is a dataset used in the Machine Reading Comprehension task, referenced in the experimental results comparing model performance.\")##\n(\"entity\"<|>\"Devlin et al., 2018\"<|>\"person\"<|>\"Devlin et al., 2018 refers to the authors of the original BERT paper, cited as the source for the BERT model used in the experiments.\")##\n(\"entity\"<|>\"Yang et al., 2019\"<|>\"person\"<|>\"Yang et al., 2019 refers to the authors of the XLNet paper, cited as the source for the XLNet model used in the experiments.\")##\n(\"entity\"<|>\"DBpedia\"<|>\"organization\"<|>\"DBpedia is a knowledge base used in the data augmentation process to replace entity mentions during the creation of balanced synthetic datasets.\")##\n(\"entity\"<|>\"Spacy\"<|>\"organization\"<|>\"Spacy is a natural language processing library used to retrieve entity mentions for data augmentation in the ablation studies.\")##\n(\"relationship\"<|>\"Table 6\"<|>\"MRC\"<|>\"Table 6 presents the experimental results specifically for the Machine Reading Comprehension (MRC) task.\"<|>9)##\n(\"relationship\"<|>\"Table 7\"<|>\"PI\"<|>\"Table 7 presents the experimental results specifically for the Paraphrase Identification (PI) task.\"<|>9)##\n(\"relationship\"<|>\"BERT\"<|>\"MRPC\"<|>\"BERT is evaluated on the MRPC dataset as part of the PI task experiments.\"<|>8)##\n(\"relationship\"<|>\"BERT\"<|>\"QQP\"<|>\"BERT is evaluated on the QQP dataset as part of the PI task experiments.\"<|>8)##\n(\"relationship\"<|>\"XLNet\"<|>\"SQuADv1.1\"<|>\"XLNet is used as a baseline model evaluated on the SQuADv1.1 dataset in the MRC task.\"<|>8)##\n(\"relationship\"<|>\"XLNet\"<|>\"SQuAD v2.0\"<|>\"XLNet is used as a baseline model evaluated on the SQuAD v2.0 dataset in the MRC task.\"<|>8)##\n(\"relationship\"<|>\"XLNet\"<|>\"QuoRef\"<|>\"XLNet is used as a baseline model evaluated on the QuoRef dataset in the MRC task.\"<|>8)##\n(\"relationship\"<|>\"DSC\"<|>\"BERT\"<|>\"DSC is applied as a training objective in combination with BERT to improve performance in both MRC and PI tasks.\"<|>9)##\n(\"relationship\"<|>\"DSC\"<|>\"XLNet\"<|>\"DSC is applied as a training objective in combination with XLNet to improve performance in both MRC and PI tasks.\"<|>9)##\n(\"relationship\"<|>\"QQP\"<|>\"DBpedia\"<|>\"DBpedia is used to augment the QQP dataset by replacing entity mentions during synthetic data creation.\"<|>7)##\n(\"relationship\"<|>\"QQP\"<|>\"Spacy\"<|>\"Spacy is used to extract entity mentions from the QQP dataset for data augmentation purposes.\"<|>7)##\n(\"relationship\"<|>\"Devlin et al., 2018\"<|>\"BERT\"<|>\"Devlin et al., 2018 are the creators of the BERT model cited in the text.\"<|>10)##\n(\"relationship\"<|>\"Yang et al., 2019\"<|>\"XLNet\"<|>\"Yang et al., 2019 are the creators of the XLNet model cited in the text.\"<|>10)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "fb256c0ab7ca1f37d6d9a41735661bfb": {
    "return": "(\"entity\"<|>\"Table 2\"<|>\"event\"<|>\"Table 2 summarizes the various loss functions discussed in the paper, serving as a reference point for experimental analysis.\")##\n(\"entity\"<|>\"Figure 1\"<|>\"event\"<|>\"Figure 1 provides a visual explanation of the derivative behavior of the DSC loss compared to other losses, illustrating how model attention changes with prediction confidence.\")##\n(\"entity\"<|>\"Table 3\"<|>\"event\"<|>\"Table 3 presents experimental results on Chinese POS tagging datasets, showing that the proposed DSC loss achieves state-of-the-art performance.\")##\n(\"entity\"<|>\"Table 4\"<|>\"event\"<|>\"Table 4 presents experimental results for English POS tagging datasets, though specific data is not detailed in the provided text.\")##\n(\"entity\"<|>\"Table 5\"<|>\"event\"<|>\"Table 5 displays experimental results for the Named Entity Recognition (NER) task on English CoNLL 2003 and OntoNotes5.0 datasets.\")##\n(\"entity\"<|>\"Chinese Treebank\"<|>\"geo\"<|>\"Chinese Treebank (CTB) 5.0/6.0 is a widely used Chinese part-of-speech tagging dataset referenced in the experiments.\")##\n(\"entity\"<|>\"UD1.4\"<|>\"geo\"<|>\"UD1.4 refers to Universal Dependencies version 1.4, a dataset used for Chinese part-of-speech tagging evaluation.\")##\n(\"entity\"<|>\"Wall Street Journal\"<|>\"geo\"<|>\"Wall Street Journal (WSJ) is an English dataset used for part-of-speech tagging experiments.\")##\n(\"entity\"<|>\"Ritter et al. (2011) dataset\"<|>\"geo\"<|>\"This is an English POS tagging dataset introduced by Ritter et al. in 2011, used as a benchmark in the study.\")##\n(\"entity\"<|>\"English CoNLL 2003\"<|>\"geo\"<|>\"English CoNLL 2003 is a standard named entity recognition dataset used for evaluating NER models.\")##\n(\"entity\"<|>\"English OntoNotes5.0\"<|>\"geo\"<|>\"English OntoNotes5.0 is another NER evaluation dataset referenced in Table 5.\")##\n(\"entity\"<|>\"Devlin et al., 2018\"<|>\"person\"<|>\"Devlin et al. (2018) are the authors of BERT, which serves as the backbone model in the experiments and introduced the BERT-Tagger baseline.\")##\n(\"entity\"<|>\"Shao et al., 2017\"<|>\"person\"<|>\"Shao et al. (2017) proposed the Joint-POS model that jointly learns Chinese word segmentation and POS tagging.\")##\n(\"entity\"<|>\"Zhang and Yang, 2018\"<|>\"person\"<|>\"Zhang and Yang (2018) developed the Lattice-LSTM model using a word-character lattice network for Chinese NLP tasks.\")##\n(\"entity\"<|>\"Li et al., 2019\"<|>\"person\"<|>\"Li et al. (2019) introduced the BERT-MRC approach, adapted here for NER tasks.\")##\n(\"entity\"<|>\"Peters et al., 2018\"<|>\"person\"<|>\"Peters et al. (2018) developed ELMo, used as a baseline NER model in Table 5.\")##\n(\"entity\"<|>\"Clark et al., 2018\"<|>\"person\"<|>\"Clark et al. (2018) proposed the CVT method, used as a baseline in both POS and NER evaluations.\")##\n(\"entity\"<|>\"BERT-Tagger\"<|>\"organization\"<|>\"BERT-Tagger is a baseline model treating POS tagging as a sequence labeling task using BERT as backbone.\")##\n(\"entity\"<|>\"Joint-POS\"<|>\"organization\"<|>\"Joint-POS is a baseline model that jointly handles Chinese word segmentation and POS tagging.\")##\n(\"entity\"<|>\"Lattice-LSTM\"<|>\"organization\"<|>\"Lattice-LSTM is a neural architecture combining character and word-level information via a lattice structure.\")##\n(\"entity\"<|>\"BERT-MRC\"<|>\"organization\"<|>\"BERT-MRC is a machine reading comprehension-based approach adapted for NER, proposed by Li et al.\")##\n(\"entity\"<|>\"ELMo\"<|>\"organization\"<|>\"ELMo is a contextualized word representation model used as a baseline for NER.\")##\n(\"entity\"<|>\"CVT\"<|>\"organization\"<|>\"CVT (Cross-View Training) is a semi-supervised learning method used as a baseline in the experiments.\")##\n(\"relationship\"<|>\"Table 2\"<|>\"Figure 1\"<|>\"Figure 1 visually explains concepts summarized in Table 2 regarding loss function derivatives.\"<|>8)##\n(\"relationship\"<|>\"Table 3\"<|>\"Chinese Treebank\"<|>\"Table 3 reports experimental results specifically on the Chinese Treebank datasets (CTB5 and CTB6).\"<|>9)##\n(\"relationship\"<|>\"Table 3\"<|>\"UD1.4\"<|>\"Table 3 includes performance results on the UD1.4 dataset for Chinese POS tagging.\"<|>9)##\n(\"relationship\"<|>\"Table 4\"<|>\"Wall Street Journal\"<|>\"Table 4 contains results for English POS tagging, including on the Wall Street Journal dataset.\"<|>8)##\n(\"relationship\"<|>\"Table 4\"<|>\"Ritter et al. (2011) dataset\"<|>\"Table 4 includes evaluation results on the Ritter et al. (2011) English POS dataset.\"<|>8)##\n(\"relationship\"<|>\"Table 5\"<|>\"English CoNLL 2003\"<|>\"Table 5 presents NER results on the English CoNLL 2003 dataset.\"<|>9)##\n(\"relationship\"<|>\"Table 5\"<|>\"English OntoNotes5.0\"<|>\"Table 5 includes NER performance metrics on the English OntoNotes5.0 dataset.\"<|>9)##\n(\"relationship\"<|>\"Devlin et al., 2018\"<|>\"BERT-Tagger\"<|>\"Devlin et al. (2018) introduced the BERT-Tagger baseline used throughout the experiments.\"<|>10)##\n(\"relationship\"<|>\"Shao et al., 2017\"<|>\"Joint-POS\"<|>\"Shao et al. (2017) are the creators of the Joint-POS model.\"<|>10)##\n(\"relationship\"<|>\"Zhang and Yang, 2018\"<|>\"Lattice-LSTM\"<|>\"Zhang and Yang (2018) developed the Lattice-LSTM architecture.\"<|>10)##\n(\"relationship\"<|>\"Li et al., 2019\"<|>\"BERT-MRC\"<|>\"Li et al. (2019) proposed the BERT-MRC framework adapted for NER.\"<|>10)##\n(\"relationship\"<|>\"Peters et al., 2018\"<|>\"ELMo\"<|>\"Peters et al. (2018) developed the ELMo model used as a baseline.\"<|>10)##\n(\"relationship\"<|>\"Clark et al., 2018\"<|>\"CVT\"<|>\"Clark et al. (2018) introduced the CVT method used as a baseline.\"<|>10)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "7dce9620e9cf83daa39bf40998b1f408": {
    "return": "(\"entity\"<|>\"Jiang et al., 2017\"<|>\"organization\"<|>\"A research group or authors who proposed learning a separate network to predict sample weights, cited in the context of addressing data imbalance.\")##\n(\"entity\"<|>\"Fan et al., 2018\"<|>\"organization\"<|>\"A research group or authors who, alongside Jiang et al., proposed using a separate network for sample weight prediction.\")##\n(\"entity\"<|>\"Li et al., 2015\"<|>\"organization\"<|>\"Researchers cited for their work on background-object label imbalance in object detection.\")##\n(\"entity\"<|>\"Girshick, 2015\"<|>\"person\"<|>\"Ross Girshick, a researcher known for contributions to object detection, cited here regarding label imbalance issues.\")##\n(\"entity\"<|>\"He et al., 2015\"<|>\"organization\"<|>\"A research team cited for work on object detection and data imbalance.\")##\n(\"entity\"<|>\"Girshick et al., 2013\"<|>\"organization\"<|>\"A research group including Ross Girshick that introduced hard negative mining (HNM) in object detection.\")##\n(\"entity\"<|>\"Ren et al., 2015\"<|>\"organization\"<|>\"Researchers cited for contributions to object detection and handling class imbalance.\")##\n(\"entity\"<|>\"Pang et al., 2019\"<|>\"organization\"<|>\"Researchers who proposed IoU-balanced sampling to address class imbalance in computer vision.\")##\n(\"entity\"<|>\"Chen et al., 2019\"<|>\"organization\"<|>\"Researchers who designed a ranking model with average-precision loss to mitigate class imbalance.\")##\n(\"entity\"<|>\"Sudre et al., 2017\"<|>\"organization\"<|>\"Researchers who addressed class imbalance in image segmentation using Generalized Dice Loss.\")##\n(\"entity\"<|>\"Shen et al., 2018\"<|>\"organization\"<|>\"Researchers who studied Dice-based loss for multi-class organ segmentation using abdominal CT volumes.\")##\n(\"entity\"<|>\"Kodym et al., 2018\"<|>\"organization\"<|>\"Researchers who proposed batch soft Dice loss for segmentation of organs at risk in medical images.\")##\n(\"entity\"<|>\"Shamir et al., 2019\"<|>\"organization\"<|>\"Researchers who extended the classical Dice coefficient for comparing binary ground truth with probabilistic maps.\")##\n(\"entity\"<|>\"Valverde et al., 2017\"<|>\"organization\"<|>\"Researchers cited regarding the challenges of selecting weighting factors in imbalanced classification tasks.\")##\n(\"entity\"<|>\"Sørensen, 1948\"<|>\"person\"<|>\"Thorvald Sørensen, a scientist who introduced the Sørensen–Dice coefficient for measuring set similarity.\")##\n(\"entity\"<|>\"Dice, 1945\"<|>\"person\"<|>\"Lee R. Dice, a scientist who independently developed the Dice coefficient, a statistic for set similarity also known as DSC.\")##\n(\"entity\"<|>\"Hard Negative Mining (HNM)\"<|>\"event\"<|>\"A technique introduced by Girshick et al. (2013) to address class imbalance in object detection by focusing on difficult negative examples.\")##\n(\"entity\"<|>\"IoU-balanced sampling\"<|>\"event\"<|>\"A method proposed by Pang et al. (2019) to balance sampling based on Intersection over Union (IoU) to mitigate data imbalance.\")##\n(\"entity\"<|>\"Generalized Dice Loss\"<|>\"event\"<|>\"A training objective proposed by Sudre et al. (2017) leveraging class re-balancing properties for unbalanced image segmentation tasks.\")##\n(\"entity\"<|>\"Batch Soft Dice Loss\"<|>\"event\"<|>\"A loss function introduced by Kodym et al. (2018) for training CNNs in organ-at-risk segmentation.\")##\n(\"entity\"<|>\"Dice Coefficient (DSC)\"<|>\"event\"<|>\"A similarity metric defined by Sørensen (1948) and Dice (1945), used to compare sets and adapted in machine learning for imbalanced tasks.\")##\n(\"relationship\"<|>\"Girshick et al., 2013\"<|>\"Hard Negative Mining (HNM)\"<|>\"Girshick et al. (2013) introduced the Hard Negative Mining technique in object detection.\"<|>9)##\n(\"relationship\"<|>\"Pang et al., 2019\"<|>\"IoU-balanced sampling\"<|>\"Pang et al. (2019) proposed the IoU-balanced sampling method.\"<|>10)##\n(\"relationship\"<|>\"Sudre et al., 2017\"<|>\"Generalized Dice Loss\"<|>\"Sudre et al. (2017) proposed using Generalized Dice Loss for image segmentation with class imbalance.\"<|>9)##\n(\"relationship\"<|>\"Kodym et al., 2018\"<|>\"Batch Soft Dice Loss\"<|>\"Kodym et al. (2018) introduced the Batch Soft Dice Loss for medical image segmentation.\"<|>9)##\n(\"relationship\"<|>\"Sørensen, 1948\"<|>\"Dice Coefficient (DSC)\"<|>\"Sørensen (1948) co-developed the foundational Dice Coefficient for set similarity.\"<|>8)##\n(\"relationship\"<|>\"Dice, 1945\"<|>\"Dice Coefficient (DSC)\"<|>\"Dice (1945) independently formulated the Dice Coefficient, which bears his name.\"<|>8)##\n(\"relationship\"<|>\"Girshick, 2015\"<|>\"Girshick et al., 2013\"<|>\"Ross Girshick is a key author in both the 2013 and 2015 works on object detection and class imbalance.\"<|>7)##\n(\"relationship\"<|>\"Chen et al., 2019\"<|>\"Data Imbalance Issue in Computer Vision\"<|>\"Chen et al. (2019) contributed a novel ranking model to address the data imbalance issue in computer vision tasks.\"<|>8)##\n(\"relationship\"<|>\"Shen et al., 2018\"<|>\"Abdominal CT Volumes\"<|>\"Shen et al. (2018) used a dataset of abdominal CT volumes in their study on Dice-based loss; 'Abdominal CT Volumes' represents a geo/medical imaging context, interpreted as a geo-related data source.\"<|>6)##\n(\"relationship\"<|>\"Shamir et al., 2019\"<|>\"Dice Coefficient (DSC)\"<|>\"Shamir et al. (2019) extended the classical Dice coefficient definition for probabilistic map comparison.\"<|>8)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "619d0d353d443eb297cea780a555d5ca": {
    "return": "(\"entity\"<|>\"MRC\"<|>\"event\"<|>\"MRC (Machine Reading Comprehension) is a natural language processing task formalized as predicting starting and ending indexes based on a query and context, characterized by extreme data imbalance with very few positive tokens.\")##\n(\"entity\"<|>\"Sørensen–Dice coefficient\"<|>\"event\"<|>\"The Sørensen–Dice coefficient, or dice loss, is a training objective proposed to address data imbalance in NLP tasks by equally weighting false positives and false negatives, serving as the harmonic mean of precision and recall.\")##\n(\"entity\"<|>\"Tversky index\"<|>\"event\"<|>\"The Tversky index is an extension of dice loss that introduces a weight parameter to trade off precision and recall, approximating the Fβ score and offering greater flexibility for handling imbalanced datasets.\")##\n(\"entity\"<|>\"Focal loss\"<|>\"event\"<|>\"Focal loss is a dynamic weighting strategy originally from computer vision that deemphasizes confident (easy) examples during training by assigning weights proportional to (1 − p), helping models focus on hard-negative examples.\")##\n(\"entity\"<|>\"Data Resampling\"<|>\"event\"<|>\"Data Resampling refers to a set of techniques—including importance sampling, boosting, hard example mining, and oversampling—used to address class imbalance by adjusting example weights or modifying data distributions.\")##\n(\"entity\"<|>\"Self-paced learning\"<|>\"event\"<|>\"Self-paced learning is a training approach where example weights and model parameters are jointly optimized to prioritize easier examples early in training, gradually incorporating harder ones.\")##\n(\"entity\"<|>\"Object detection\"<|>\"event\"<|>\"Object detection is a computer vision task noted for severe background-object label imbalance, which has motivated extensive research into handling data imbalance.\")##\n(\"entity\"<|>\"Section 2\"<|>\"event\"<|>\"Section 2 of the paper presents related work on handling data imbalance, including data resampling and approaches from computer vision.\")##\n(\"entity\"<|>\"Section 3\"<|>\"event\"<|>\"Section 3 describes the proposed loss functions, including dice loss and Tversky index, designed to mitigate data imbalance issues in NLP.\")##\n(\"entity\"<|>\"Section 4\"<|>\"event\"<|>\"Section 4 presents experimental results demonstrating performance improvements on data-imbalanced NLP tasks using the proposed methods.\")##\n(\"entity\"<|>\"Section 5\"<|>\"event\"<|>\"Section 5 contains ablation studies evaluating the contribution of individual components of the proposed approach.\")##\n(\"entity\"<|>\"Section 6\"<|>\"event\"<|>\"Section 6 provides a brief conclusion summarizing the paper’s contributions and findings.\")##\n(\"entity\"<|>\"Kahn and Marshall\"<|>\"person\"<|>\"Kahn and Marshall (1953) pioneered importance sampling, a method for assigning weights to samples to alter data distribution during training.\")##\n(\"entity\"<|>\"Lin et al.\"<|>\"person\"<|>\"Lin et al. (2017) introduced focal loss, a soft weighting scheme that emphasizes harder examples during training, originally in computer vision but influential in NLP.\")##\n(\"entity\"<|>\"Kumar et al.\"<|>\"person\"<|>\"Kumar et al. (2010) developed self-paced learning, a method that jointly optimizes model parameters and example weights to learn from easy to hard examples.\")##\n(\"entity\"<|>\"Chawla et al.\"<|>\"person\"<|>\"Chawla et al. (2002) contributed to data imbalance solutions through oversampling techniques to balance class distributions.\")##\n(\"entity\"<|>\"Girshick\"<|>\"person\"<|>\"Girshick (2015) is cited in the context of object detection research addressing background-object label imbalance.\")##\n(\"entity\"<|>\"He et al.\"<|>\"person\"<|>\"He et al. (2015) are referenced for their work on object detection and handling label imbalance in computer vision.\")##\n(\"entity\"<|>\"Ren et al.\"<|>\"person\"<|>\"Ren et al. are mentioned in relation to object detection research tackling data imbalance challenges.\")##\n(\"entity\"<|>\"Malisiewicz et al.\"<|>\"person\"<|>\"Malisiewicz et al. (2011) proposed hard example mining, which downsamples the majority class and focuses on difficult examples.\")##\n(\"entity\"<|>\"Jiang et al.\"<|>\"person\"<|>\"Jiang et al. (2017) proposed learning a separate network to predict sample weights for handling data imbalance.\")##\n(\"entity\"<|>\"Fan et al.\"<|>\"person\"<|>\"Fan et al. (2018) also worked on predicting sample weights via a separate network to address imbalanced training data.\")##\n(\"entity\"<|>\"Sorensen\"<|>\"person\"<|>\"Sorensen (1948) introduced the Sørensen–Dice coefficient, foundational to dice loss used in imbalanced classification.\")##\n(\"entity\"<|>\"Tversky\"<|>\"person\"<|>\"Tversky (1977) proposed the Tversky index, a generalization of dice loss that allows flexible trade-offs between precision and recall.\")##\n(\"relationship\"<|>\"MRC\"<|>\"Data Resampling\"<|>\"MRC suffers from severe data imbalance, which motivates the application of data resampling techniques to improve model performance.\"<|>8)##\n(\"relationship\"<|>\"MRC\"<|>\"Sørensen–Dice coefficient\"<|>\"The Sørensen–Dice coefficient is proposed specifically to address the training-test discrepancy in MRC caused by extreme label imbalance.\"<|>9)##\n(\"relationship\"<|>\"MRC\"<|>\"Tversky index\"<|>\"The Tversky index is used as a more flexible alternative to dice loss for handling MRC's data imbalance by approximating Fβ scores.\"<|>8)##\n(\"relationship\"<|>\"MRC\"<|>\"Focal loss\"<|>\"Focal loss inspires the dynamic weighting strategy applied to MRC to reduce the dominance of easy-negative examples.\"<|>7)##\n(\"relationship\"<|>\"Sørensen–Dice coefficient\"<|>\"Sorensen\"<|>\"Sorensen (1948) originally introduced the Sørensen–Dice coefficient, forming the theoretical basis for dice loss.\"<|>10)##\n(\"relationship\"<|>\"Tversky index\"<|>\"Tversky\"<|>\"Tversky (1977) developed the Tversky index, which extends the Dice coefficient with adjustable weights.\"<|>10)##\n(\"relationship\"<|>\"Focal loss\"<|>\"Lin et al.\"<|>\"Lin et al. (2017) originated focal loss in computer vision, which inspired the dynamic weighting approach in this NLP context.\"<|>9)##\n(\"relationship\"<|>\"Data Resampling\"<|>\"Kahn and Marshall\"<|>\"Kahn and Marshall (1953) laid early groundwork for data resampling via importance sampling.\"<|>7)##\n(\"relationship\"<|>\"Data Resampling\"<|>\"Chawla et al.\"<|>\"Chawla et al. (2002) contributed oversampling methods to data resampling for class imbalance.\"<|>7)##\n(\"relationship\"<|>\"Data Resampling\"<|>\"Malisiewicz et al.\"<|>\"Malisiewicz et al. (2011) advanced data resampling through hard example mining.\"<|>8)##\n(\"relationship\"<|>\"Self-paced learning\"<|>\"Kumar et al.\"<|>\"Kumar et al. (2010) introduced self-paced learning as a dynamic data resampling strategy.\"<|>9)##\n(\"relationship\"<|>\"Object detection\"<|>\"Girshick\"<|>\"Girshick (2015) conducted influential work on object detection where label imbalance is a core challenge.\"<|>8)##\n(\"relationship\"<|>\"Object detection\"<|>\"He et al.\"<|>\"He et al. (2015) addressed data imbalance in object detection, contributing to the field’s understanding of the issue.\"<|>8)##\n(\"relationship\"<|>\"Object detection\"<|>\"Ren et al.\"<|>\"Ren et al. are associated with object detection research that tackles background-object imbalance.\"<|>7)##\n(\"relationship\"<|>\"Section 2\"<|>\"Data Resampling\"<|>\"Section 2 reviews data resampling as a key category of related work for handling imbalance.\"<|>9)##\n(\"relationship\"<|>\"Section 2\"<|>\"Object detection\"<|>\"Section 2 discusses object detection as a domain where data imbalance has been extensively studied.\"<|>8)##\n(\"relationship\"<|>\"Section 3\"<|>\"Sørensen–Dice coefficient\"<|>\"Section 3 details the use of Sørensen–Dice coefficient as a proposed loss function.\"<|>10)##\n(\"relationship\"<|>\"Section 3\"<|>\"Tversky index\"<|>\"Section 3 describes the Tversky index as an alternative loss function for imbalanced NLP tasks.\"<|>10)##\n(\"relationship\"<|>\"Section 4\"<|>\"MRC\"<|>\"Section 4 presents experimental results on MRC and other NLP tasks using the proposed methods.\"<|>9)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "9d4ad539499a938fa1e5c260af4bc5f8": {
    "return": "(\"entity\"<|>\"Table 5\"<|>\"event\"<|>\"Table 5 presents experimental results on Named Entity Recognition (NER) datasets, comparing the performance of various models including DSC and BERT-MRC.\")##\n(\"entity\"<|>\"Table 6\"<|>\"event\"<|>\"Table 6 displays experimental results for the Machine Reading Comprehension (MRC) task across three datasets: SQuAD v1.1, SQuAD v2.0, and Quoref.\")##\n(\"entity\"<|>\"Peters et al. (2018)\"<|>\"organization\"<|>\"Peters et al. (2018) is a research group or author team that introduced ELMo, a tagging model with pretraining used as a baseline in NER experiments.\")##\n(\"entity\"<|>\"Zhang and Yang (2018)\"<|>\"organization\"<|>\"Zhang and Yang (2018) developed the Lattice-LSTM model, which constructs a word-character lattice and is applied only to Chinese datasets.\")##\n(\"entity\"<|>\"Clark et al. (2018)\"<|>\"organization\"<|>\"Clark et al. (2018) proposed Cross-View Training (CVT) to enhance representations in a Bi-LSTM encoder.\")##\n(\"entity\"<|>\"Devlin et al. (2018)\"<|>\"organization\"<|>\"Devlin et al. (2018) introduced BERT, used here both as BERT-Tagger for NER and as a baseline model for MRC.\")##\n(\"entity\"<|>\"Wu et al. (2019)\"<|>\"organization\"<|>\"Wu et al. (2019) developed Glyce-BERT, which integrates Chinese glyph information with BERT pretraining.\")##\n(\"entity\"<|>\"Li et al. (2019)\"<|>\"organization\"<|>\"Li et al. (2019) formulated NER as a machine reading comprehension task using BERT-MRC and achieved state-of-the-art results on multiple benchmarks.\")##\n(\"entity\"<|>\"Yu et al. (2018b)\"<|>\"organization\"<|>\"Yu et al. (2018b) developed QANet, an MRC model based on convolutions and self-attention mechanisms.\")##\n(\"entity\"<|>\"Yang et al. (2019)\"<|>\"organization\"<|>\"Yang et al. (2019) proposed XLNet, a generalized autoregressive pretraining method used as a strong baseline in MRC tasks.\")##\n(\"entity\"<|>\"Dasigi et al. (2019)\"<|>\"organization\"<|>\"Dasigi et al. (2019) introduced the Quoref dataset, used in machine reading comprehension evaluations.\")##\n(\"entity\"<|>\"Rajpurkar et al. (2016, 2018)\"<|>\"organization\"<|>\"Rajpurkar et al. (2016, 2018) are the creators of the SQuAD v1.1 and SQuAD v2.0 machine reading comprehension datasets.\")##\n(\"entity\"<|>\"Seo et al. (2016)\"<|>\"organization\"<|>\"Seo et al. (2016) established standard protocols for the machine reading comprehension task, focusing on predicting start and end indexes of answer spans.\")##\n(\"entity\"<|>\"CoNLL2003\"<|>\"geo\"<|>\"CoNLL2003 is a widely used Named Entity Recognition dataset, often associated with English-language text from news sources; treated here as a benchmark geo-tagged corpus.\")##\n(\"entity\"<|>\"OntoNotes5.0\"<|>\"geo\"<|>\"OntoNotes5.0 is a large-scale multilingual NER benchmark dataset covering multiple domains and languages, used for evaluating model performance.\")##\n(\"entity\"<|>\"MSRA\"<|>\"geo\"<|>\"MSRA is a Chinese NER dataset provided by Microsoft Research Asia, used as a benchmark for evaluating named entity recognition systems.\")##\n(\"entity\"<|>\"OntoNotes4.0\"<|>\"geo\"<|>\"OntoNotes4.0 is an earlier version of the OntoNotes NER benchmark dataset, used for evaluating entity recognition across diverse text genres.\")##\n(\"entity\"<|>\"SQuAD v1.1\"<|>\"geo\"<|>\"SQuAD v1.1 is a machine reading comprehension dataset consisting of questions posed by crowdworkers on Wikipedia passages, used as a standard evaluation benchmark.\")##\n(\"entity\"<|>\"SQuAD v2.0\"<|>\"geo\"<|>\"SQuAD v2.0 extends SQuAD v1.1 by including unanswerable questions, increasing the challenge of the machine reading comprehension task.\")##\n(\"entity\"<|>\"Quoref\"<|>\"geo\"<|>\"Quoref is a machine reading comprehension dataset focused on questions requiring coreference resolution, introduced by Dasigi et al. (2019).\")##\n(\"relationship\"<|>\"Table 5\"<|>\"CoNLL2003\"<|>\"Table 5 reports NER model performance results on the CoNLL2003 dataset.\"<|>9)##\n(\"relationship\"<|>\"Table 5\"<|>\"OntoNotes5.0\"<|>\"Table 5 includes experimental results for models evaluated on the OntoNotes5.0 dataset.\"<|>9)##\n(\"relationship\"<|>\"Table 5\"<|>\"MSRA\"<|>\"Table 5 presents model performance metrics on the MSRA NER dataset.\"<|>9)##\n(\"relationship\"<|>\"Table 5\"<|>\"OntoNotes4.0\"<|>\"Table 5 shows results for NER models tested on the OntoNotes4.0 dataset.\"<|>9)##\n(\"relationship\"<|>\"Table 6\"<|>\"SQuAD v1.1\"<|>\"Table 6 displays EM and F1 scores for MRC models on the SQuAD v1.1 dataset.\"<|>9)##\n(\"relationship\"<|>\"Table 6\"<|>\"SQuAD v2.0\"<|>\"Table 6 includes evaluation results for MRC models on the SQuAD v2.0 dataset.\"<|>9)##\n(\"relationship\"<|>\"Table 6\"<|>\"Quoref\"<|>\"Table 6 reports performance metrics on the Quoref dataset for various MRC models.\"<|>9)##\n(\"relationship\"<|>\"Li et al. (2019)\"<|>\"BERT-MRC\"<|>\"Li et al. (2019) developed the BERT-MRC model, which formulates NER as a machine reading comprehension task.\"<|>10)##\n(\"relationship\"<|>\"Devlin et al. (2018)\"<|>\"BERT\"<|>\"Devlin et al. (2018) introduced the BERT model, which is used as a baseline in both NER and MRC tasks.\"<|>10)##\n(\"relationship\"<|>\"Yang et al. (2019)\"<|>\"XLNet\"<|>\"Yang et al. (2019) proposed the XLNet model, which is evaluated in Table 6 for MRC performance.\"<|>10)##\n(\"relationship\"<|>\"Yu et al. (2018b)\"<|>\"QANet\"<|>\"Yu et al. (2018b) developed the QANet model, whose results are reported in Table 6 for MRC benchmarks.\"<|>10)##\n(\"relationship\"<|>\"Rajpurkar et al. (2016, 2018)\"<|>\"SQuAD v1.1\"<|>\"Rajpurkar et al. created the SQuAD v1.1 dataset, which is used in the MRC evaluation.\"<|>8)##\n(\"relationship\"<|>\"Rajpurkar et al. (2016, 2018)\"<|>\"SQuAD v2.0\"<|>\"Rajpurkar et al. are the authors of the SQuAD v2.0 dataset, included in the MRC experiments.\"<|>8)##\n(\"relationship\"<|>\"Dasigi et al. (2019)\"<|>\"Quoref\"<|>\"Dasigi et al. (2019) introduced the Quoref dataset, which appears in Table 6’s MRC results.\"<|>9)##\n(\"relationship\"<|>\"Seo et al. (2016)\"<|>\"Machine Reading Comprehension\"<|>\"Seo et al. (2016) defined standard protocols for the MRC task followed in this study.\"<|>7)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "45ad4f503d50c1daccf18f2f327ba953": {
    "return": "(\"entity\"<|>\"Xiaoya Li\"<|>\"person\"<|>\"Xiaoya Li is a researcher and co-author of the paper 'Dice Loss for Data-imbalanced NLP Tasks', affiliated with Shannon.AI.\")##\n(\"entity\"<|>\"Xiaofei Sun\"<|>\"person\"<|>\"Xiaofei Sun is a researcher and co-author of the paper, affiliated with Shannon.AI.\")##\n(\"entity\"<|>\"Yuxian Meng\"<|>\"person\"<|>\"Yuxian Meng is a researcher and co-author of the paper, affiliated with Shannon.AI.\")##\n(\"entity\"<|>\"Junjun Liang\"<|>\"person\"<|>\"Junjun Liang is a researcher and co-author of the paper, affiliated with Shannon.AI.\")##\n(\"entity\"<|>\"Fei Wu\"<|>\"person\"<|>\"Fei Wu is a researcher and co-author of the paper, affiliated with the Department of Computer Science and Technology, Zhejiang University.\")##\n(\"entity\"<|>\"Jiwei Li\"<|>\"person\"<|>\"Jiwei Li is a researcher and co-author of the paper, affiliated with Shannon.AI.\")##\n(\"entity\"<|>\"Zhejiang University\"<|>\"organization\"<|>\"Zhejiang University is an academic institution where Fei Wu is affiliated, specifically through its Department of Computer Science and Technology.\")##\n(\"entity\"<|>\"Shannon.AI\"<|>\"organization\"<|>\"Shannon.AI is a research organization where several authors (Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, and Jiwei Li) are affiliated.\")##\n(\"entity\"<|>\"CoNLL03\"<|>\"event\"<|>\"CoNLL03 is a Named Entity Recognition (NER) benchmark dataset used in the paper to evaluate the proposed dice loss method.\")##\n(\"entity\"<|>\"OntoNotes5.0\"<|>\"event\"<|>\"OntoNotes5.0 is a large-scale NER dataset referenced in the paper, noted for its severe data imbalance with a negative-to-positive ratio of 8.18.\")##\n(\"entity\"<|>\"SQuAD 1.1\"<|>\"event\"<|>\"SQuAD 1.1 is a machine reading comprehension (MRC) dataset introduced by Rajpurkar et al. (2016), used in the paper as an example of a highly imbalanced NLP task with a ratio of 55.9.\")##\n(\"entity\"<|>\"SQuAD 2.0\"<|>\"event\"<|>\"SQuAD 2.0 is an MRC dataset introduced by Rajpurkar et al. (2018), exhibiting extreme data imbalance with a negative-to-positive ratio of 82.0.\")##\n(\"entity\"<|>\"QUOREF\"<|>\"event\"<|>\"QUOREF is an MRC dataset introduced by Dasigi et al. (2019), cited in the paper as having the highest imbalance ratio of 169.\")##\n(\"entity\"<|>\"CTB5\"<|>\"event\"<|>\"CTB5 is a part-of-speech tagging benchmark on which the authors achieved state-of-the-art (SOTA) results using their proposed dice loss.\")##\n(\"entity\"<|>\"CTB6\"<|>\"event\"<|>\"CTB6 is a part-of-speech tagging benchmark where the authors reported SOTA performance with their method.\")##\n(\"entity\"<|>\"UD1.4\"<|>\"event\"<|>\"UD1.4 refers to Universal Dependencies version 1.4, a POS tagging dataset on which the authors achieved SOTA results.\")##\n(\"entity\"<|>\"MSRA\"<|>\"event\"<|>\"MSRA is a named entity recognition dataset evaluated in the paper, where the proposed method achieved competitive or better results.\")##\n(\"entity\"<|>\"OntoNotes4.0\"<|>\"event\"<|>\"OntoNotes4.0 is another NER dataset included in the evaluation, showing strong performance with the proposed approach.\")##\n(\"entity\"<|>\"Table 1\"<|>\"event\"<|>\"Table 1 presents statistics on the number of positive and negative examples and their ratios across various data-imbalanced NLP tasks discussed in the paper.\")##\n(\"relationship\"<|>\"Xiaoya Li\"<|>\"Shannon.AI\"<|>\"Xiaoya Li is affiliated with Shannon.AI as indicated by the email domain and author footnote.\"<|>9)##\n(\"relationship\"<|>\"Xiaofei Sun\"<|>\"Shannon.AI\"<|>\"Xiaofei Sun is affiliated with Shannon.AI as indicated by the email domain and author footnote.\"<|>9)##\n(\"relationship\"<|>\"Yuxian Meng\"<|>\"Shannon.AI\"<|>\"Yuxian Meng is affiliated with Shannon.AI as indicated by the email domain and author footnote.\"<|>9)##\n(\"relationship\"<|>\"Junjun Liang\"<|>\"Shannon.AI\"<|>\"Junjun Liang is affiliated with Shannon.AI as indicated by the email domain and author footnote.\"<|>9)##\n(\"relationship\"<|>\"Jiwei Li\"<|>\"Shannon.AI\"<|>\"Jiwei Li is affiliated with Shannon.AI as indicated by the email domain and author footnote.\"<|>9)##\n(\"relationship\"<|>\"Fei Wu\"<|>\"Zhejiang University\"<|>\"Fei Wu is affiliated with the Department of Computer Science and Technology at Zhejiang University, as shown in the author list.\"<|>9)##\n(\"relationship\"<|>\"Shannon.AI\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"Shannon.AI is the primary research organization behind the development and publication of the proposed dice loss methodology.\"<|>8)##\n(\"relationship\"<|>\"Zhejiang University\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"Zhejiang University contributed to the research through Fei Wu’s involvement in the paper.\"<|>7)##\n(\"relationship\"<|>\"Table 1\"<|>\"CoNLL03\"<|>\"Table 1 includes quantitative data about the CoNLL03 dataset, specifically its negative-positive example ratio.\"<|>8)##\n(\"relationship\"<|>\"Table 1\"<|>\"OntoNotes5.0\"<|>\"Table 1 provides statistics on the data imbalance in the OntoNotes5.0 dataset.\"<|>8)##\n(\"relationship\"<|>\"Table 1\"<|>\"SQuAD 1.1\"<|>\"Table 1 lists the number of negative and positive examples and the ratio for SQuAD 1.1.\"<|>8)##\n(\"relationship\"<|>\"Table 1\"<|>\"SQuAD 2.0\"<|>\"Table 1 includes data imbalance metrics for the SQuAD 2.0 dataset.\"<|>8)##\n(\"relationship\"<|>\"Table 1\"<|>\"QUOREF\"<|>\"Table 1 reports the extreme imbalance ratio (169) for the QUOREF dataset.\"<|>8)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"CTB5\"<|>\"The paper reports achieving state-of-the-art results on CTB5 using the proposed dice loss.\"<|>9)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"CTB6\"<|>\"The paper reports achieving state-of-the-art results on CTB6 using the proposed dice loss.\"<|>9)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"UD1.4\"<|>\"The paper reports achieving state-of-the-art results on UD1.4 using the proposed dice loss.\"<|>9)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"CoNLL03\"<|>\"The proposed method achieves competitive or better results on the CoNLL03 NER task.\"<|>9)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"OntoNotes5.0\"<|>\"The proposed method achieves competitive or better results on the OntoNotes5.0 NER task.\"<|>9)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"MSRA\"<|>\"The proposed method achieves competitive or better results on the MSRA NER task.\"<|>8)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"OntoNotes4.0\"<|>\"The proposed method achieves competitive or better results on the OntoNotes4.0 NER task.\"<|>8)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"SQuAD 1.1\"<|>\"SQuAD 1.1 is cited as an example of a highly imbalanced MRC task addressed by the proposed methodology.\"<|>8)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"SQuAD 2.0\"<|>\"SQuAD 2.0 is cited as an example of a highly imbalanced MRC task addressed by the proposed methodology.\"<|>8)##\n(\"relationship\"<|>\"Dice Loss for Data-imbalanced NLP Tasks\"<|>\"QUOREF\"<|>\"QUOREF is cited as an example of an extremely imbalanced MRC task relevant to the paper's focus.\"<|>8)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "c9af80976132705f239a9ecd41a90593": {
    "return": "(\"entity\"<|>\"BERT-Tagger (Devlin et al., 2018)\"<|>\"organization\"<|>\"A named entity recognition model introduced by Devlin et al. in 2018, used as a baseline in the experiments across multiple datasets including CoNLL2003, MSRA, and OntoNotes.\")##\n(\"entity\"<|>\"BERT-MRC (Li et al., 2019)\"<|>\"organization\"<|>\"A state-of-the-art named entity recognition model proposed by Li et al. in 2019 that frames NER as a machine reading comprehension (MRC) task; serves as the backbone for the current implementation.\")##\n(\"entity\"<|>\"Lattice-LSTM (Zhang and Yang, 2018)\"<|>\"organization\"<|>\"A Chinese-specific NER model developed by Zhang and Yang in 2018 that constructs a word-character lattice structure; used only on Chinese datasets like MSRA and Chinese OntoNotes 4.0.\")##\n(\"entity\"<|>\"Glyce-BERT (Wu et al., 2019)\"<|>\"organization\"<|>\"A BERT-based model adapted for Chinese text by Wu et al. in 2019, incorporating glyph information; evaluated on Chinese NER benchmarks.\")##\n(\"entity\"<|>\"ELMo (Peters et al., 2018)\"<|>\"organization\"<|>\"A pretraining-based tagging model introduced by Peters et al. in 2018, used as a baseline for comparison in named entity recognition tasks.\")##\n(\"entity\"<|>\"CVT (Clark et al., 2018)\"<|>\"organization\"<|>\"A method using Cross-View Training to enhance Bi-LSTM encoder representations, proposed by Clark et al. in 2018, included among the baselines.\")##\n(\"entity\"<|>\"Devlin\"<|>\"person\"<|>\"One of the authors of the BERT-Tagger model (2018), a foundational contributor to the BERT architecture used in NER.\")##\n(\"entity\"<|>\"Li\"<|>\"person\"<|>\"Lead author of the BERT-MRC model (2019), whose work forms the backbone of the current NER implementation.\")##\n(\"entity\"<|>\"Zhang\"<|>\"person\"<|>\"Co-author (with Yang) of the Lattice-LSTM model (2018) for Chinese NER.\")##\n(\"entity\"<|>\"Yang\"<|>\"person\"<|>\"Co-author (with Zhang) of the Lattice-LSTM model (2018) designed for Chinese named entity recognition.\")##\n(\"entity\"<|>\"Wu\"<|>\"person\"<|>\"Lead author of the Glyce-BERT model (2019), which integrates glyph embeddings into BERT for Chinese NER.\")##\n(\"entity\"<|>\"Peters\"<|>\"person\"<|>\"Lead author of the ELMo model (2018), a contextualized word representation system used in NER baselines.\")##\n(\"entity\"<|>\"Clark\"<|>\"person\"<|>\"Lead author of the CVT (Cross-View Training) approach (2018) for improving sequence modeling in NER.\")##\n(\"entity\"<|>\"OntoNotes 4.0\"<|>\"event\"<|>\"A benchmark dataset for named entity recognition, released by Pradhan et al. in 2011, used in the evaluation of NER models.\")##\n(\"entity\"<|>\"OntoNotes 5.0\"<|>\"event\"<|>\"An updated version of the OntoNotes dataset, released by Pradhan et al. in 2013, serving as a standard NER evaluation benchmark.\")##\n(\"entity\"<|>\"MSRA\"<|>\"event\"<|>\"A Chinese named entity recognition dataset created by Levow in 2006, commonly used for evaluating Chinese NER systems.\")##\n(\"entity\"<|>\"CoNLL2003\"<|>\"event\"<|>\"A widely used English-language NER benchmark dataset introduced by Sang and Meulder in 2003.\")##\n(\"entity\"<|>\"Chinese MSRA\"<|>\"geo\"<|>\"Refers to the Chinese variant or context of the MSRA dataset, indicating its linguistic and geographic association with China.\")##\n(\"entity\"<|>\"Chinese OntoNotes 4.0\"<|>\"geo\"<|>\"The Chinese-language portion of the OntoNotes 4.0 corpus, geographically and linguistically tied to Chinese-speaking regions.\")##\n(\"relationship\"<|>\"BERT-MRC (Li et al., 2019)\"<|>\"Li\"<|>\"Li is the lead author of the BERT-MRC model published in 2019.\"<|>10)##\n(\"relationship\"<|>\"BERT-Tagger (Devlin et al., 2018)\"<|>\"Devlin\"<|>\"Devlin is a key author of the BERT-Tagger model from 2018.\"<|>10)##\n(\"relationship\"<|>\"Lattice-LSTM (Zhang and Yang, 2018)\"<|>\"Zhang\"<|>\"Zhang co-developed the Lattice-LSTM model in 2018.\"<|>9)##\n(\"relationship\"<|>\"Lattice-LSTM (Zhang and Yang, 2018)\"<|>\"Yang\"<|>\"Yang co-developed the Lattice-LSTM model in 2018.\"<|>9)##\n(\"relationship\"<|>\"Glyce-BERT (Wu et al., 2019)\"<|>\"Wu\"<|>\"Wu is the lead author of the Glyce-BERT model from 2019.\"<|>10)##\n(\"relationship\"<|>\"ELMo (Peters et al., 2018)\"<|>\"Peters\"<|>\"Peters is the lead author of the ELMo model from 2018.\"<|>10)##\n(\"relationship\"<|>\"CVT (Clark et al., 2018)\"<|>\"Clark\"<|>\"Clark is the lead author of the CVT method from 2018.\"<|>10)##\n(\"relationship\"<|>\"BERT-MRC (Li et al., 2019)\"<|>\"OntoNotes 4.0\"<|>\"The BERT-MRC model is evaluated on the OntoNotes 4.0 dataset as part of the experimental setup.\"<|>8)##\n(\"relationship\"<|>\"BERT-MRC (Li et al., 2019)\"<|>\"MSRA\"<|>\"The BERT-MRC model is tested on the MSRA dataset, particularly in its Chinese form.\"<|>8)##\n(\"relationship\"<|>\"BERT-MRC (Li et al., 2019)\"<|>\"CoNLL2003\"<|>\"The BERT-MRC model is applied to the CoNLL2003 benchmark for English NER evaluation.\"<|>8)##\n(\"relationship\"<|>\"Lattice-LSTM (Zhang and Yang, 2018)\"<|>\"Chinese MSRA\"<|>\"Lattice-LSTM is specifically designed for and evaluated on Chinese datasets like Chinese MSRA.\"<|>9)##\n(\"relationship\"<|>\"Lattice-LSTM (Zhang and Yang, 2018)\"<|>\"Chinese OntoNotes 4.0\"<|>\"Lattice-LSTM is applied to the Chinese portion of OntoNotes 4.0.\"<|>9)##\n(\"relationship\"<|>\"MSRA\"<|>\"Chinese MSRA\"<|>\"Chinese MSRA refers to the Chinese-language version of the MSRA dataset, indicating a direct linguistic and contextual relationship.\"<|>10)##\n(\"relationship\"<|>\"OntoNotes 4.0\"<|>\"Chinese OntoNotes 4.0\"<|>\"Chinese OntoNotes 4.0 is the Chinese subset of the OntoNotes 4.0 corpus.\"<|>10)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "0864a1e8389b59622b7abc388ff8826c": {
    "return": "(\"entity\"<|>\"CTB5\"<|>\"organization\"<|>\"CTB5 (Chinese Treebank 5.0) is a Chinese dataset used for part-of-speech tagging and parsing, containing text extracted from newswire sources including Xinhua, the Information Services Department of HKSAR, and Sinorama Magazine.\")##\n(\"entity\"<|>\"CTB6\"<|>\"organization\"<|>\"CTB6 (Chinese Treebank 6.0) is an extended version of CTB5, containing a larger collection of words, characters, and sentences for linguistic annotation tasks.\")##\n(\"entity\"<|>\"UD1.4\"<|>\"organization\"<|>\"UD1.4 refers to version 1.4 of Universal Dependencies, a framework for consistent grammatical annotation across languages, used here specifically for Chinese part-of-speech tagging.\")##\n(\"entity\"<|>\"CoNLL2003\"<|>\"organization\"<|>\"CoNLL2003 is an English benchmark dataset for named entity recognition with four entity types: Location, Organization, Person, and Miscellaneous.\")##\n(\"entity\"<|>\"OntoNotes5.0\"<|>\"organization\"<|>\"English OntoNotes5.0 is a dataset comprising texts from diverse sources with 18 entity types, commonly used in NER tasks and evaluated using the CoNLL2012 shared task split.\")##\n(\"entity\"<|>\"MSRA\"<|>\"organization\"<|>\"MSRA is a Chinese benchmark dataset for named entity recognition containing three entity types, sourced from news articles, with no original development set provided.\")##\n(\"entity\"<|>\"OntoNotes4.0\"<|>\"organization\"<|>\"Chinese OntoNotes4.0 is a named entity recognition dataset consisting of news-domain texts with 18 entity types, used with a data split following Wu et al. (2019).\")##\n(\"entity\"<|>\"SQuAD v1.1\"<|>\"organization\"<|>\"SQuAD v1.1 is a widely used machine reading comprehension dataset containing 100K crowdsourced question-answer pairs based on Wikipedia passages.\")##\n(\"entity\"<|>\"SQuAD v2.0\"<|>\"organization\"<|>\"SQuAD v2.0 extends SQuAD v1.1 by including questions that may not have answers in the given passage, testing the ability of models to recognize unanswerable questions.\")##\n(\"entity\"<|>\"Quoref\"<|>\"organization\"<|>\"Quoref is a question answering dataset designed to evaluate coreferential reasoning in reading comprehension systems, containing 24K questions over 4.7K Wikipedia paragraphs.\")##\n(\"entity\"<|>\"MRPC\"<|>\"organization\"<|>\"MRPC (Microsoft Research Paraphrase Corpus) is a paraphrase identification dataset of sentence pairs from online news sources, annotated for semantic equivalence with imbalanced class distribution.\")##\n(\"entity\"<|>\"QQP\"<|>\"organization\"<|>\"QQP (Quora Question Pairs) is a paraphrase identification dataset consisting of over 400,000 question pairs from Quora, labeled for semantic equivalence with imbalanced classes.\")##\n(\"entity\"<|>\"Xinhua\"<|>\"organization\"<|>\"Xinhua is a Chinese news agency whose articles (from 1994–1998) are included in the CTB5 dataset.\")##\n(\"entity\"<|>\"Information Services Department of HKSAR\"<|>\"organization\"<|>\"The Information Services Department of the Hong Kong Special Administrative Region (HKSAR) contributed 55 news articles from 1997 to the CTB5 dataset.\")##\n(\"entity\"<|>\"Sinorama Magazine\"<|>\"organization\"<|>\"Sinorama Magazine is a publication that contributed 132 articles (from 1996–1998 and 2000–2001) to the CTB5 dataset.\")##\n(\"entity\"<|>\"Ma and Hovy\"<|>\"person\"<|>\"Ma and Hovy are researchers cited for their data processing protocols used in the CoNLL2003 NER experiments.\")##\n(\"entity\"<|>\"Wu et al.\"<|>\"person\"<|>\"Wu et al. are researchers whose data split methodology for Chinese OntoNotes4.0 was adopted in this work.\")##\n(\"entity\"<|>\"arXiv:1805.02023\"<|>\"event\"<|>\"arXiv:1805.02023 is a 2018 preprint paper titled 'Chinese NER using Lattice LSTM,' describing experiments on multiple NLP benchmarks.\")##\n(\"relationship\"<|>\"CTB5\"<|>\"Xinhua\"<|>\"CTB5 includes 698 articles sourced from Xinhua news agency.\"<|>8)##\n(\"relationship\"<|>\"CTB5\"<|>\"Information Services Department of HKSAR\"<|>\"CTB5 contains 55 articles from the Information Services Department of HKSAR.\"<|>8)##\n(\"relationship\"<|>\"CTB5\"<|>\"Sinorama Magazine\"<|>\"CTB5 incorporates 132 articles from Sinorama Magazine.\"<|>8)##\n(\"relationship\"<|>\"CTB6\"<|>\"CTB5\"<|>\"CTB6 is an extension of the CTB5 dataset, indicating a direct lineage and expansion.\"<|>9)##\n(\"relationship\"<|>\"UD1.4\"<|>\"Universal Dependencies\"<|>\"UD1.4 is a specific version of the Universal Dependencies framework used for Chinese POS tagging.\"<|>10)##\n(\"relationship\"<|>\"CoNLL2003\"<|>\"Ma and Hovy\"<|>\"The CoNLL2003 experiments followed data processing protocols established by Ma and Hovy (2016).\"<|>7)##\n(\"relationship\"<|>\"Chinese OntoNotes4.0\"<|>\"Wu et al.\"<|>\"The data split for Chinese OntoNotes4.0 follows the approach used by Wu et al. (2019).\"<|>8)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"CTB5\"<|>\"The arXiv paper uses CTB5 as one of its benchmark datasets for linguistic experiments.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"CTB6\"<|>\"The arXiv paper includes CTB6 in its experimental evaluation.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"UD1.4\"<|>\"The arXiv paper utilizes UD1.4 for Chinese part-of-speech tagging tasks.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"MSRA\"<|>\"The arXiv paper evaluates its NER model on the MSRA Chinese dataset.\"<|>8)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"OntoNotes4.0\"<|>\"The arXiv paper uses Chinese OntoNotes4.0 for named entity recognition experiments.\"<|>8)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"CoNLL2003\"<|>\"The arXiv paper includes CoNLL2003 as an English NER benchmark.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"OntoNotes5.0\"<|>\"The arXiv paper employs English OntoNotes5.0 in its NER evaluation.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"SQuAD v1.1\"<|>\"The arXiv paper uses SQuAD v1.1 for machine reading comprehension tasks.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"SQuAD v2.0\"<|>\"The arXiv paper includes SQuAD v2.0 in its MRC experiments.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"Quoref\"<|>\"The arXiv paper evaluates on the Quoref dataset for coreferential reasoning in reading comprehension.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"MRPC\"<|>\"The arXiv paper conducts paraphrase identification experiments using MRPC.\"<|>7)##\n(\"relationship\"<|>\"arXiv:1805.02023\"<|>\"QQP\"<|>\"The arXiv paper uses QQP as a paraphrase identification benchmark.\"<|>7)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "6870388284c2f9b805f8522bd3d8404f": {
    "return": "(\"entity\"<|>\"Seventh Conference on Natural Language Learning\"<|>\"event\"<|>\"An academic conference (CoNLL 2003) held in Edmonton, Canada from May 31 to June 1, 2003, featuring research on natural language learning.\")##\n(\"entity\"<|>\"HLT-NAACL 2003\"<|>\"event\"<|>\"A joint human language technology and North American chapter of the Association for Computational Linguistics conference held in 2003, with which CoNLL 2003 was held in cooperation.\")##\n(\"entity\"<|>\"Edmonton\"<|>\"geo\"<|>\"A city in Canada where the Seventh Conference on Natural Language Learning (CoNLL 2003) and HLT-NAACL 2003 were held.\")##\n(\"entity\"<|>\"Canada\"<|>\"geo\"<|>\"Country where the Seventh Conference on Natural Language Learning (CoNLL 2003) and HLT-NAACL 2003 took place.\")##\n(\"entity\"<|>\"Minjoon Seo\"<|>\"person\"<|>\"Author of the 2016 paper 'Bidirectional attention flow for machine comprehension' published as an arXiv preprint.\")##\n(\"entity\"<|>\"Aniruddha Kembhavi\"<|>\"person\"<|>\"Co-author of the 2016 paper 'Bidirectional attention flow for machine comprehension'.\")##\n(\"entity\"<|>\"Ali Farhadi\"<|>\"person\"<|>\"Co-author of the 2016 paper 'Bidirectional attention flow for machine comprehension'.\")##\n(\"entity\"<|>\"Hannaneh Hajishirzi\"<|>\"person\"<|>\"Co-author of the 2016 paper 'Bidirectional attention flow for machine comprehension'.\")##\n(\"entity\"<|>\"Reuben R. Shamir\"<|>\"person\"<|>\"Author of the 2019 paper 'Continuous dice coefficient: a method for evaluating probabilistic segmentations' published in CoRR.\")##\n(\"entity\"<|>\"Yuval Duchin\"<|>\"person\"<|>\"Co-author of the 2019 paper on continuous dice coefficient for segmentation evaluation.\")##\n(\"entity\"<|>\"Jinyoung Kim\"<|>\"person\"<|>\"Co-author of the 2019 paper on continuous dice coefficient for segmentation evaluation.\")##\n(\"entity\"<|>\"Guillermo Sapiro\"<|>\"person\"<|>\"Co-author of the 2019 paper on continuous dice coefficient for segmentation evaluation.\")##\n(\"entity\"<|>\"Noam Harel\"<|>\"person\"<|>\"Co-author of the 2019 paper on continuous dice coefficient for segmentation evaluation.\")##\n(\"entity\"<|>\"Yan Shao\"<|>\"person\"<|>\"Author of the 2017 paper on character-based joint segmentation and POS tagging for Chinese using bidirectional RNN-CRF.\")##\n(\"entity\"<|>\"Christian Hardmeier\"<|>\"person\"<|>\"Co-author of the 2017 Chinese segmentation and POS tagging paper.\")##\n(\"entity\"<|>\"Jorg Tiedemann\"<|>\"person\"<|>\"Co-author of the 2017 Chinese segmentation and POS tagging paper.\")##\n(\"entity\"<|>\"Joakim Nivre\"<|>\"person\"<|>\"Co-author of the 2017 Chinese segmentation and POS tagging paper.\")##\n(\"entity\"<|>\"Chen Shen\"<|>\"person\"<|>\"Author of the 2018 paper on the influence of Dice loss in multi-class organ segmentation using 3D fully convolutional networks.\")##\n(\"entity\"<|>\"Holger R. Roth\"<|>\"person\"<|>\"Co-author of the 2018 organ segmentation paper.\")##\n(\"entity\"<|>\"Hirohisa Oda\"<|>\"person\"<|>\"Co-author of the 2018 organ segmentation paper.\")##\n(\"entity\"<|>\"Masahiro Oda\"<|>\"person\"<|>\"Co-author of the 2018 organ segmentation paper.\")##\n(\"entity\"<|>\"Yuichiro Hayashi\"<|>\"person\"<|>\"Co-author of the 2018 organ segmentation paper.\")##\n(\"entity\"<|>\"Kazunari Misawa\"<|>\"person\"<|>\"Co-author of the 2018 organ segmentation paper.\")##\n(\"entity\"<|>\"Kensaku Mori\"<|>\"person\"<|>\"Co-author of the 2018 organ segmentation paper.\")##\n(\"entity\"<|>\"Yelong Shen\"<|>\"person\"<|>\"Author of the 2017 paper 'Reasonet: Learning to stop reading in machine comprehension' presented at KDD 2017.\")##\n(\"entity\"<|>\"Po-Sen Huang\"<|>\"person\"<|>\"Co-author of the 2017 Reasonet paper.\")##\n(\"entity\"<|>\"Jianfeng Gao\"<|>\"person\"<|>\"Co-author of the 2017 Reasonet paper.\")##\n(\"entity\"<|>\"Weizhu Chen\"<|>\"person\"<|>\"Co-author of the 2017 Reasonet paper.\")##\n(\"entity\"<|>\"23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\"<|>\"event\"<|>\"A major data mining conference in 2017 where the Reasonet paper was presented.\")##\n(\"entity\"<|>\"ACM\"<|>\"organization\"<|>\"Association for Computing Machinery, the publisher and organizer of the KDD conference.\")##\n(\"entity\"<|>\"Th A Sorensen\"<|>\"person\"<|>\"Author of a 1948 paper on plant sociology and species similarity, introducing what later became known as the Sørensen index.\")##\n(\"entity\"<|>\"Carole H. Sudre\"<|>\"person\"<|>\"Author of the 2017 paper on Generalised Dice overlap as a deep learning loss function for unbalanced segmentations.\")##\n(\"entity\"<|>\"Wenqi Li\"<|>\"person\"<|>\"Co-author of the 2017 Generalised Dice overlap paper.\")##\n(\"entity\"<|>\"Tom Vercauteren\"<|>\"person\"<|>\"Co-author of the 2017 Generalised Dice overlap paper.\")##\n(\"entity\"<|>\"Sebastien Ourselin\"<|>\"person\"<|>\"Co-author of the 2017 Generalised Dice overlap paper.\")##\n(\"entity\"<|>\"M. Jorge Cardoso\"<|>\"person\"<|>\"Co-author of the 2017 Generalised Dice overlap paper.\")##\n(\"entity\"<|>\"Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support\"<|>\"event\"<|>\"Third International Workshop (DLMIA 2017) held in conjunction with MICCAI 2017 in Quebec City, QC, Canada.\")##\n(\"entity\"<|>\"MICCAI 2017\"<|>\"event\"<|>\"The 2017 Medical Image Computing and Computer Assisted Intervention conference, with which DLMIA and ML-CDS workshops were held.\")##\n(\"entity\"<|>\"Quebec City\"<|>\"geo\"<|>\"City in Quebec, Canada, where DLMIA 2017 and ML-CDS 2017 workshops were held in September 2017.\")##\n(\"entity\"<|>\"QC\"<|>\"geo\"<|>\"Abbreviation for Quebec, a province in Canada; used here to denote the location of the DLMIA 2017 workshop.\")##\n(\"entity\"<|>\"Amos Tversky\"<|>\"person\"<|>\"Author of the 1977 Psychological Review paper 'Features of similarity', a foundational work in cognitive psychology.\")##\n(\"entity\"<|>\"Sergi Valverde\"<|>\"person\"<|>\"Lead author of the 2017 NeuroImage paper on improving multiple sclerosis lesion segmentation using cascaded 3D CNNs.\")##\n(\"entity\"<|>\"Mariano Cabezas\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Eloy Roura\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Sandra Gonzalez-Villa\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Deborah Pareto\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Joan C Vilanova\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Lluis Ramio-Torrenta\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Alex Rovira\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Arnau Oliver\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Xavier Llado\"<|>\"person\"<|>\"Co-author of the 2017 multiple sclerosis lesion segmentation paper.\")##\n(\"entity\"<|>\"Shuohang Wang\"<|>\"person\"<|>\"Author of the 2016 paper 'Machine comprehension using match-LSTM and answer pointer' published as an arXiv preprint.\")##\n(\"entity\"<|>\"Jing Jiang\"<|>\"person\"<|>\"Co-author of the 2016 match-LSTM machine comprehension paper.\")##\n(\"entity\"<|>\"Zhiguo Wang\"<|>\"person\"<|>\"Author of the 2016 paper 'Multi-perspective context matching for machine comprehension' published as an arXiv preprint.\")##\n(\"entity\"<|>\"Haitao Mi\"<|>\"person\"<|>\"Co-author of the 2016 multi-perspective context matching paper.\")##\n(\"entity\"<|>\"Wael Hamza\"<|>\"person\"<|>\"Co-author of the 2016 multi-perspective context matching paper.\")##\n(\"entity\"<|>\"Radu Florian\"<|>\"person\"<|>\"Co-author of the 2016 multi-perspective context matching paper.\")##\n(\"entity\"<|>\"Wei Wu\"<|>\"person\"<|>\"Lead author of the 2019 paper 'Glyce: Glyph-vectors for Chinese character representations' published as an arXiv preprint.\")##\n(\"entity\"<|>\"Yuxian Meng\"<|>\"person\"<|>\"Co-author of the 2019 Glyce paper.\")##\n(\"entity\"<|>\"Qinghong Han\"<|>\"person\"<|>\"Co-author of the 2019 Glyce paper.\")##\n(\"entity\"<|>\"Muyu Li\"<|>\"person\"<|>\"Co-author of the 2019 Glyce paper.\")##\n(\"entity\"<|>\"Xiaoya Li\"<|>\"person\"<|>\"Co-author of the 2019 Glyce paper.\")##\n(\"entity\"<|>\"Jie Mei\"<|>\"person\"<|>\"Co-author of the 2019 Glyce paper.\")##\n(\"entity\"<|>\"Ping Nie\"<|>\"person\"<|>\"Co-author of the 2019 Glyce paper.\")##\n(\"entity\"<|>\"Xiaofei Sun\"<|>\"person\"<|>\"Co-author of the 2019 Glyce paper.\")##\n(\"entity\"<|>\"Jiwei Li\"<|>\"person\"<|>\"Co-author of the 2019 Glyce paper.\")##\n(\"entity\"<|>\"Naiwen Xue\"<|>\"person\"<|>\"Author of the 2005 paper on the Penn Chinese Treebank, published in Natural Language Engineering.\")##\n(\"entity\"<|>\"Fei Xia\"<|>\"person\"<|>\"Co-author of the 2005 Penn Chinese Treebank paper.\")##\n(\"entity\"<|>\"Fudong Choiu\"<|>\"person\"<|>\"Co-author of the 2005 Penn Chinese Treebank paper.\")##\n(\"entity\"<|>\"Marta Palmer\"<|>\"person\"<|>\"Co-author of the 2005 Penn Chinese Treebank paper.\")##\n(\"entity\"<|>\"Zhilin Yang\"<|>\"person\"<|>\"Lead author of the 2019 XLNet paper published in CoRR.\")##\n(\"entity\"<|>\"Zihang Dai\"<|>\"person\"<|>\"Co-author of the 2019 XLNet paper.\")##\n(\"entity\"<|>\"Yiming Yang\"<|>\"person\"<|>\"Co-author of the 2019 XLNet paper.\")##\n(\"entity\"<|>\"Jaime G. Carbonell\"<|>\"person\"<|>\"Co-author of the 2019 XLNet paper.\")##\n(\"entity\"<|>\"Ruslan Salakhutdinov\"<|>\"person\"<|>\"Co-author of the 2019 XLNet paper.\")##\n(\"entity\"<|>\"Quoc V. Le\"<|>\"person\"<|>\"Co-author of the 2019 XLNet paper and other machine comprehension papers including QANet.\")##\n(\"entity\"<|>\"Adams Wei Yu\"<|>\"person\"<|>\"Lead author of the 2018 QANet papers on reading comprehension using convolution and self-attention.\")##\n(\"entity\"<|>\"David Dohan\"<|>\"person\"<|>\"Co-author of the 2018 QANet papers.\")##\n(\"entity\"<|>\"Minh-Thang Luong\"<|>\"person\"<|>\"Co-author of the 2018 QANet papers.\")##\n(\"entity\"<|>\"Rui Zhao\"<|>\"person\"<|>\"Co-author of the 2018 QANet papers.\")##\n(\"entity\"<|>\"Kai Chen\"<|>\"person\"<|>\"Co-author of the 2018 QANet papers.\")##\n(\"entity\"<|>\"Mohammad Norouzi\"<|>\"person\"<|>\"Co-author of the 2018 QANet papers.\")##\n(\"entity\"<|>\"6th International Conference on Learning Representations\"<|>\"event\"<|>\"ICLR 2018, held in Vancouver, BC, Canada from April 30 to May 3, 2018, where the QANet paper was presented.\")##\n(\"entity\"<|>\"Vancouver\"<|>\"geo\"<|>\"City in British Columbia, Canada, where ICLR 2018 was held.\")##\n(\"entity\"<|>\"BC\"<|>\"geo\"<|>\"British Columbia, a province in Canada; location of ICLR 2018.\")##\n(\"entity\"<|>\"Yue Zhang\"<|>\"person\"<|>\"Author of the 2018 paper 'Chinese NER using lattice LSTM' published as an arXiv preprint.\")##\n(\"entity\"<|>\"Jie Yang\"<|>\"person\"<|>\"Co-author of the 2018 Chinese NER using lattice LSTM paper.\")##\n(\"entity\"<|>\"Chinese Treebank 5.0/6.0\"<|>\"event\"<|>\"Benchmark datasets used for part-of-speech tagging and syntactic parsing of Chinese text.\")##\n(\"entity\"<|>\"UD 1.4\"<|>\"event\"<|>\"Universal Dependencies version 1.4, a benchmark dataset used for part-of-speech tagging and syntactic parsing across languages.\")##\n(\"relationship\"<|>\"Seventh Conference on Natural Language Learning\"<|>\"Edmonton\"<|>\"The conference was held in Edmonton, establishing a clear location-event relationship.\"<|>9)##\n(\"relationship\"<|>\"Seventh Conference on Natural Language Learning\"<|>\"Canada\"<|>\"The conference took place in Canada.\"<|>9)##\n(\"relationship\"<|>\"HLT-NAACL 2003\"<|>\"Edmonton\"<|>\"HLT-NAACL 2003 was co-located with CoNLL 2003 in Edmonton.\"<|>8)##\n(\"relationship\"<|>\"23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\"<|>\"ACM\"<|>\"ACM organized and published proceedings of the KDD conference.\"<|>10)##\n(\"relationship\"<|>\"Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support\"<|>\"MICCAI 2017\"<|>\"The DLMIA 2017 workshop was held in conjunction with MICCAI 2017.\"<|>9)##\n(\"relationship\"<|>\"Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support\"<|>\"Quebec City\"<|>\"The workshop was held in Quebec City.\"<|>9)##\n(\"relationship\"<|>\"6th International Conference on Learning Representations\"<|>\"Vancouver\"<|>\"ICLR 2018 was held in Vancouver.\"<|>9)##\n(\"relationship\"<|>\"Minjoon Seo\"<|>\"Aniruddha Kembhavi\"<|>\"Co-authors on the same 2016 machine comprehension paper.\"<|>8)##\n(\"relationship\"<|>\"Minjoon Seo\"<|>\"Ali Farhadi\"<|>\"Co-authors on the same 2016 machine comprehension paper.\"<|>8)##\n(\"relationship\"<|>\"Minjoon Seo\"<|>\"Hannaneh Hajishirzi\"<|>\"Co-authors on the same 2016 machine comprehension paper.\"<|>8)##\n(\"relationship\"<|>\"Yelong Shen\"<|>\"Po-Sen Huang\"<|>\"Co-authors on the 2017 Reasonet paper presented at KDD.\"<|>8)##\n(\"relationship\"<|>\"Yelong Shen\"<|>\"23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\"<|>\"Yelong Shen et al. presented their paper at this conference.\"<|>9)##\n(\"relationship\"<|>\"Adams Wei Yu\"<|>\"6th International Conference on Learning Representations\"<|>\"Adams Wei Yu et al. presented QANet at ICLR 2018.\"<|>9)##\n(\"relationship\"<|>\"Naiwen Xue\"<|>\"Penn Chinese Treebank\"<|>\"Naiwen Xue is the lead author of the paper describing the Penn Chinese Treebank.\"<|>9)##\n(\"relationship\"<|>\"Chinese Treebank 5.0/6.0\"<|>\"Naiwen Xue\"<|>\"The Chinese Treebank datasets are associated with the work of Naiwen Xue and colleagues.\"<|>8)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "fc379204c6cc45701872a052f1af0e2a": {
    "return": "(\"entity\"<|>\"Xiaoya Li\"<|>\"person\"<|>\"Xiaoya Li is a researcher and co-author of a 2019 paper on a unified MRC framework for named entity recognition.\")##\n(\"entity\"<|>\"Jingrong Feng\"<|>\"person\"<|>\"Jingrong Feng is a researcher and co-author of a 2019 paper on a unified MRC framework for named entity recognition.\")##\n(\"entity\"<|>\"Yuxian Meng\"<|>\"person\"<|>\"Yuxian Meng is a researcher who co-authored multiple papers, including one on a unified MRC framework for NER (2019) and another on Dsreg (2019).\")##\n(\"entity\"<|>\"Qinghong Han\"<|>\"person\"<|>\"Qinghong Han is a researcher and co-author of a 2019 paper on a unified MRC framework for named entity recognition.\")##\n(\"entity\"<|>\"Fei Wu\"<|>\"person\"<|>\"Fei Wu is a researcher and co-author of a 2019 paper on a unified MRC framework for named entity recognition.\")##\n(\"entity\"<|>\"Jiwei Li\"<|>\"person\"<|>\"Jiwei Li is a researcher who co-authored multiple papers, including one on a unified MRC framework for NER (2019) and another on Dsreg (2019).\")##\n(\"entity\"<|>\"Tsung-Yi Lin\"<|>\"person\"<|>\"Tsung-Yi Lin is a researcher and lead author of the 2017 paper on Focal Loss for dense object detection.\")##\n(\"entity\"<|>\"Priya Goyal\"<|>\"person\"<|>\"Priya Goyal is a researcher and co-author of the 2017 paper on Focal Loss presented at the IEEE International Conference on Computer Vision.\")##\n(\"entity\"<|>\"Ross Girshick\"<|>\"person\"<|>\"Ross Girshick is a researcher and co-author of the 2017 Focal Loss paper and also associated with computer vision research.\")##\n(\"entity\"<|>\"Kaiming He\"<|>\"person\"<|>\"Kaiming He is a researcher who co-authored the 2017 Focal Loss paper and the 2015 Faster R-CNN paper.\")##\n(\"entity\"<|>\"Piotr Dollar\"<|>\"person\"<|>\"Piotr Dollar is a researcher and co-author of the 2017 Focal Loss paper for dense object detection.\")##\n(\"entity\"<|>\"Xuezhe Ma\"<|>\"person\"<|>\"Xuezhe Ma is a researcher and co-author of a 2016 paper on end-to-end sequence labeling using bi-directional LSTM-CNNs-CRF.\")##\n(\"entity\"<|>\"Eduard Hovy\"<|>\"person\"<|>\"Eduard Hovy is a researcher and co-author of a 2016 paper on end-to-end sequence labeling.\")##\n(\"entity\"<|>\"Tomasz Malisiewicz\"<|>\"person\"<|>\"Tomasz Malisiewicz is a researcher and lead author of a 2011 paper on Ensemble of Exemplar-SVMs for object detection.\")##\n(\"entity\"<|>\"Abhinav Gupta\"<|>\"person\"<|>\"Abhinav Gupta is a researcher and co-author of the 2011 ICCV paper on Ensemble of Exemplar-SVMs.\")##\n(\"entity\"<|>\"Alexei A. Efros\"<|>\"person\"<|>\"Alexei A. Efros is a researcher and co-author of the 2011 ICCV paper on Ensemble of Exemplar-SVMs.\")##\n(\"entity\"<|>\"Bryan McCann\"<|>\"person\"<|>\"Bryan McCann is a researcher and lead author of a 2018 paper on The Natural Language Decathlon.\")##\n(\"entity\"<|>\"Nitish Shirish Keskar\"<|>\"person\"<|>\"Nitish Shirish Keskar is a researcher and co-author of the 2018 Natural Language Decathlon paper.\")##\n(\"entity\"<|>\"Caiming Xiong\"<|>\"person\"<|>\"Caiming Xiong is a researcher and co-author of the 2018 Natural Language Decathlon paper.\")##\n(\"entity\"<|>\"Richard Socher\"<|>\"person\"<|>\"Richard Socher is a researcher and co-author of the 2018 Natural Language Decathlon paper.\")##\n(\"entity\"<|>\"Muyu Li\"<|>\"person\"<|>\"Muyu Li is a researcher and co-author of the 2019 Dsreg paper.\")##\n(\"entity\"<|>\"Wei Wu\"<|>\"person\"<|>\"Wei Wu is a researcher and co-author of the 2019 Dsreg paper.\")##\n(\"entity\"<|>\"Fausto Milletari\"<|>\"person\"<|>\"Fausto Milletari is a researcher and lead author of a 2016 paper on V-Net for medical image segmentation.\")##\n(\"entity\"<|>\"Nassir Navab\"<|>\"person\"<|>\"Nassir Navab is a researcher and co-author of the 2016 V-Net paper.\")##\n(\"entity\"<|>\"Seyed-Ahmad Ahmadi\"<|>\"person\"<|>\"Seyed-Ahmad Ahmadi is a researcher and co-author of the 2016 V-Net paper.\")##\n(\"entity\"<|>\"David Nadeau\"<|>\"person\"<|>\"David Nadeau is a researcher and co-author of a 2007 survey on named entity recognition and classification.\")##\n(\"entity\"<|>\"Satoshi Sekine\"<|>\"person\"<|>\"Satoshi Sekine is a researcher and co-author of a 2007 survey on named entity recognition and classification.\")##\n(\"entity\"<|>\"Tri Nguyen\"<|>\"person\"<|>\"Tri Nguyen is a researcher and lead author of the 2016 MS MARCO dataset paper.\")##\n(\"entity\"<|>\"Mir Rosenberg\"<|>\"person\"<|>\"Mir Rosenberg is a researcher and co-author of the 2016 MS MARCO paper.\")##\n(\"entity\"<|>\"Xia Song\"<|>\"person\"<|>\"Xia Song is a researcher and co-author of the 2016 MS MARCO paper.\")##\n(\"entity\"<|>\"Jianfeng Gao\"<|>\"person\"<|>\"Jianfeng Gao is a researcher and co-author of the 2016 MS MARCO paper.\")##\n(\"entity\"<|>\"Saurabh Tiwary\"<|>\"person\"<|>\"Saurabh Tiwary is a researcher and co-author of the 2016 MS MARCO paper.\")##\n(\"entity\"<|>\"Rangan Majumder\"<|>\"person\"<|>\"Rangan Majumder is a researcher and co-author of the 2016 MS MARCO paper.\")##\n(\"entity\"<|>\"Li Deng\"<|>\"person\"<|>\"Li Deng is a researcher and co-author of the 2016 MS MARCO paper.\")##\n(\"entity\"<|>\"Jiangmiao Pang\"<|>\"person\"<|>\"Jiangmiao Pang is a researcher and lead author of the 2019 Libra R-CNN paper.\")##\n(\"entity\"<|>\"Kai Chen\"<|>\"person\"<|>\"Kai Chen is a researcher and co-author of the 2019 Libra R-CNN paper.\")##\n(\"entity\"<|>\"Jianping Shi\"<|>\"person\"<|>\"Jianping Shi is a researcher and co-author of the 2019 Libra R-CNN paper.\")##\n(\"entity\"<|>\"Huajun Feng\"<|>\"person\"<|>\"Huajun Feng is a researcher and co-author of the 2019 Libra R-CNN paper.\")##\n(\"entity\"<|>\"Wanli Ouyang\"<|>\"person\"<|>\"Wanli Ouyang is a researcher and co-author of the 2019 Libra R-CNN paper.\")##\n(\"entity\"<|>\"Dahua Lin\"<|>\"person\"<|>\"Dahua Lin is a researcher and co-author of the 2019 Libra R-CNN paper.\")##\n(\"entity\"<|>\"Matthew E Peters\"<|>\"person\"<|>\"Matthew E Peters is a researcher and lead author of the 2018 paper on Deep Contextualized Word Representations (ELMo).\")##\n(\"entity\"<|>\"Mark Neumann\"<|>\"person\"<|>\"Mark Neumann is a researcher and co-author of the 2018 ELMo paper.\")##\n(\"entity\"<|>\"Mohit Iyyer\"<|>\"person\"<|>\"Mohit Iyyer is a researcher and co-author of the 2018 ELMo paper.\")##\n(\"entity\"<|>\"Matt Gardner\"<|>\"person\"<|>\"Matt Gardner is a researcher and co-author of the 2018 ELMo paper.\")##\n(\"entity\"<|>\"Christopher Clark\"<|>\"person\"<|>\"Christopher Clark is a researcher and co-author of the 2018 ELMo paper.\")##\n(\"entity\"<|>\"Kenton Lee\"<|>\"person\"<|>\"Kenton Lee is a researcher and co-author of the 2018 ELMo paper.\")##\n(\"entity\"<|>\"Luke Zettlemoyer\"<|>\"person\"<|>\"Luke Zettlemoyer is a researcher and co-author of the 2018 ELMo paper.\")##\n(\"entity\"<|>\"Sameer Pradhan\"<|>\"person\"<|>\"Sameer Pradhan is a researcher involved in multiple shared tasks on computational linguistics, including CoNLL and OntoNotes.\")##\n(\"entity\"<|>\"Mitchell P. Marcus\"<|>\"person\"<|>\"Mitchell P. Marcus is an editor of the 2011 CoNLL Shared Task proceedings.\")##\n(\"entity\"<|>\"Martha Palmer\"<|>\"person\"<|>\"Martha Palmer is an editor of the 2011 CoNLL Shared Task proceedings.\")##\n(\"entity\"<|>\"Lance A. Ramshaw\"<|>\"person\"<|>\"Lance A. Ramshaw is an editor of the 2011 CoNLL Shared Task proceedings.\")##\n(\"entity\"<|>\"Ralph M. Weischedel\"<|>\"person\"<|>\"Ralph M. Weischedel is an editor of the 2011 CoNLL Shared Task proceedings.\")##\n(\"entity\"<|>\"Nianwen Xue\"<|>\"person\"<|>\"Nianwen Xue is a researcher involved in both the 2011 CoNLL Shared Task and the 2013 OntoNotes robust linguistic analysis paper.\")##\n(\"entity\"<|>\"Alessandro Moschitti\"<|>\"person\"<|>\"Alessandro Moschitti is a researcher and co-author of the 2013 OntoNotes paper.\")##\n(\"entity\"<|>\"Hwee Tou Ng\"<|>\"person\"<|>\"Hwee Tou Ng is a researcher and co-author of the 2013 OntoNotes paper.\")##\n(\"entity\"<|>\"Anders Bjorkelund\"<|>\"person\"<|>\"Anders Bjorkelund is a researcher and co-author of the 2013 OntoNotes paper.\")##\n(\"entity\"<|>\"Olga Uryupina\"<|>\"person\"<|>\"Olga Uryupina is a researcher and co-author of the 2013 OntoNotes paper.\")##\n(\"entity\"<|>\"Yuchen Zhang\"<|>\"person\"<|>\"Yuchen Zhang is a researcher and co-author of the 2013 OntoNotes paper.\")##\n(\"entity\"<|>\"Zhi Zhong\"<|>\"person\"<|>\"Zhi Zhong is a researcher and co-author of the 2013 OntoNotes paper.\")##\n(\"entity\"<|>\"Pranav Rajpurkar\"<|>\"person\"<|>\"Pranav Rajpurkar is a researcher who co-authored the SQuAD (2016) and SQuAD unanswerable questions (2018) papers.\")##\n(\"entity\"<|>\"Robin Jia\"<|>\"person\"<|>\"Robin Jia is a researcher and co-author of the 2018 SQuAD unanswerable questions paper.\")##\n(\"entity\"<|>\"Percy Liang\"<|>\"person\"<|>\"Percy Liang is a researcher who co-authored both the SQuAD (2016) and SQuAD unanswerable questions (2018) papers.\")##\n(\"entity\"<|>\"Jian Zhang\"<|>\"person\"<|>\"Jian Zhang is a researcher and co-author of the 2016 SQuAD paper.\")##\n(\"entity\"<|>\"Konstantin Lopyrev\"<|>\"person\"<|>\"Konstantin Lopyrev is a researcher and co-author of the 2016 SQuAD paper.\")##\n(\"entity\"<|>\"Shaoqing Ren\"<|>\"person\"<|>\"Shaoqing Ren is a researcher and lead author of the 2015 Faster R-CNN paper.\")##\n(\"entity\"<|>\"Ross B. Girshick\"<|>\"person\"<|>\"Ross B. Girshick is a researcher and co-author of the 2015 Faster R-CNN paper.\")##\n(\"entity\"<|>\"Jian Sun\"<|>\"person\"<|>\"Jian Sun is a researcher and co-author of the 2015 Faster R-CNN paper.\")##\n(\"entity\"<|>\"Alan Ritter\"<|>\"person\"<|>\"Alan Ritter is a researcher and lead author of a 2011 paper on Named Entity Recognition in Tweets.\")##\n(\"entity\"<|>\"Sam Clark\"<|>\"person\"<|>\"Sam Clark is a researcher and co-author of the 2011 NER in Tweets paper.\")##\n(\"entity\"<|>\"Mausam\"<|>\"person\"<|>\"Mausam is a researcher and co-author of the 2011 NER in Tweets paper.\")##\n(\"entity\"<|>\"Oren Etzioni\"<|>\"person\"<|>\"Oren Etzioni is a researcher and co-author of the 2011 NER in Tweets paper.\")##\n(\"entity\"<|>\"Erik F Sang\"<|>\"person\"<|>\"Erik F Sang is a researcher and co-author of the 2003 CoNLL-2003 shared task paper on language-independent NER.\")##\n(\"entity\"<|>\"Fien De Meulder\"<|>\"person\"<|>\"Fien De Meulder is a researcher and co-author of the 2003 CoNLL-2003 shared task paper.\")##\n(\"entity\"<|>\"Erik F. Tjong Kim Sang\"<|>\"person\"<|>\"Erik F. Tjong Kim Sang is a researcher and co-author of the 2003 CoNLL-2003 shared task paper; full name variant of Erik F Sang.\")##\n(\"entity\"<|>\"Minjoon Seo\"<|>\"person\"<|>\"Minjoon Seo is a researcher and lead author of a 2016 paper on Bidirectional Attention Flow for machine comprehension.\")##\n(\"entity\"<|>\"Aniruddha Kembhavi\"<|>\"person\"<|>\"Aniruddha Kembhavi is a researcher and co-author of the 2016 BiDAF paper.\")##\n(\"entity\"<|>\"Ali Farhadi\"<|>\"person\"<|>\"Ali Farhadi is a researcher and co-author of the 2016 BiDAF paper.\")##\n(\"entity\"<|>\"Hannaneh Hajishirzi\"<|>\"person\"<|>\"Hannaneh Hajishirzi is a researcher and co-author of the 2016 BiDAF paper.\")##\n(\"entity\"<|>\"IEEE International Conference on Computer Vision\"<|>\"event\"<|>\"The IEEE International Conference on Computer Vision (ICCV) is a major academic conference where several cited papers were presented, including the 2011 and 2017 works.\")##\n(\"entity\"<|>\"ICCV 2011\"<|>\"event\"<|>\"ICCV 2011 refers to the 2011 IEEE International Conference on Computer Vision held in Barcelona, Spain, November 6–13, 2011.\")##\n(\"entity\"<|>\"Barcelona\"<|>\"geo\"<|>\"Barcelona is a city in Spain where ICCV 2011 was held.\")##\n(\"entity\"<|>\"Spain\"<|>\"geo\"<|>\"Spain is the country where ICCV 2011 took place in Barcelona.\")##\n(\"entity\"<|>\"IEEE Conference on Computer Vision and Pattern Recognition\"<|>\"event\"<|>\"The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) is a premier conference; CVPR 2019 was held in Long Beach, CA, USA.\")##\n(\"entity\"<|>\"CVPR 2019\"<|>\"event\"<|>\"CVPR 2019 is the 2019 edition of the IEEE Conference on Computer Vision and Pattern Recognition, held in Long Beach, CA, USA, June 16–20, 2019.\")##\n(\"entity\"<|>\"Long Beach\"<|>\"geo\"<|>\"Long Beach is a city in California, USA, where CVPR 2019 was held.\")##\n(\"entity\"<|>\"CA\"<|>\"geo\"<|>\"CA (California) is a U.S. state where CVPR 2019 took place in Long Beach.\")##\n(\"entity\"<|>\"USA\"<|>\"geo\"<|>\"USA is the country where CVPR 2019 was held.\")##\n(\"entity\"<|>\"2016 Fourth International Conference on 3D Vision\"<|>\"event\"<|>\"The 2016 Fourth International Conference on 3D Vision (3DV) is a conference where the V-Net paper was presented.\")##\n(\"entity\"<|>\"Edmonton\"<|>\"geo\"<|>\"Edmonton is a city in Canada where the CoNLL 2003 workshop was held.\")##\n(\"entity\"<|>\"Canada\"<|>\"geo\"<|>\"Canada is the country where the CoNLL 2003 workshop took place in Edmonton.\")##\n(\"entity\"<|>\"CoNLL 2003\"<|>\"event\"<|>\"CoNLL 2003 refers to the Seventh Conference on Natural Language Learning, held in Edmonton, Canada, May 31 – June 1, 2003.\")##\n(\"entity\"<|>\"HLT-NAACL 2003\"<|>\"event\"<|>\"HLT-NAACL 2003 is the Human Language Technology–North American Chapter of the ACL conference, held in cooperation with CoNLL 2003.\")##\n(\"entity\"<|>\"Sofia\"<|>\"geo\"<|>\"Sofia is the capital city of Bulgaria where the Seventeenth Conference on Computational Natural Language Learning was held in 2013.\")##\n(\"entity\"<|>\"Bulgaria\"<|>\"geo\"<|>\"Bulgaria is the country where the 2013 Computational Natural Language Learning conference took place in Sofia.\")##\n(\"entity\"<|>\"Association for Computational Linguistics\"<|>\"organization\"<|>\"The Association for Computational Linguistics (ACL) is a professional organization that sponsors conferences such as CoNLL and publishes proceedings in computational linguistics.\")##\n(\"entity\"<|>\"Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task\"<|>\"event\"<|>\"This refers to the 2011 CoNLL Shared Task workshop organized by ACL.\")##\n(\"entity\"<|>\"Proceedings of the Seventeenth Conference on Computational Natural Language Learning\"<|>\"event\"<|>\"This 2013 conference event included the paper on OntoNotes and was held in Sofia, Bulgaria.\")##\n(\"relationship\"<|>\"Xiaoya Li\"<|>\"Jingrong Feng\"<|>\"Both are co-authors of the same 2019 paper on a unified MRC framework for NER.\"<|>9)##\n(\"relationship\"<|>\"Yuxian Meng\"<|>\"Jiwei Li\"<|>\"They co-authored two papers together in 2019: one on MRC for NER and another on Dsreg.\"<|>10)##\n(\"relationship\"<|>\"Tsung-Yi Lin\"<|>\"Kaiming He\"<|>\"Both are co-authors of the influential 2017 Focal Loss paper.\"<|>9)##\n(\"relationship\"<|>\"Shaoqing Ren\"<|>\"Kaiming He\"<|>\"Both co-authored the 2015 Faster R-CNN paper.\"<|>9)##\n(\"relationship\"<|>\"Ross Girshick\"<|>\"Ross B. Girshick\"<|>\"These likely refer to the same person, appearing with and without middle initial in different publications.\"<|>10)##\n(\"relationship\"<|>\"Pranav Rajpurkar\"<|>\"Percy Liang\"<|>\"They co-authored both the SQuAD (2016) and its follow-up on unanswerable questions (2018).\"<|>10)##\n(\"relationship\"<|>\"Sameer Pradhan\"<|>\"Nianwen Xue\"<|>\"They collaborated on multiple computational linguistics shared tasks, including CoNLL 2011 and OntoNotes 2013.\"<|>8)##\n(\"relationship\"<|>\"Erik F Sang\"<|>\"Erik F. Tjong Kim Sang\"<|>\"These are name variants of the same person, co-author of the CoNLL-2003 shared task paper.\"<|>10)##\n(\"relationship\"<|>\"ICCV 2011\"<|>\"Barcelona\"<|>\"ICCV 2011 was held in Barcelona, Spain.\"<|>10)##\n(\"relationship\"<|>\"CVPR 2019\"<|>\"Long Beach\"<|>\"CVPR 2019 took place in Long Beach, CA, USA.\"<|>10)##\n(\"relationship\"<|>\"CoNLL 2003\"<|>\"Edmonton\"<|>\"The CoNLL 2003 workshop was held in Edmonton, Canada.\"<|>10)##\n(\"relationship\"<|>\"Proceedings of the Seventeenth Conference on Computational Natural Language Learning\"<|>\"Sofia\"<|>\"The 2013 conference was held in Sofia, Bulgaria.\"<|>10)##\n(\"relationship\"<|>\"Association for Computational Linguistics\"<|>\"Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task\"<|>\"ACL sponsored and published the proceedings of this shared task.\"<|>9)##\n(\"relationship\"<|>\"Association for Computational Linguistics\"<|>\"Proceedings of the Seventeenth Conference on Computational Natural Language Learning\"<|>\"ACL is the organizing body behind this conference event.\"<|>9)<|COMPLETE|>",
    "model": "qwen3-max"
  },
  "fafedb181adb9208a169bbc03f4dcef8": {
    "return": "(\"entity\"<|>\"IEEE Transactions on Neural Networks\"<|>\"organization\"<|>\"IEEE Transactions on Neural Networks is a scholarly journal that published the paper 'Ramoboost: Ranked minority oversampling in boosting.'\")##\n(\"entity\"<|>\"Kevin Clark\"<|>\"person\"<|>\"Kevin Clark is a co-author of the 2018 paper on semi-supervised sequence modeling with cross-view training presented at EMNLP.\")##\n(\"entity\"<|>\"Minh-Thang Luong\"<|>\"person\"<|>\"Minh-Thang Luong is a co-author of the 2018 paper on semi-supervised sequence modeling with cross-view training presented at EMNLP.\")##\n(\"entity\"<|>\"Christopher D. Manning\"<|>\"person\"<|>\"Christopher D. Manning is a co-author of the 2018 paper on semi-supervised sequence modeling with cross-view training and is a prominent researcher in natural language processing.\")##\n(\"entity\"<|>\"Quoc V. Le\"<|>\"person\"<|>\"Quoc V. Le is a co-author of the 2018 paper on semi-supervised sequence modeling with cross-view training.\")##\n(\"entity\"<|>\"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\"<|>\"event\"<|>\"This is an academic conference event held in Brussels, Belgium from October 31 to November 4, 2018, where research papers in NLP were presented.\")##\n(\"entity\"<|>\"Brussels\"<|>\"geo\"<|>\"Brussels is the capital city of Belgium and the location where the 2018 Conference on Empirical Methods in Natural Language Processing was held.\")##\n(\"entity\"<|>\"Belgium\"<|>\"geo\"<|>\"Belgium is a European country where both the 2018 EMNLP conference and Ghent University (affiliation of Frederic Godin) are located.\")##\n(\"entity\"<|>\"Pradeep Dasigi\"<|>\"person\"<|>\"Pradeep Dasigi is a co-author of the 2019 paper introducing the Quoref reading comprehension dataset.\")##\n(\"entity\"<|>\"Nelson F Liu\"<|>\"person\"<|>\"Nelson F Liu is a co-author of the 2019 Quoref dataset paper.\")##\n(\"entity\"<|>\"Ana Marasovic\"<|>\"person\"<|>\"Ana Marasovic is a co-author of the 2019 Quoref dataset paper.\")##\n(\"entity\"<|>\"Noah A Smith\"<|>\"person\"<|>\"Noah A Smith is a co-author of the 2019 Quoref dataset paper and a well-known computational linguist.\")##\n(\"entity\"<|>\"Matt Gardner\"<|>\"person\"<|>\"Matt Gardner is a co-author of the 2019 Quoref dataset paper.\")##\n(\"entity\"<|>\"Jacob Devlin\"<|>\"person\"<|>\"Jacob Devlin is the lead author of the 2018 BERT paper, a foundational work in language understanding.\")##\n(\"entity\"<|>\"Ming-Wei Chang\"<|>\"person\"<|>\"Ming-Wei Chang is a co-author of the 2018 BERT paper.\")##\n(\"entity\"<|>\"Kenton Lee\"<|>\"person\"<|>\"Kenton Lee is a co-author of the 2018 BERT paper.\")##\n(\"entity\"<|>\"Kristina Toutanova\"<|>\"person\"<|>\"Kristina Toutanova is a co-author of the 2018 BERT paper.\")##\n(\"entity\"<|>\"Lee R Dice\"<|>\"person\"<|>\"Lee R Dice is the author of a 1945 paper on ecological association measures, known for the Dice coefficient.\")##\n(\"entity\"<|>\"William B. Dolan\"<|>\"person\"<|>\"William B. Dolan is a co-author of a 2005 paper on automatically constructing a paraphrase corpus.\")##\n(\"entity\"<|>\"Chris Brockett\"<|>\"person\"<|>\"Chris Brockett is a co-author of a 2005 paper on paraphrase corpus construction.\")##\n(\"entity\"<|>\"Yang Fan\"<|>\"person\"<|>\"Yang Fan is the lead author of the 2018 'Learning to teach' paper.\")##\n(\"entity\"<|>\"Fei Tian\"<|>\"person\"<|>\"Fei Tian is a co-author of the 2018 'Learning to teach' paper.\")##\n(\"entity\"<|>\"Tao Qin\"<|>\"person\"<|>\"Tao Qin is a co-author of the 2018 'Learning to teach' paper.\")##\n(\"entity\"<|>\"Xiuping Li\"<|>\"person\"<|>\"Xiuping Li is a co-author of the 2018 'Learning to teach' paper.\")##\n(\"entity\"<|>\"Tie-Yan Liu\"<|>\"person\"<|>\"Tie-Yan Liu is a co-author of the 2018 'Learning to teach' paper.\")##\n(\"entity\"<|>\"Ross B. Girshick\"<|>\"person\"<|>\"Ross B. Girshick is a computer vision researcher and author of the Fast R-CNN paper (2015) and co-author of earlier object detection work.\")##\n(\"entity\"<|>\"Jeff Donahue\"<|>\"person\"<|>\"Jeff Donahue is a co-author of the 2013 paper on rich feature hierarchies for object detection.\")##\n(\"entity\"<|>\"Trevor Darrell\"<|>\"person\"<|>\"Trevor Darrell is a co-author of the 2013 paper on rich feature hierarchies for object detection.\")##\n(\"entity\"<|>\"Jitendra Malik\"<|>\"person\"<|>\"Jitendra Malik is a co-author of the 2013 paper on rich feature hierarchies for object detection.\")##\n(\"entity\"<|>\"Kaiming He\"<|>\"person\"<|>\"Kaiming He is a co-author of the 2015 deep residual learning paper and the 2017 Focal Loss paper.\")##\n(\"entity\"<|>\"Xiangyu Zhang\"<|>\"person\"<|>\"Xiangyu Zhang is a co-author of the 2015 deep residual learning paper.\")##\n(\"entity\"<|>\"Shaoqing Ren\"<|>\"person\"<|>\"Shaoqing Ren is a co-author of the 2015 deep residual learning paper.\")##\n(\"entity\"<|>\"Jian Sun\"<|>\"person\"<|>\"Jian Sun is a co-author of the 2015 deep residual learning paper.\")##\n(\"entity\"<|>\"Lu Jiang\"<|>\"person\"<|>\"Lu Jiang is the lead author of the 2017 Mentornet paper.\")##\n(\"entity\"<|>\"Zhengyuan Zhou\"<|>\"person\"<|>\"Zhengyuan Zhou is a co-author of the 2017 Mentornet paper.\")##\n(\"entity\"<|>\"Thomas Leung\"<|>\"person\"<|>\"Thomas Leung is a co-author of the 2017 Mentornet paper.\")##\n(\"entity\"<|>\"Li-Jia Li\"<|>\"person\"<|>\"Li-Jia Li is a co-author of the 2017 Mentornet paper.\")##\n(\"entity\"<|>\"Li Fei-Fei\"<|>\"person\"<|>\"Li Fei-Fei is a co-author of the 2017 Mentornet paper and a leading figure in computer vision.\")##\n(\"entity\"<|>\"ICML\"<|>\"event\"<|>\"ICML (International Conference on Machine Learning) is a premier machine learning conference where papers like Mentornet and importance sampling were presented.\")##\n(\"entity\"<|>\"H. Kahn\"<|>\"person\"<|>\"H. Kahn is a co-author of a 1953 paper on Monte Carlo methods.\")##\n(\"entity\"<|>\"A. W. Marshall\"<|>\"person\"<|>\"A. W. Marshall is a co-author of a 1953 paper on Monte Carlo methods.\")##\n(\"entity\"<|>\"Anil Kanduri\"<|>\"person\"<|>\"Anil Kanduri is the lead author of the 2018 adboost paper on thermal-aware performance boosting.\")##\n(\"entity\"<|>\"Mohammad Hashem Haghbayan\"<|>\"person\"<|>\"Mohammad Hashem Haghbayan is a co-author of the 2018 adboost paper.\")##\n(\"entity\"<|>\"Amir M. Rahmani\"<|>\"person\"<|>\"Amir M. Rahmani is a co-author of the 2018 adboost paper.\")##\n(\"entity\"<|>\"Muhammad Shafique\"<|>\"person\"<|>\"Muhammad Shafique is a co-author of the 2018 adboost paper.\")##\n(\"entity\"<|>\"Axel Jantsch\"<|>\"person\"<|>\"Axel Jantsch is a co-author of the 2018 adboost paper.\")##\n(\"entity\"<|>\"Pasi Liljeberg\"<|>\"person\"<|>\"Pasi Liljeberg is a co-author of the 2018 adboost paper.\")##\n(\"entity\"<|>\"IEEE Trans. Computers\"<|>\"organization\"<|>\"IEEE Transactions on Computers is a journal that published the 2018 adboost paper.\")##\n(\"entity\"<|>\"Angelos Katharopoulos\"<|>\"person\"<|>\"Angelos Katharopoulos is the lead author of the 2018 ICML paper on importance sampling in deep learning.\")##\n(\"entity\"<|>\"François Fleuret\"<|>\"person\"<|>\"François Fleuret is a co-author of the 2018 ICML paper on importance sampling.\")##\n(\"entity\"<|>\"Tomáš Kočiský\"<|>\"person\"<|>\"Tomáš Kočiský is the lead author of the 2018 NarrativeQA reading comprehension challenge paper.\")##\n(\"entity\"<|>\"Jonathan Schwarz\"<|>\"person\"<|>\"Jonathan Schwarz is a co-author of the 2018 NarrativeQA paper.\")##\n(\"entity\"<|>\"Phil Blunsom\"<|>\"person\"<|>\"Phil Blunsom is a co-author of the 2018 NarrativeQA paper.\")##\n(\"entity\"<|>\"Chris Dyer\"<|>\"person\"<|>\"Chris Dyer is a co-author of both the 2018 NarrativeQA paper and the 2016 NER architecture paper.\")##\n(\"entity\"<|>\"Karl Moritz Hermann\"<|>\"person\"<|>\"Karl Moritz Hermann is a co-author of the 2018 NarrativeQA paper.\")##\n(\"entity\"<|>\"Gábor Melis\"<|>\"person\"<|>\"Gábor Melis is a co-author of the 2018 NarrativeQA paper.\")##\n(\"entity\"<|>\"Edward Grefenstette\"<|>\"person\"<|>\"Edward Grefenstette is a co-author of the 2018 NarrativeQA paper.\")##\n(\"entity\"<|>\"Oldrich Kodym\"<|>\"person\"<|>\"Oldrich Kodym is the lead author of the 2018 GCPR paper on head and neck organ segmentation.\")##\n(\"entity\"<|>\"Michal Spanel\"<|>\"person\"<|>\"Michal Spanel is a co-author of the 2018 GCPR paper.\")##\n(\"entity\"<|>\"Adam Herout\"<|>\"person\"<|>\"Adam Herout is a co-author of the 2018 GCPR paper.\")##\n(\"entity\"<|>\"Pattern Recognition - 40th German Conference, GCPR 2018\"<|>\"event\"<|>\"GCPR 2018 is a German conference on pattern recognition held in Stuttgart, Germany from October 9–12, 2018.\")##\n(\"entity\"<|>\"Stuttgart\"<|>\"geo\"<|>\"Stuttgart is a city in Germany where the GCPR 2018 conference was held.\")##\n(\"entity\"<|>\"Germany\"<|>\"geo\"<|>\"Germany is the country where the GCPR 2018 conference took place.\")##\n(\"entity\"<|>\"M. Pawan Kumar\"<|>\"person\"<|>\"M. Pawan Kumar is the lead author of the 2010 self-paced learning paper presented at NeurIPS.\")##\n(\"entity\"<|>\"Benjamin Packer\"<|>\"person\"<|>\"Benjamin Packer is a co-author of the 2010 self-paced learning paper.\")##\n(\"entity\"<|>\"Daphne Koller\"<|>\"person\"<|>\"Daphne Koller is a co-author of the 2010 self-paced learning paper and a renowned AI researcher.\")##\n(\"entity\"<|>\"Advances in Neural Information Processing Systems 23\"<|>\"event\"<|>\"NeurIPS 2010 (formerly NIPS) was held December 6–9, 2010 in Vancouver, British Columbia, Canada.\")##\n(\"entity\"<|>\"Vancouver\"<|>\"geo\"<|>\"Vancouver is a city in British Columbia, Canada, where NeurIPS 2010 was held.\")##\n(\"entity\"<|>\"British Columbia\"<|>\"geo\"<|>\"British Columbia is a province in Canada where Vancouver is located and where NeurIPS 2010 took place.\")##\n(\"entity\"<|>\"Canada\"<|>\"geo\"<|>\"Canada is the country where NeurIPS 2010 was held.\")##\n(\"entity\"<|>\"Guillaume Lample\"<|>\"person\"<|>\"Guillaume Lample is the lead author of the 2016 paper on neural architectures for named entity recognition.\")##\n(\"entity\"<|>\"Miguel Ballesteros\"<|>\"person\"<|>\"Miguel Ballesteros is a co-author of the 2016 NER paper.\")##\n(\"entity\"<|>\"Sandeep Subramanian\"<|>\"person\"<|>\"Sandeep Subramanian is a co-author of the 2016 NER paper.\")##\n(\"entity\"<|>\"Kazuya Kawakami\"<|>\"person\"<|>\"Kazuya Kawakami is a co-author of the 2016 NER paper.\")##\n(\"entity\"<|>\"Gina-Anne Levow\"<|>\"person\"<|>\"Gina-Anne Levow is the author of a 2006 paper on Chinese language processing presented at the Fifth SIGHAN Workshop.\")##\n(\"entity\"<|>\"Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing\"<|>\"event\"<|>\"The Fifth SIGHAN Workshop was held in 2006 in Sydney, Australia, focusing on Chinese language processing tasks.\")##\n(\"entity\"<|>\"Sydney\"<|>\"geo\"<|>\"Sydney is a city in Australia where the Fifth SIGHAN Workshop was held.\")##\n(\"entity\"<|>\"Australia\"<|>\"geo\"<|>\"Australia is the country where the Fifth SIGHAN Workshop took place.\")##\n(\"entity\"<|>\"Association for Computational Linguistics\"<|>\"organization\"<|>\"The Association for Computational Linguistics (ACL) sponsored the Fifth SIGHAN Workshop on Chinese Language Processing.\")##\n(\"entity\"<|>\"H. Li\"<|>\"person\"<|>\"H. Li is the lead author of the 2015 CVPR paper on CNN cascade for face detection.\")##\n(\"entity\"<|>\"Z. Lin\"<|>\"person\"<|>\"Z. Lin is a co-author of the 2015 CVPR face detection paper.\")##\n(\"entity\"<|>\"X. Shen\"<|>\"person\"<|>\"X. Shen is a co-author of the 2015 CVPR face detection paper.\")##\n(\"entity\"<|>\"J. Brandt\"<|>\"person\"<|>\"J. Brandt is a co-author of the 2015 CVPR face detection paper.\")##\n(\"entity\"<|>\"G. Hua\"<|>\"person\"<|>\"G. Hua is a co-author of the 2015 CVPR face detection paper.\")##\n(\"entity\"<|>\"2015 IEEE Conference on Computer Vision and Pattern Recognition\"<|>\"event\"<|>\"CVPR 2015 is a major computer vision conference where multiple cited papers were presented.\")##\n(\"entity\"<|>\"Xiaoya Li\"<|>\"person\"<|>\"Xiaoya Li is the lead author of the 2019 unified MRC framework for named entity recognition.\")##\n(\"entity\"<|>\"Jingrong Feng\"<|>\"person\"<|>\"Jingrong Feng is a co-author of the 2019 MRC-NER paper.\")##\n(\"entity\"<|>\"Yuxian Meng\"<|>\"person\"<|>\"Yuxian Meng is a co-author of the 2019 MRC-NER paper.\")##\n(\"entity\"<|>\"Qinghong Han\"<|>\"person\"<|>\"Qinghong Han is a co-author of the 2019 MRC-NER paper.\")##\n(\"entity\"<|>\"Fei Wu\"<|>\"person\"<|>\"Fei Wu is a co-author of the 2019 MRC-NER paper.\")##\n(\"entity\"<|>\"Jiwei Li\"<|>\"person\"<|>\"Jiwei Li is a co-author of the 2019 MRC-NER paper.\")##\n(\"entity\"<|>\"Tsung-Yi Lin\"<|>\"person\"<|>\"Tsung-Yi Lin is the lead author of the 2017 Focal Loss paper for dense object detection.\")##\n(\"entity\"<|>\"Priya Goyal\"<|>\"person\"<|>\"Priya Goyal is a co-author of the 2017 Focal Loss paper.\")##\n(\"entity\"<|>\"Piotr Dollar\"<|>\"person\"<|>\"Piotr Dollar is a co-author of the 2017 Focal Loss paper.\")##\n(\"entity\"<|>\"Proceedings of the IEEE international conference on computer\"<|>\"event\"<|>\"This refers to an IEEE computer vision or AI conference (likely CVPR) where the Focal Loss paper was presented.\")##\n(\"entity\"<|>\"Frederic Godin\"<|>\"person\"<|>\"Frederic Godin is the author of a 2019 Ph.D. thesis at Ghent University, Belgium, on neural networks for NLP.\")##\n(\"entity\"<|>\"Ghent University\"<|>\"organization\"<|>\"Ghent University is a Belgian university where Frederic Godin completed his Ph.D. thesis in 2019.\")##\n(\"relationship\"<|>\"Kevin Clark\"<|>\"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\"<|>\"Kevin Clark co-authored a paper presented at this conference.\"<|>9)##\n(\"relationship\"<|>\"Minh-Thang Luong\"<|>\"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\"<|>\"Minh-Thang Luong co-authored a paper presented at this conference.\"<|>9)##\n(\"relationship\"<|>\"Christopher D. Manning\"<|>\"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\"<|>\"Christopher D. Manning co-authored a paper presented at this conference.\"<|>9)##\n(\"relationship\"<|>\"Quoc V. Le\"<|>\"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\"<|>\"Quoc V. Le co-authored a paper presented at this conference.\"<|>9)##\n(\"relationship\"<|>\"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\"<|>\"Brussels\"<|>\"The conference was held in Brussels.\"<|>10)##\n(\"relationship\"<|>\"Brussels\"<|>\"Belgium\"<|>\"Brussels is the capital city of Belgium.\"<|>10)##\n(\"relationship\"<|>\"Pradeep Dasigi\"<|>\"arXiv preprint arXiv:1908.05803\"<|>\"Pradeep Dasigi is the lead author of the Quoref dataset paper posted on arXiv.\"<|>9)##\n(\"relationship\"<|>\"Jacob Devlin\"<|>\"arXiv preprint arXiv:1810.04805\"<|>\"Jacob Devlin is the lead author of the BERT paper posted on arXiv.\"<|>10)##\n(\"relationship\"<|>\"Ross B. Girshick\"<|>\"2015 IEEE International Conference on Computer Vision (ICCV)\"<|>\"Ross B. Girshick presented the Fast R-CNN paper at ICCV 2015.\"<|>9)##\n(\"relationship\"<|>\"Kaiming He\"<|>\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\"<|>\"Kaiming He presented the ResNet paper at CVPR 2016.\"<|>9)##\n(\"relationship\"<|>\"Lu Jiang\"<|>\"ICML\"<|>\"Lu Jiang presented the Mentornet paper at ICML.\"<|>9)##\n(\"relationship\"<|>\"Angelos Katharopoulos\"<|>\"ICML\"<|>\"Angelos Katharopoulos presented the importance sampling paper at ICML.\"<|>9)##\n(\"relationship\"<|>\"Oldrich Kodym\"<|>\"Pattern Recognition - 40th German Conference, GCPR 2018\"<|>\"Oldrich Kodym presented a paper at GCPR 2018.\"<|>9)##\n(\"relationship\"<|>\"Pattern Recognition - 40th German Conference, GCPR 2018\"<|>\"Stuttgart\"<|>\"The GCPR 2018 conference was held in Stuttgart.\"<|>10)##\n(\"relationship\"<|>\"Stuttgart\"<|>\"Germany\"<|>\"Stuttgart is a city in Germany.\"<|>10)##\n(\"relationship\"<|>\"M. Pawan Kumar\"<|>\"Advances in Neural Information Processing Systems 23\"<|>\"M. Pawan Kumar presented a paper at NeurIPS 2010.\"<|>9)##\n(\"relationship\"<|>\"Advances in Neural Information Processing Systems 23\"<|>\"Vancouver\"<|>\"NeurIPS 2010 was held in Vancouver.\"<|>10)##\n(\"relationship\"<|>\"Vancouver\"<|>\"British Columbia\"<|>\"Vancouver is located in British Columbia.\"<|>10)##\n(\"relationship\"<|>\"British Columbia\"<|>\"Canada\"<|>\"British Columbia is a province in Canada.\"<|>10)##\n(\"relationship\"<|>\"Guillaume Lample\"<|>\"arXiv preprint arXiv:1603.01360\"<|>\"Guillaume Lample is the lead author of the NER architecture paper on arXiv.\"<|>9)##\n(\"relationship\"<|>\"Gina-Anne Levow\"<|>\"Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing\"<|>\"Gina-Anne Levow presented a paper at the Fifth SIGHAN Workshop.\"<|>9)##\n(\"relationship\"<|>\"Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing\"<|>\"Sydney\"<|>\"The workshop was held in Sydney.\"<|>10)##\n(\"relationship\"<|>\"Sydney\"<|>\"Australia\"<|>\"Sydney is a city in Australia.\"<|>10)##\n(\"relationship\"<|>\"Association for Computational Linguistics\"<|>\"Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing\"<|>\"ACL sponsored the SIGHAN Workshop.\"<|>8)##\n(\"relationship\"<|>\"H. Li\"<|>\"2015 IEEE Conference on Computer Vision and Pattern Recognition\"<|>\"H. Li presented a face detection paper at CVPR 2015.\"<|>9)##\n(\"relationship\"<|>\"Xiaoya Li\"<|>\"CoRR, abs/1910.11476\"<|>\"Xiaoya Li is the lead author of the MRC-NER paper posted on CoRR (arXiv).\"<|>9)##\n(\"relationship\"<|>\"Tsung-Yi Lin\"<|>\"Proceedings of the IEEE international conference on computer\"<|>\"Tsung-Yi Lin presented the Focal Loss paper at this IEEE conference.\"<|>9)##\n(\"relationship\"<|>\"Frederic Godin\"<|>\"Ghent University\"<|>\"Frederic Godin completed his Ph.D. at Ghent University.\"<|>10)##\n(\"relationship\"<|>\"Ghent University\"<|>\"Belgium\"<|>\"Ghent University is located in Belgium.\"<|>10)<|COMPLETE|>",
    "model": "qwen3-max"
  }
}