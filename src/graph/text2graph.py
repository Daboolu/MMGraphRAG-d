"""
æ–‡æœ¬çŸ¥è¯†å›¾è°±æ„å»ºæ¨¡å—

ä»æ–‡æœ¬å—ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ã€‚

ä¸»è¦åŠŸèƒ½:
1. éå†æ–‡æœ¬å—
2. ä½¿ç”¨LLMæå–å®ä½“å’Œå…³ç³»
3. åˆå¹¶é‡å¤å®ä½“
4. æ›´æ–°çŸ¥è¯†å›¾è°±
"""
import asyncio
import json
import os
import re
from collections import defaultdict
from functools import partial
from typing import Union, cast, Type
from dataclasses import dataclass

from tqdm import tqdm

from ..core.base import (
    logger,
    pack_user_ass_to_openai_messages,
    split_string_by_multi_markers,
    limit_async_func_call,
)
from .utils import (
    _handle_single_entity_extraction,
    _handle_single_relationship_extraction,
    _merge_nodes_then_upsert,
    _merge_edges_then_upsert,
)
from ..llm import model_if_cache
from ..core.prompt import PROMPTS
from ..core.storage import (
    BaseGraphStorage,
    BaseKVStorage,
    JsonKVStorage,
    NetworkXStorage,
    StorageNameSpace,
    TextChunkSchema,
)
from .. import parameter


# ============================================================================
# å¸¸é‡
# ============================================================================

TUPLE_DELIMITER = PROMPTS["DEFAULT_TUPLE_DELIMITER"]
RECORD_DELIMITER = PROMPTS["DEFAULT_RECORD_DELIMITER"]
COMPLETION_DELIMITER = PROMPTS["DEFAULT_COMPLETION_DELIMITER"]
PROCESS_TICKERS = PROMPTS["process_tickers"]


# ============================================================================
# ä¸»æå–å‡½æ•°
# ============================================================================

async def extract_entities(
    cache_storage: BaseKVStorage,
    chunks: dict[str, TextChunkSchema],
    knwoledge_graph_inst: BaseGraphStorage,
) -> Union[BaseGraphStorage, None]:
    """
    ä»æ–‡æœ¬å—ä¸­æå–å®ä½“å’Œå…³ç³»ã€‚
    
    Args:
        cache_storage: ç¼“å­˜å­˜å‚¨
        chunks: æ–‡æœ¬å—å­—å…¸
        knwoledge_graph_inst: çŸ¥è¯†å›¾è°±å®ä¾‹
    
    Returns:
        æ›´æ–°åçš„çŸ¥è¯†å›¾è°±ï¼Œå¦‚æœæœªæå–åˆ°å®ä½“åˆ™è¿”å›None
    """
    output_path = os.path.join(parameter.WORKING_DIR, "kv_store_chunk_knowledge_graph.json")
    
    llm_func = limit_async_func_call(16)(
        partial(model_if_cache, hashing_kv=cache_storage)
    )
    
    max_gleaning = parameter.ENTITY_EXTRACT_MAX_GLEANING
    ordered_chunks = list(chunks.items())
    
    # æç¤ºæ¨¡æ¿
    entity_prompt = PROMPTS["entity_extraction"]
    continue_prompt = PROMPTS["entity_continue_extraction"]
    loop_prompt = PROMPTS["entity_if_loop_extraction"]
    
    context = {
        "tuple_delimiter": TUPLE_DELIMITER,
        "record_delimiter": RECORD_DELIMITER,
        "completion_delimiter": COMPLETION_DELIMITER,
        "entity_types": ",".join(PROMPTS["DEFAULT_ENTITY_TYPES"]),
    }
    
    # ç»Ÿè®¡è®¡æ•°
    stats = {"processed": 0, "entities": 0, "relations": 0}
    chunk_kg_info = {}
    
    async def process_chunk(chunk_item: tuple[str, TextChunkSchema]) -> tuple[dict, dict]:
        """å¤„ç†å•ä¸ªæ–‡æœ¬å—"""
        chunk_key, chunk_data = chunk_item
        content = chunk_data["content"]
        chunk_index = chunk_data["chunk_order_index"]
        
        # ç¬¬ä¸€æ¬¡æå–
        prompt = entity_prompt.format(**context, input_text=content)
        result = await llm_func(prompt)
        history = pack_user_ass_to_openai_messages(prompt, result)
        
        # è¿­ä»£æå–
        for i in range(max_gleaning):
            glean_result = await llm_func(continue_prompt, history_messages=history)
            history += pack_user_ass_to_openai_messages(continue_prompt, glean_result)
            result += glean_result
            
            if i < max_gleaning - 1:
                should_continue = await llm_func(loop_prompt, history_messages=history)
                if should_continue.strip().strip("'\"").lower() != "yes":
                    break
        
        # è§£æè®°å½•
        records = split_string_by_multi_markers(result, [RECORD_DELIMITER, COMPLETION_DELIMITER])
        
        nodes = defaultdict(list)
        edges = defaultdict(list)
        chunk_result = {"chunk_key": chunk_key, "entities": [], "relationships": []}
        
        for record in records:
            match = re.search(r"\((.*)\)", record)
            if not match:
                continue
            
            attrs = split_string_by_multi_markers(match.group(1), [TUPLE_DELIMITER])
            
            entity = await _handle_single_entity_extraction(attrs, chunk_key)
            if entity:
                nodes[entity["entity_name"]].append(entity)
                chunk_result["entities"].append(entity)
                continue
            
            relation = await _handle_single_relationship_extraction(attrs, chunk_key)
            if relation:
                edges[(relation["src_id"], relation["tgt_id"])].append(relation)
                chunk_result["relationships"].append(relation)
        
        chunk_kg_info[chunk_index] = chunk_result
        return dict(nodes), dict(edges)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡æœ¬å—ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
    tasks = [process_chunk(c) for c in ordered_chunks]
    results = []
    for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="ğŸ“ æ–‡æœ¬å®ä½“æå–", unit="å—"):
        result = await coro
        results.append(result)
    
    # ä¿å­˜å—çº§çŸ¥è¯†å›¾è°±ä¿¡æ¯
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunk_kg_info, f, ensure_ascii=False, indent=2)
    
    # æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹
    all_nodes = defaultdict(list)
    all_edges = defaultdict(list)
    
    for nodes, edges in results:
        for k, v in nodes.items():
            all_nodes[k].extend(v)
        for k, v in edges.items():
            all_edges[tuple(sorted(k))].extend(v)  # æ— å‘å›¾æ’åº
    
    # æ›´æ–°çŸ¥è¯†å›¾è°±
    entities_data = await asyncio.gather(*[
        _merge_nodes_then_upsert(k, v, knwoledge_graph_inst)
        for k, v in all_nodes.items()
    ])
    
    await asyncio.gather(*[
        _merge_edges_then_upsert(k[0], k[1], v, knwoledge_graph_inst)
        for k, v in all_edges.items()
    ])
    
    if not entities_data:
        logger.warning("æœªæå–åˆ°ä»»ä½•å®ä½“")
        return None
    
    return knwoledge_graph_inst


# ============================================================================
# æå–å™¨ç±»
# ============================================================================

@dataclass
class TextEntityExtractor:
    """æ–‡æœ¬å®ä½“æå–å™¨"""
    extraction_func: callable = extract_entities
    kv_storage_cls: Type[BaseKVStorage] = JsonKVStorage
    graph_storage_cls: Type[BaseGraphStorage] = NetworkXStorage
    
    def __post_init__(self):
        self.llm_cache = self.kv_storage_cls(
            namespace="llm_response_cache",
            storage_dir=parameter.CACHE_PATH
        )
        self.graph = self.graph_storage_cls(namespace="chunk_entity_relation")
    
    async def text_entity_extraction(self, chunks: dict):
        """æå–æ–‡æœ¬å®ä½“"""
        try:
            logger.info("ğŸ” æ­£åœ¨æå–å®ä½“...")
            
            result = await self.extraction_func(
                self.llm_cache,
                chunks,
                knwoledge_graph_inst=self.graph,
            )
            
            if result is None:
                logger.warning("æœªæ‰¾åˆ°æ–°å®ä½“")
            else:
                self.graph = result
        finally:
            await self._save()
    
    async def _save(self):
        """ä¿å­˜å­˜å‚¨"""
        tasks = [
            cast(StorageNameSpace, s).index_done_callback()
            for s in [self.llm_cache, self.graph] if s
        ]
        await asyncio.gather(*tasks)