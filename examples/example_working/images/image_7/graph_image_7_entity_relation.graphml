<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d6" for="edge" attr.name="order" attr.type="long"/>
<key id="d5" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d4" for="edge" attr.name="description" attr.type="string"/>
<key id="d3" for="edge" attr.name="weight" attr.type="double"/>
<key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;LOSS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A category representing different types of loss functions used in machine learning models, particularly for classification tasks."</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;CE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Cross-Entropy loss function, defined as the negative sum over binary labels y_ij of y_ij times the logarithm of predicted probability p_ij. Used to measure the performance of a classification model whose output is a probability value between 0 and 1."</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;WCE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Weighted Cross-Entropy loss function, an extension of CE with a weighting factor α_i that adjusts the contribution of each sample based on class imbalance or other criteria."</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;DL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Dice Loss function, defined as 1 minus a fraction involving predicted probabilities p_i1, true labels y_i1, and a smoothing term γ. Designed to improve segmentation accuracy by focusing on overlap between predicted and ground truth regions."</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;TL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Tversky Loss function, a generalization of Dice Loss incorporating two weighting parameters α and β to control the sensitivity to false positives and false negatives. The formula includes terms for both positive and negative predictions adjusted by γ."</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;DSC&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Dice Similarity Coefficient loss, defined as 1 minus a fraction involving (1-p_i1), p_i1, y_i1, and γ. It measures the similarity between predicted and actual segmentations, commonly used in medical image analysis."</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;FL&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Focal Loss function, designed to address class imbalance by down-weighting easy examples and focusing more on hard-to-classify samples. It uses a modulating factor (1-p_ij) raised to the power γ, multiplied by the log probability."</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;FORMULA (ONE SAMPLE X_I)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The mathematical expression defining each loss function for a single sample x_i, involving predicted probabilities p_ij, true labels y_ij, and hyperparameters such as α, β, γ."</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;IMAGE_7&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">IMAGE_7 is a table titled "Table 2: Different losses and their formulas," which presents six loss functions commonly used in machine learning classification tasks, along with their corresponding mathematical formulations for a single sample \( x_i \). The table consists of two columns: the first lists the abbreviations of the loss functions—CE (Cross-Entropy), WCE (Weighted Cross-Entropy), DL (Dice Loss), TL (Tversky Loss), DSC (Dice Similarity Coefficient Loss), and FL (Focal Loss)—while the second column provides their respective formulas. These formulas involve variables such as \( y_{ij} \) (the true label), \( p_{ij} \) (the predicted probability), and hyperparameters or smoothing factors denoted by \( \alpha \), \( \beta \), and \( \gamma \).

Specifically:
- **CE** is defined as \( -\sum_{j\in\{0,1\}} y_{ij} \log p_{ij} \).
- **WCE** introduces a weighting factor \( \alpha_i \): \( -\alpha_i \sum_{j\in\{0,1\}} y_{ij} \log p_{ij} \).
- **DL** is given by \( 1 - \frac{2p_{i1}y_{i1} + \gamma}{p_{i1}^2 + y_{i1}^2 + \gamma} \).
- **TL** incorporates asymmetric penalties via \( \alpha \) and \( \beta \): \( 1 - \frac{p_{i1}y_{i1} + \gamma}{p_{i1}y_{i1} + \alpha p_{i1}y_{i0} + \beta p_{i0}y_{i1} + \gamma} \).
- **DSC** uses a modified form: \( 1 - \frac{2(1-p_{i1})p_{i1}y_{i1} + \gamma}{(1-p_{i1})p_{i1} + y_{i1} + \gamma} \).
- **FL** adds a modulating factor to focus on hard examples: \( -\alpha_i \sum_{j\in\{0,1\}} (1 - p_{ij})^\gamma \log p_{ij} \).

The</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<edge source="&quot;LOSS&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Loss是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LOSS&quot;" target="&quot;CE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"CE is a type of loss function within the broader category of loss functions."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LOSS&quot;" target="&quot;WCE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"WCE is a variant of the loss function family, specifically a weighted version of cross-entropy."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LOSS&quot;" target="&quot;DL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"DL is a type of loss function focused on improving segmentation through dice-based metrics."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LOSS&quot;" target="&quot;TL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"TL is a generalized form of loss function that extends the concept of DL with tunable parameters."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LOSS&quot;" target="&quot;DSC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"DSC is a specific loss formulation derived from the Dice coefficient, used to evaluate segmentation quality."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LOSS&quot;" target="&quot;FL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"FL is a specialized loss function designed to handle class imbalance issues in classification tasks."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CE&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CE是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CE&quot;" target="&quot;WCE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"WCE builds upon CE by introducing a weight α_i to adjust the influence of individual samples, making it suitable for imbalanced datasets."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CE&quot;" target="&quot;FORMULA (ONE SAMPLE X_I)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The formula for CE is expressed using the summation over binary labels and log probabilities, which is part of the general structure described under Formula (one sample x_i)."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;WCE&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"WCE是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;WCE&quot;" target="&quot;FORMULA (ONE SAMPLE X_I)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The WCE formula incorporates α_i into the CE framework, fitting within the overall formula structure for one sample."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DL&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"DL是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DL&quot;" target="&quot;TL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"TL generalizes DL by incorporating additional parameters α and β to balance false positives and false negatives."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DL&quot;" target="&quot;DSC&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"DSC is closely related to DL, as both are based on the Dice coefficient but may differ slightly in implementation or normalization."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DL&quot;" target="&quot;FORMULA (ONE SAMPLE X_I)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The DL formula uses p_i1, y_i1, and γ in a fractional form consistent with the general format provided for one sample."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TL&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"TL是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TL&quot;" target="&quot;FORMULA (ONE SAMPLE X_I)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The TL formula expands on DL with extra parameters α and β, still adhering to the same per-sample computational pattern."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DSC&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"DSC是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DSC&quot;" target="&quot;FORMULA (ONE SAMPLE X_I)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The DSC formula follows the same structural pattern as other losses, applying a transformation based on p_i1, y_i1, and γ."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FL&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"FL是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FL&quot;" target="&quot;FORMULA (ONE SAMPLE X_I)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The FL formula introduces a focal term (1-p_ij)^γ, modifying the standard log probability term while remaining consistent with the per-sample formula style."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FORMULA (ONE SAMPLE X_I)&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Formula (one sample x_i)是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
</graph></graphml>