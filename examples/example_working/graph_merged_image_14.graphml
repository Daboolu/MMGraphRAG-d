<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d6" for="edge" attr.name="order" attr.type="long"/>
<key id="d5" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d4" for="edge" attr.name="description" attr.type="string"/>
<key id="d3" for="edge" attr.name="weight" attr.type="double"/>
<key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;FIGURE 1&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">Figure 1 is an illustration depicting the derivatives of four loss functions, highlighting how different losses behave with respect to prediction probability p, particularly around the threshold of 0.5. It is a 2D line graph plotting derivatives on the y-axis against the probability of the ground-truth label divided by p̂i on the x-axis, ranging from 0 to 1. The graph contains four distinct curves representing different loss functions: Focal Loss (FL) with γ=1 (blue), Dice Loss (DL) with γ=1 (orange), Tversky Loss (TL) with β=0.5 (yellow), and Dice Similarity Coefficient (DSC) (purple). A legend in the top-right corner provides color-coded labels for each curve. Axes are labeled 'Probability of the ground-truth label / p̂i' (x-axis, 0–1) and 'Derivatives' (y-axis, –2 to 2).</data>
  <data key="d2">examples/example_working/images/image_14.jpg&lt;SEP&gt;chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
</node>
<node id="&quot;∇ FL(Γ=1)&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">A blue curve representing the derivative of Focal Loss with γ=1, starting at approximately -2 when the probability is 0 and increasing to near 0 as the probability approaches 1. Focal Loss (FL) is a loss function originally proposed by Lin et al. (2017) for object detection to mitigate class imbalance by down-weighting well-classified examples using a (1 - p)^γ factor.</data>
  <data key="d2">examples/example_working/images/image_14.jpg</data>
</node>
<node id="&quot;∇ DL(Γ=1)&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">An orange curve representing the derivative of Dice Loss with γ=1, starting at -1 when the probability is 0 and gradually increasing to cross zero around 0.85 probability. Dice Loss (DL) is a loss function derived from the Dice Coefficient, used in training machine learning models, particularly in segmentation tasks. It exists in individual-sample and set-level formulations and often includes a smoothing factor γ.</data>
  <data key="d2">examples/example_working/images/image_14.jpg</data>
</node>
<node id="&quot;∇ TL(Β=0.5)&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">A yellow curve representing the derivative of Tversky Loss with β=0.5, starting at -2 when the probability is 0 and rising smoothly to cross zero around 0.7 probability. Tversky Loss (TL) is a loss function derived from the Tversky Index, designed to improve model training by adjusting penalties for false positives and false negatives.</data>
  <data key="d2">examples/example_working/images/image_14.jpg</data>
</node>
<node id="&quot;∇ DSC&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">A purple curve representing the derivative of Dice Similarity Coefficient (DSC), starting at -1 when the probability is 0 and steadily increasing to reach approximately 1 when the probability is 1. Dice Similarity Coefficient (DSC) is a measure of overlap between two sets, referenced as a special case of the Tversky Index when α = β = 0.5, and serves as a basis for several loss functions discussed.</data>
  <data key="d2">examples/example_working/images/image_14.jpg</data>
</node>
<node id="&quot;Legend&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">A box typically positioned in the top-right corner of the graph that provides color-coded labels corresponding to the derivatives of different loss functions: blue for ∇ FL(γ=1) (Focal Loss), orange for ∇ DL(γ=1) (Dice Loss), yellow for ∇ TL(β=0.5) (Tversky Loss with β=0.5), and purple for ∇ DSC (Dice Coefficient loss). These visual cues help distinguish how each loss function's gradient behaves with respect to prediction probability.</data>
  <data key="d2">examples/example_working/images/image_14.jpg</data>
</node>
<node id="&quot;X-Axis&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">The horizontal axis of the graph, labeled 'Probability of the ground-truth label / p̂i', which spans from 0 to 1 with tick marks at intervals of 0.1. This axis represents the model's predicted probability for the true class of a given sample, illustrating how loss function derivatives respond across the full range of confidence levels—from completely uncertain (0) to fully confident (1).</data>
  <data key="d2">examples/example_working/images/image_14.jpg</data>
</node>
<node id="&quot;Y-Axis&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">The vertical axis of the graph, labeled 'Derivatives', ranging from -2 to 2 with tick marks at intervals of 0.5. It depicts the gradient (derivative) of various loss functions with respect to the predicted probability, showing how strongly each loss pushes the model to adjust its predictions—especially critical near decision boundaries like p = 0.5, where behaviors of DSC, FL, DL, and TL diverge significantly.</data>
  <data key="d2">examples/example_working/images/image_14.jpg</data>
</node>
<node id="&quot;IMAGE_14&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">IMAGE_14 is a line graph that visualizes the derivatives of four distinct loss functions with respect to the probability of the ground-truth label, denoted as \( \bar{p}_i \), which varies from 0 to 1 along the x-axis. The y-axis, labeled "Derivatives," spans from -2 to 2. Each curve corresponds to a specific loss function, as identified in the legend: blue for ∇ FL(γ=1) (Focal Loss with γ=1), orange for ∇ DL(γ=1) (Dice Loss with γ=1), yellow for ∇ TL(β=0.5) (Tversky Loss with β=0.5), and purple for ∇ DSC (Dice Similarity Coefficient).

The blue curve (∇ FL) starts at approximately -2 when \( \bar{p}_i = 0 \), remains relatively flat near this value until around \( \bar{p}_i = 0.5 \), and then rises sharply to reach about 0.5 at \( \bar{p}_i = 1 \). The orange curve (∇ DL) begins at -1 when \( \bar{p}_i = 0 \), increases gradually, crosses zero near \( \bar{p}_i = 0.7 \), and plateaus at approximately 0.5 when \( \bar{p}_i = 1 \). The yellow curve (∇ TL) originates at -2 at \( \bar{p}_i = 0 \), rises slowly, crosses zero around \( \bar{p}_i = 0.6 \), and reaches roughly 0.4 at \( \bar{p}_i = 1 \). The purple curve (∇ DSC) starts at -1 when \( \bar{p}_i = 0 \), increases steadily, crosses zero near \( \bar{p}_i = 0.6 \), and attains a value of approximately 1 at \( \bar{p}_i = 1 \).

All curves are smooth and continuous, illustrating how each loss function’s gradient responds to varying levels of prediction confidence. The graph underscores the heightened sensitivity of Focal Loss (blue) to low-confidence predictions—evident from its steep derivative increase after \( \bar{p}_i = 0.5 \)—and provides insight into the distinct</data>
  <data key="d2">examples/example_working/images/image_14.jpg</data>
</node>
<node id="&quot;IMAGE_13&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula representing a modified version of the F1 score, specifically tailored for binary classification tasks involving probabilistic predictions. The formula is written as: F1(x_i) = 2 \frac{\mathbb{I}(p_{i1} &gt; 0.5)y_{i1}}{\mathbb{I}(p_{i1} &gt; 0.5) + y_{i1}}Here, x_i denotes an input example, p_{i1} represents the predicted probability that the example belongs to class 1 (positive class), and y_{i1} is the true label (binary: 0 or 1). The indicator function \mathbb{I}(p_{i1} &gt; 0.5) evaluates to 1 if the predicted probability exceeds 0.5, and 0 otherwise. This formulation computes the F1 score based on hard decisions derived from thresholding probabilities at 0.5, rather than using continuous probabilities directly. The context explains that this form contrasts with a 'soft' F1 variant (Eq.5) which uses continuous probabilities, and highlights the potential issue when training data contains many easy-negative examples—these can dominate training due to their low prediction probabilities being easily pushed to zero, while hard-negative examples remain indistinguishable from positives, degrading final F1 performance. The equation is presented in standard LaTeX-style mathematical notation, centered and clearly formatted, with no additional visual elements such as graphs, tables, or diagrams."</data>
  <data key="d2">examples/example_working/images/image_13.jpg</data>
</node>
<node id="&quot;F1 SCORE / DICE COEFFICIENT&quot;">
  <data key="d0">CONCEPT</data>
  <data key="d1">The F1 score for a given instance, mathematically represented as F1(x_i), is equivalent to the Dice Coefficient (DSC)—an F1-oriented statistic used to measure the similarity between two sets, commonly applied in evaluating model predictions against ground truth in machine learning contexts. It is calculated using true labels (Y_I1), predicted probabilities (P_I1), and an indicator function I(P_I1 &gt; 0.5) to determine positive predictions, with a standard formulation involving a factor of 2 in the numerator.</data>
  <data key="d2">examples/example_working/images/image_13.jpg</data>
</node>
<node id="&quot;Numerical constant multiplier (2) in the Dice coefficient formula&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">The constant '2' serves as a numerical multiplier in the Dice coefficient (DSC) formula, appearing in both the numerator of the set-based and sample-wise formulations. It ensures proper normalization when computing the similarity between predicted and ground truth sets by accounting for the intersection relative to the sum of set cardinalities. In the context of binary classification, it scales the true positives (TP) to balance precision and recall in the F1-score-equivalent expression of DSC.</data>
  <data key="d2">examples/example_working/images/image_13.jpg</data>
</node>
<node id="&quot;Indicator function I(p_i1 &gt; 0.5)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">This is a binary indicator function used in the hard-thresholded computation of the F1 score for an individual example x_i. It evaluates to 1 if the model's predicted probability p_i1 for the positive class exceeds 0.5, indicating a positive prediction, and 0 otherwise. Unlike the soft probabilistic version used in standard Dice loss, this discrete formulation reflects actual classification decisions and is referenced to highlight the gap between optimization objectives (soft Dice) and evaluation metrics (hard F1).</data>
  <data key="d2">examples/example_working/images/image_13.jpg</data>
</node>
<node id="&quot;True label Y_i1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">Y_i1 denotes the ground truth binary label for instance x_i, where a value of 1 indicates the instance belongs to the positive class and 0 indicates the negative class. In the context of Dice coefficient and related losses (e.g., Dice Loss, Tversky Loss), Y_i1 is used alongside the predicted probability p_i1 to compute similarity or discrepancy at the sample level. It plays a critical role in determining whether an example contributes to the loss, especially after smoothing with a gamma term that allows even negative examples (Y_i1 = 0) to influence training.</data>
  <data key="d2">examples/example_working/images/image_13.jpg</data>
</node>
<node id="&quot;IMAGE_12&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula representing the Tversky Loss (TL), which is a loss function used in machine learning, particularly in segmentation tasks. The formula is written in LaTeX-style mathematical notation and reads: TL = \frac{1}{N} \sum_{i} \left[ 1 - \frac{p_{i1} y_{i1} + \gamma}{p_{i1} y_{i1} + \alpha \, p_{i1} y_{i0} + \beta \, p_{i0} y_{i1} + \gamma} \right]. In this equation, TL denotes the Tversky Loss, N is the total number of samples, and the summation is over individual samples i. The terms p_{i1} and p_{i0} represent the predicted probabilities for class 1 and class 0, respectively, for sample i. Similarly, y_{i1} and y_{i0} are the true labels for class 1 and class 0, respectively. The parameters \alpha and \beta control the trade-off between false positives and false negatives, with \gamma being a small constant added to avoid division by zero. When \alpha = \beta = 0.5, the Tversky loss reduces to the Dice Similarity Coefficient (DSC). The context indicates that this loss function is part of a discussion on self-adjusting Dice loss, where the behavior of the model under specific conditions (e.g., binary classification) is analyzed. The formula is presented in a clean, black-on-white format typical of academic or technical documents."</data>
  <data key="d2">examples/example_working/images/image_12.jpg</data>
</node>
<node id="&quot;TVERSKY LOSS (TL)&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">Tversky Loss (TL) is a loss function derived from the Tversky Index, designed to improve model training by adjusting penalties for false positives and false negatives. It is a generalization of Dice Loss incorporating two weighting parameters α and β to control the sensitivity to false positives and false negatives. The formula includes terms for both positive and negative predictions adjusted by γ.</data>
  <data key="d2">examples/example_working/images/image_7.jpg&lt;SEP&gt;chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
</node>
<node id="&quot;i&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">An index variable ranging from 1 to N, used to iterate over individual data points in the summation. In the context of Dice and Tversky losses, it refers to the i-th sample $x_i$ for which predictions and labels are evaluated.</data>
  <data key="d2">examples/example_working/images/image_12.jpg</data>
</node>
<node id="&quot;p_{i1}&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">The predicted probability assigned by a model to the positive class for pixel or instance x_i. In binary segmentation or classification, p_{i1} ∈ [0, 1] represents the model's confidence that x_i belongs to the foreground or positive class, with p_{i0} = 1 − p_{i1} for the negative class.</data>
  <data key="d2">examples/example_working/images/image_8.jpg</data>
</node>
<node id="&quot;y_{i1}&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">The true binary label (ground truth) for pixel or instance x_i, indicating whether it belongs to the positive class. y_{i1} ∈ {0, 1}, where 1 denotes a positive (foreground) label and 0 denotes a negative (background) label. This is used alongside predicted probabilities in loss functions like Dice Loss and Cross Entropy.</data>
  <data key="d2">examples/example_working/images/image_8.jpg</data>
</node>
<node id="&quot;\gamma&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">A small positive constant (often set to 1 in practice) added to both numerator and denominator of Dice and Tversky formulations to avoid division by zero and ensure numerical stability, especially when dealing with negative examples or sparse predictions.</data>
  <data key="d2">examples/example_working/images/image_12.jpg</data>
</node>
<node id="&quot;\alpha&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">A scaling hyperparameter in the Tversky Loss (TL) that weights the contribution of false positives (specifically the term $p_{i1} y_{i0}$) in the denominator. It controls the penalty for false positives and allows tuning the trade-off between precision and recall.</data>
  <data key="d2">examples/example_working/images/image_12.jpg</data>
</node>
<node id="&quot;\beta&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">A scaling hyperparameter in the Tversky Loss (TL) that weights the contribution of false negatives (specifically the term $p_{i0} y_{i1}$) in the denominator. Like $\alpha$, it enables flexible control over the balance between false negatives and false positives; when $\alpha = \beta = 0.5$, Tversky reduces to the Dice coefficient.</data>
  <data key="d2">examples/example_working/images/image_12.jpg</data>
</node>
<node id="&quot;p_{i0}&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">Probability assigned to class 0 for the i-th sample, complementary to $p_{i1}$ under a binary classification assumption (i.e., $p_{i0} = 1 - p_{i1}$). It appears in Tversky Loss to model the predicted negative class probability.</data>
  <data key="d2">examples/example_working/images/image_12.jpg</data>
</node>
<node id="&quot;y_{i0}&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">Binary label indicating whether the i-th sample belongs to class 0 (value 1) or not (value 0), complementary to $y_{i1}$ (i.e., $y_{i0} = 1 - y_{i1}$). It is used in Tversky Loss to represent ground-truth negative labels.</data>
  <data key="d2">examples/example_working/images/image_12.jpg</data>
</node>
<node id="&quot;IMAGE_11&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula for the Tversky Index (TI), which is used to measure the similarity between two sets, A and B. The formula is presented in a clean, centered LaTeX-style typeset format. It reads: TI = \frac{|A \cap B|}{|A \cap B| + \alpha |A \setminus B| + \beta |B \setminus A|}The numerator represents the size of the intersection of sets A and B, denoted as |A ∩ B|. The denominator includes the same intersection term plus weighted terms for the set differences: α times the size of A minus B (|A \ B|), and β times the size of B minus A (|B \ A|). The parameters α and β are weighting factors that allow control over the penalty for false positives and false negatives, respectively. When α = β = 0.5, the Tversky Index reduces to the Dice Similarity Coefficient (DSC). This formula is commonly used in machine learning, particularly in segmentation tasks, where it serves as a loss function or evaluation metric. The context explains that this index generalizes the Dice coefficient and offers flexibility in balancing false positives and false negatives, making it suitable for scenarios requiring nuanced similarity measures."</data>
  <data key="d2">examples/example_working/images/image_11.jpg</data>
</node>
<node id="&quot;TVERSKY INDEX (TI)&quot;">
  <data key="d0">CONCEPT</data>
  <data key="d1">The Tversky Index (TI) is a generalization of the Dice Coefficient that approximates the Fβ score, allowing for asymmetric penalization of false positives and false negatives. It is mathematically defined as a ratio involving the intersection of two sets A and B in the numerator, and a weighted sum of set differences (A \ B and B \ A) scaled by coefficients α and β in the denominator. TI serves as the basis for Tversky Loss and includes the Dice Similarity Coefficient as a special case when α = β = 0.5.</data>
  <data key="d2">examples/example_working/images/image_11.jpg&lt;SEP&gt;chunk-bff777a634a5af0182c5787f1022b29f</data>
</node>
<node id="&quot;α (alpha)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">A scalar coefficient (denoted as α) that weights the contribution of the set difference A \ B (false positives) in the denominator of the Tversky Index. By adjusting α, one can control the emphasis on penalizing false positives; for example, setting α = β = 0.5 recovers the Dice Similarity Coefficient (DSC).</data>
  <data key="d2">examples/example_working/images/image_11.jpg</data>
</node>
<node id="&quot;β (beta)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">A scalar coefficient (denoted as β) that weights the contribution of the set difference B \ A (false negatives) in the denominator of the Tversky Index. Adjusting β allows flexible trade-offs between recall and precision, making TI a generalization of the Fβ score.</data>
  <data key="d2">examples/example_working/images/image_11.jpg</data>
</node>
<node id="&quot;Tversky Index (TI) Formula&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">A mathematical expression defining the Tversky Index (TI) as a similarity measure between two sets A and B: TI = |A ∩ B| / (|A ∩ B| + α|A \ B| + β|B \ A|). This formula generalizes the Dice Coefficient and approximates the Fβ score, offering tunable sensitivity to false positives and false negatives via coefficients α and β. When α = β = 0.5, TI reduces to the Dice Similarity Coefficient (DSC).</data>
  <data key="d2">examples/example_working/images/image_11.jpg</data>
</node>
<node id="&quot;IMAGE_10&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula representing the Dice Loss (DL) in the context of set-level computation, commonly used in machine learning tasks such as segmentation or classification. The formula is written in LaTeX-style mathematical notation and reads: $$\mathrm{DL} = 1 - \frac{2 \sum_i p_{i1} y_{i1} + \gamma}{\sum_i p_{i1}^2 + \sum_i y_{i1}^2 + \gamma}$$Here, $\mathrm{DL}$ denotes the Dice Loss, which measures the dissimilarity between predicted probabilities $p_{i1}$ and true labels $y_{i1}$ across all instances $i$. The numerator includes twice the sum of element-wise products of predictions and labels, regularized by a small constant $\gamma$ to prevent division by zero. The denominator consists of the sum of squared predictions and squared true labels, also regularized by $\gamma$. This formulation is designed to improve optimization stability by computing a set-level Dice coefficient rather than summing individual ones. The context indicates that this version of DL is related to the Tversky index (TI), a generalization of the Dice coefficient that approximates the $F_\beta$ score. The equation is presented in clean, black serif font on a white background, typical of academic or technical documentation."</data>
  <data key="d2">examples/example_working/images/image_10.jpg</data>
</node>
<node id="&quot;γ (gamma)&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">A small smoothing constant (often set to 1) added to both the numerator and denominator in Dice-based loss functions to ensure numerical stability and prevent division by zero. It also allows negative examples to contribute to the loss during training, improving gradient flow for imbalanced datasets.</data>
  <data key="d2">examples/example_working/images/image_8.jpg</data>
</node>
<node id="&quot;Σ (summation operator)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The mathematical summation symbol used to aggregate terms across all training instances i in loss functions such as Cross Entropy, Dice Loss, and Tversky Loss. In set-level Dice Loss formulations, separate summations over p_{i1}^2 and y_{i1}^2 appear in the denominator, while the numerator sums the product p_{i1} y_{i1}, reflecting global rather than per-sample optimization.</data>
  <data key="d2">examples/example_working/images/image_10.jpg</data>
</node>
<node id="&quot;IMAGE_9&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula representing the Dice Loss (DL) used in machine learning, particularly in segmentation tasks. The formula is written in LaTeX-style mathematical notation and reads: \[ \mathrm{DL} = \frac{1}{N} \sum_{i} \left[ 1 - \frac{2p_{i1}y_{i1} + \gamma}{p_{i1}^2 + y_{i1}^2 + \gamma} \right] \]This equation computes the average loss across N samples, where each term in the summation corresponds to a single sample i. The variables involved are:- \( p_{i1} \): predicted probability for class 1 of sample i,- \( y_{i1} \): true label (binary) for class 1 of sample i,- \( \gamma \): a small positive constant (smoothing factor) added to avoid division by zero and stabilize training.The numerator contains the product of predictions and labels scaled by 2, plus the smoothing term \( \gamma \), while the denominator uses the sum of squares of both prediction and label values, also including \( \gamma \). This formulation is a modified version of the standard Dice coefficient loss, designed to improve convergence speed by using squared terms in the denominator, as suggested by Milletari et al. (2016). The context indicates that this form of DL allows negative examples (where \( y_{i1} = 0 \)) to contribute meaningfully to training, with their DSC contribution being \( \frac{\gamma}{p_{i1} + \gamma} \). The overall structure of the formula emphasizes its use in optimizing models for binary segmentation tasks, such as medical image analysis or object detection, where balanced performance between foreground and background is critical."</data>
  <data key="d2">examples/example_working/images/image_9.jpg</data>
</node>
<node id="&quot;N (Total Number of Samples)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">In the context of machine learning and statistical modeling, 'N' denotes the total number of data points or samples in the training dataset. As shown in the provided text, N appears in loss function formulations such as Cross Entropy Loss and Dice Loss, where it serves as a normalization factor—e.g., in the equation CE = -1/N ∑ᵢ∑ⱼ yᵢⱼ log pᵢⱼ—to ensure the loss is averaged over all samples.</data>
  <data key="d2">examples/example_working/images/image_9.jpg</data>
</node>
<node id="&quot;γ (smoothing factor)&quot;">
  <data key="d0">CONCEPT</data>
  <data key="d1">The symbol 'γ' (gamma) is a smoothing factor commonly used in Dice Loss and related formulations to avoid division by zero and stabilize training. It appears in mathematical expressions of loss functions derived from the Dice Coefficient.</data>
  <data key="d2">examples/example_working/images/image_9.jpg&lt;SEP&gt;examples/example_working/images/image_7.jpg&lt;SEP&gt;chunk-bff777a634a5af0182c5787f1022b29f</data>
</node>
<node id="&quot;π₁ (pi1)&quot;">
  <data key="d2">examples/example_working/images/image_9.jpg</data>
  <data key="d1">The symbol 'π₁' (pi1) appears to be an extracted mathematical entity from image_9, though it is not explicitly referenced or defined in the provided chunk_text. Given the context of the document—which discusses probabilistic predictions, class imbalances, and loss functions—it may potentially represent a class prior probability, a weighting factor, or a model parameter, but no direct mention confirms its role in this excerpt.</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;yᵢ₁ (yi1)&quot;">
  <data key="d2">examples/example_working/images/image_9.jpg</data>
  <data key="d1">The symbol 'yᵢ₁' (yi1) refers to the ground-truth binary label for class 1 of the i-th training instance in a binary classification setting, as clearly defined in Section 3.1 of the chunk_text. Specifically, each instance xᵢ has a label vector yᵢ = [yᵢ₀, yᵢ₁], where yᵢ₁ ∈ {0, 1} indicates whether the instance belongs to the positive class. This notation is central to the formulation of loss functions like Cross Entropy and Dice Loss discussed in the text.</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;Σ (Summation Operator)&quot;">
  <data key="d2">examples/example_working/images/image_9.jpg</data>
  <data key="d1">The symbol 'Σ' (sigma) is the mathematical summation operator, frequently used throughout the chunk_text in equations defining loss functions. For example, in the Cross Entropy Loss formula CE = -1/N ∑ᵢ ∑ⱼ yᵢⱼ log pᵢⱼ, Σ aggregates contributions across all samples (i) and classes (j). It also appears in Dice Loss formulations that sum over predicted and true labels across the dataset. Its presence in image_9 likely reflects its use in these core mathematical expressions related to model training and evaluation.</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;IMAGE_8&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula representing the Dice Similarity Coefficient (DSC) for a given sample \( x_i \). The formula is written in LaTeX-style mathematical notation and reads: \[ \text{DSC}(x_i) = \frac{2p_{i1}y_{i1} + \gamma}{p_{i1} + y_{i1} + \gamma} \]Here, \( p_{i1} \) represents the predicted probability of the positive class for sample \( x_i \), \( y_{i1} \) is the true label (typically binary, with 1 indicating the positive class), and \( \gamma \) is a small smoothing constant added to prevent division by zero and improve numerical stability. This formulation is commonly used in medical image segmentation tasks where class imbalance is prevalent. The numerator includes twice the product of the prediction and ground truth, emphasizing agreement between them, while the denominator sums all relevant values, ensuring normalization. The context provided indicates that this DSC is part of a broader discussion on loss functions, particularly in relation to Dice Loss (DL), and notes that adding a constant (like +1) ensures positivity in certain loss formulations. Additionally, it references Milletari et al. (2016), who proposed modifying the denominator to a squared form for faster convergence, though this specific variant uses a linear denominator. The image contains no visual elements beyond the mathematical expression; it is purely symbolic and analytical."</data>
  <data key="d2">examples/example_working/images/image_8.jpg</data>
</node>
<node id="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;">
  <data key="d0">CONCEPT</data>
  <data key="d1">Dice Similarity Coefficient (DSC) is a measure of overlap between two sets, referenced as a special case of the Tversky Index when α = β = 0.5, and serves as a basis for several loss functions discussed. It is a similarity metric defined by Sørensen (1948) and Dice (1945), used to compare sets and adapted in machine learning for imbalanced tasks, often applied in image segmentation tasks.</data>
  <data key="d2">examples/example_working/images/image_6.jpg&lt;SEP&gt;examples/example_working/images/image_5.jpg&lt;SEP&gt;examples/example_working/images/image_4.jpg&lt;SEP&gt;chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
</node>
<node id="&quot;x_i&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">A pixel or region in an image, represented as a variable in segmentation or classification formulas. In the context of binary classification tasks, x_i denotes an individual training instance (e.g., a pixel) for which predictions and ground truth labels are defined.</data>
  <data key="d2">examples/example_working/images/image_8.jpg</data>
</node>
<node id="&quot;Loss Functions in Machine Learning&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">A category encompassing various loss functions used in machine learning models, especially for classification tasks. These functions quantify the discrepancy between predicted outputs and true labels, guiding model optimization. Common types include Cross-Entropy (CE), Weighted Cross-Entropy (WCE), Dice Loss (DL), Tversky Loss (TL), and Focal Loss-inspired variants, each designed to address specific challenges such as class imbalance, easy-negative dominance, or optimization stability.</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;Cross-Entropy Loss (CE)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">Cross-Entropy Loss (CE), also referred to as the maximum likelihood estimation (MLE) objective, is a standard loss function used in machine learning for classification tasks, especially in natural language processing. It is defined as CE = -1/N ∑_i ∑_{j∈{0,1}} y_ij log p_ij, where N is the total number of samples, y_ij is the true label for sample i and class j, and p_ij is the predicted probability. CE treats all training examples equally in the objective function, which can lead to poor performance on imbalanced datasets due to bias toward the majority class and the overwhelming influence of easy-negative examples.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;Weighted Cross-Entropy Loss (WCE)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">Weighted Cross-Entropy Loss (WCE) extends the standard Cross-Entropy Loss by introducing a sample-specific or class-specific weighting factor α_i to adjust the contribution of each instance during training. This is particularly useful in imbalanced datasets, where minority classes are assigned higher weights—often computed using inverse class frequency or formulas like log((n − n_t)/n_t + K), where n_t is the count of class t and n is the total number of samples. While effective, improper weighting can bias the model toward rare classes.</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;FOCAL LOSS (FL)&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">Focal Loss (FL) is a loss function originally proposed by Lin et al. (2017) for object detection to mitigate class imbalance by down-weighting well-classified examples using a (1 - p)^γ factor. It is designed to address class imbalance by down-weighting easy examples and focusing more on hard-to-classify samples, using a modulating factor (1-p_ij) raised to the power γ, multiplied by the log probability.</data>
  <data key="d2">examples/example_working/images/image_7.jpg&lt;SEP&gt;chunk-76ac26952e67515817b1d024e77de8e0&lt;SEP&gt;chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
</node>
<node id="&quot;Loss Function Formula (One Sample x_i)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">This refers to the mathematical formulation of various loss functions applied to a single training instance x_i. These formulas typically involve predicted probabilities p_ij, ground-truth binary labels y_ij, and optional hyperparameters such as weighting coefficients (α, β), smoothing constants (γ), or adaptive terms like (1 − p_ij). Examples include CE: −∑ y_ij log p_ij; WCE: −α_i ∑ y_ij log p_ij; Dice Loss (DL): 1 − (2 p_i1 y_i1 + γ)/(p_i1² + y_i1² + γ); and Tversky Loss (TL): 1 − (p_i1 y_i1 + γ)/(p_i1 y_i1 + α p_i1 y_i0 + β p_i0 y_i1 + γ). These expressions enable fine-grained control over how individual samples influence model learning, especially in contexts like class imbalance or hard example mining.</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;IMAGE_7&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">IMAGE_7 is a table titled "Table 2: Different losses and their formulas," which presents six loss functions commonly used in machine learning classification tasks, along with their corresponding mathematical formulations for a single sample \( x_i \). The table consists of two columns: the first lists the abbreviations of the loss functions—CE (Cross-Entropy), WCE (Weighted Cross-Entropy), DL (Dice Loss), TL (Tversky Loss), DSC (Dice Similarity Coefficient Loss), and FL (Focal Loss)—while the second column provides their respective formulas. These formulas involve variables such as \( y_{ij} \) (the true label), \( p_{ij} \) (the predicted probability), and hyperparameters or smoothing factors denoted by \( \alpha \), \( \beta \), and \( \gamma \).

Specifically:
- **CE** is defined as \( -\sum_{j\in\{0,1\}} y_{ij} \log p_{ij} \).
- **WCE** introduces a weighting factor \( \alpha_i \): \( -\alpha_i \sum_{j\in\{0,1\}} y_{ij} \log p_{ij} \).
- **DL** is given by \( 1 - \frac{2p_{i1}y_{i1} + \gamma}{p_{i1}^2 + y_{i1}^2 + \gamma} \).
- **TL** incorporates asymmetric penalties via \( \alpha \) and \( \beta \): \( 1 - \frac{p_{i1}y_{i1} + \gamma}{p_{i1}y_{i1} + \alpha p_{i1}y_{i0} + \beta p_{i0}y_{i1} + \gamma} \).
- **DSC** uses a modified form: \( 1 - \frac{2(1-p_{i1})p_{i1}y_{i1} + \gamma}{(1-p_{i1})p_{i1} + y_{i1} + \gamma} \).
- **FL** adds a modulating factor to focus on hard examples: \( -\alpha_i \sum_{j\in\{0,1\}} (1 - p_{ij})^\gamma \log p_{ij} \).

The</data>
  <data key="d2">examples/example_working/images/image_7.jpg</data>
</node>
<node id="&quot;TABLE 2&quot;">
  <data key="d0">"TABLE"</data>
  <data key="d1">"Table 2 presents a comparison of different loss functions and their mathematical formulas, including CE, WCE, DL, TL, and DSC FL, with notes on smoothing and positivity adjustments."&lt;SEP&gt;"Table 2 summarizes the various loss functions discussed in the paper, serving as a reference point for experimental analysis."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-bff777a634a5af0182c5787f1022b29f</data>
</node>
<node id="&quot;IMAGE_6&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula for the Dice Similarity Coefficient (DSC) applied to an individual example $x_i$. The equation is presented in a clean, typeset format typical of academic or technical documents. The formula is written as: $$ \mathrm{DSC}(x_i) = \frac{2p_{i1}y_{i1}}{p_{i1} + y_{i1}} $$ where $p_{i1}$ represents the predicted probability of class 1 for example $x_i$, and $y_{i1}$ is the true label (0 or 1) for that example. This formulation indicates that the DSC is computed based on the product of the predicted and actual values, scaled by twice the numerator divided by their sum. The context explains that when $y_{i1} = 0$ (a negative example), the term does not contribute to the objective function, meaning only positive examples influence the loss. For numerical stability, a smoothing factor $\gamma$ is typically added to both the numerator and denominator, though it is set to 1 in the described case. The surrounding text provides additional insight into the use of this metric in machine learning, particularly in binary classification tasks where precision and recall are balanced via the DSC, which is mathematically equivalent to the F1-score in certain contexts."</data>
  <data key="d2">examples/example_working/images/image_6.jpg</data>
</node>
<node id="&quot;Input Data Point (X_I)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">In the context of Dice Similarity Coefficient (DSC) calculation for binary classification, X_I represents a single training instance or input data point—such as a pixel in image segmentation or a token in NLP tasks. It serves as the basis for computing predicted probabilities and ground truth alignment, particularly when evaluating similarity between model predictions and actual labels using DSC-based losses.</data>
  <data key="d2">examples/example_working/images/image_6.jpg</data>
</node>
<node id="&quot;Predicted Probability for Class 1 (P_I1)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">P_I1 denotes the model's predicted probability that the i-th input instance belongs to class 1 (e.g., the positive class). In DSC and related loss formulations like Dice Loss (DL) or Tversky Loss (TL), P_I1 is a continuous value in [0, 1] used to compute soft similarity measures with ground truth labels. It plays a central role in differentiating easy vs. hard examples, especially in imbalanced settings, and is often adjusted dynamically (e.g., via (1 - P_I1) weighting in Self-adjusting Dice Loss) to reduce the influence of well-classified samples.</data>
  <data key="d2">examples/example_working/images/image_6.jpg</data>
</node>
<node id="&quot;Ground Truth Label for Class 1 (Y_I1)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">Y_I1 is the binary ground truth label indicating whether the i-th instance truly belongs to class 1. It takes values in {0, 1}, where 1 signifies a positive example and 0 a negative one. In Dice-based metrics and losses, Y_I1 defines the target set against which predicted probabilities (P_I1) are compared. Unlike cross-entropy, standard DSC ignores negative examples (Y_I1 = 0), but smoothed variants (with γ &gt; 0) allow them to contribute minimally to the loss, aiding training stability in highly imbalanced datasets such as those in medical image segmentation or NLP sequence labeling.</data>
  <data key="d2">examples/example_working/images/image_6.jpg</data>
</node>
<node id="&quot;IMAGE_5&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula for the Dice Similarity Coefficient (DSC), commonly used in evaluating the performance of binary classification models, particularly in image segmentation tasks. The equation is presented in three equivalent forms. The first form expresses DSC as: DSC = (2TP) / (2TP + FN + FP), where TP denotes true positives, FN denotes false negatives, and FP denotes false positives. The second form rewrites this using precision (Pre) and recall (Rec): DSC = (2 × Pre × Rec) / (Pre + Rec). The third form shows that DSC is mathematically equivalent to the F1 score: DSC = F1. The formula is derived from set theory, with A representing predicted positive examples and B representing actual (golden) positive examples. The context explains that for a single example xi, the Dice coefficient can be written as DSC(xi) = (2pi1yi1) / (pi1 + yi1), where pi1 and yi1 are binary indicators for prediction and ground truth, respectively. The image contains no graphical elements, colors, or visual objects beyond the black text on a white background, formatted in standard mathematical notation."</data>
  <data key="d2">examples/example_working/images/image_5.jpg</data>
</node>
<node id="&quot;True Positives (TP)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">True Positives (TP) refer to the number of instances that are correctly identified as positive in a binary classification task. In the context of evaluation metrics like the Dice coefficient and F1 score, TP plays a central role, as it appears in both the numerator and denominator of these measures. Specifically, the Dice coefficient is defined as 2*TP / (2*TP + FN + FP), directly linking TP to the model's ability to capture relevant positive cases accurately.</data>
  <data key="d2">examples/example_working/images/image_5.jpg</data>
</node>
<node id="&quot;False Negatives (FN)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">False Negatives (FN) represent actual positive instances that were incorrectly classified as negative by the model. In imbalanced datasets—common in tasks like object detection and medical image segmentation—FN can significantly impact performance metrics such as recall and the Dice coefficient. The Tversky index, for example, allows explicit control over the penalty for FN through its β parameter, enabling models to prioritize sensitivity when needed.</data>
  <data key="d2">examples/example_working/images/image_5.jpg</data>
</node>
<node id="&quot;False Positives (FP)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">False Positives (FP) are instances that are actually negative but have been incorrectly predicted as positive by the classifier. In scenarios with severe class imbalance, such as background-object imbalance in object detection or rare-class identification in NLP, excessive FP can distort precision and overall model reliability. Techniques like hard negative mining and Dice-based losses aim to mitigate the influence of FP by adjusting how prediction errors contribute to the loss function.</data>
  <data key="d2">examples/example_working/images/image_5.jpg</data>
</node>
<node id="&quot;Precision (PRE)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">Precision (PRE) is defined as the ratio of true positives (TP) to the total number of predicted positives (TP + FP), reflecting the accuracy of positive predictions made by a model. It is a key component of the F1 score, which is equivalent to the Dice coefficient in binary classification: F1 = 2*(Precision × Recall) / (Precision + Recall). In imbalanced settings, high precision alone may be misleading if recall is low, motivating the use of combined metrics like F1 or adaptive losses that balance both concerns.</data>
  <data key="d2">examples/example_working/images/image_5.jpg</data>
</node>
<node id="&quot;Recall (REC)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">Recall (REC), also known as sensitivity, measures the proportion of actual positive instances that are correctly identified by the model, calculated as TP / (TP + FN). It is crucial in applications where missing positive cases is costly, such as medical diagnosis or rare event detection. The Dice coefficient and Tversky index incorporate recall alongside precision, and advanced loss functions like self-adjusting Dice loss dynamically modulate the contribution of easy vs. hard examples to improve recall without sacrificing precision excessively.</data>
  <data key="d2">examples/example_working/images/image_5.jpg</data>
</node>
<node id="&quot;F1 SCORE&quot;">
  <data key="d0">CONCEPT</data>
  <data key="d1">F1 Score, the harmonic mean of precision and recall, used to evaluate the performance of classification models. It is an F1-oriented statistic used to measure the similarity between two sets, commonly applied in evaluating model predictions against ground truth in machine learning contexts.</data>
  <data key="d2">examples/example_working/images/image_5.jpg</data>
</node>
<node id="&quot;IMAGE_4&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula representing the Sørensen–Dice coefficient (DSC), a statistical measure used to quantify the similarity between two sets. The formula is written in LaTeX-style mathematical notation and reads: DSC(A, B) = 2|A ∩ B| / (|A| + |B|). Here, A and B are sets, |A ∩ B| denotes the cardinality (number of elements) in the intersection of sets A and B, and |A| and |B| represent the cardinalities of sets A and B, respectively. The coefficient ranges from 0 to 1, where 1 indicates perfect overlap between the two sets and 0 indicates no overlap. In the context provided, set A corresponds to the set of positive examples predicted by a model, and set B corresponds to the set of true positive examples in the dataset (ground truth). This formulation is commonly used in evaluation metrics for segmentation tasks, particularly in medical imaging or natural language processing, where it serves as an F1-oriented metric. The expression is presented in a clean, centered format using standard mathematical typography, with clear use of absolute value bars, set notation, and division symbols."</data>
  <data key="d2">examples/example_working/images/image_4.jpg</data>
</node>
<node id="&quot;Set A (Predicted Positive Examples)&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">In the context of the Dice Similarity Coefficient (DSC), Set A represents the set of all positive examples predicted by a specific model. It is one of the two sets compared to evaluate segmentation or classification performance, particularly in tasks with class imbalance such as medical image segmentation or NLP.</data>
  <data key="d2">examples/example_working/images/image_4.jpg</data>
</node>
<node id="&quot;Set B (Ground Truth Positive Examples)&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">In the DSC formula, Set B denotes the set of all golden (ground truth) positive examples in the dataset. This set serves as the reference against which the model’s predictions (Set A) are compared to compute similarity, commonly used in evaluating binary or multi-class segmentation and classification tasks.</data>
  <data key="d2">examples/example_working/images/image_4.jpg</data>
</node>
<node id="&quot;|A ∩ B| (True Positives)&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">The cardinality of the intersection of sets A and B, |A ∩ B|, corresponds to the number of true positive predictions—elements correctly identified as positive by the model that also appear in the ground truth. In probabilistic formulations, this term is often represented as the sum of element-wise products between predicted probabilities and ground truth labels.</data>
  <data key="d2">examples/example_working/images/image_4.jpg</data>
</node>
<node id="&quot;|A| (Total Predicted Positives)&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">The cardinality of set A, |A|, represents the total number of elements predicted as positive by the model. In soft (probabilistic) Dice formulations used in deep learning, this is often replaced by the sum of squared predicted probabilities (∑p_i1²) to ensure differentiability and smoother optimization.</data>
  <data key="d2">examples/example_working/images/image_4.jpg</data>
</node>
<node id="&quot;|B| (Total Ground Truth Positives)&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">The cardinality of set B, |B|, indicates the total number of actual positive examples in the ground truth. In soft Dice loss implementations, this is typically represented as the sum of squared ground truth labels (∑y_i1²), although since y_i1 is binary, this equals the count of true positives.</data>
  <data key="d2">examples/example_working/images/image_4.jpg</data>
</node>
<node id="&quot;Constant Multiplier 2 in DSC Numerator&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">The constant '2' in the numerator of the Dice Similarity Coefficient (DSC) formula serves to scale the intersection term so that the coefficient aligns with the F1-score. Specifically, DSC = 2|A ∩ B| / (|A| + |B|) ensures that the metric equally balances precision and recall, making it particularly effective for evaluating performance in imbalanced datasets common in computer vision and NLP.</data>
  <data key="d2">examples/example_working/images/image_4.jpg</data>
</node>
<node id="&quot;IMAGE_3&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical equation labeled as 'CE' (Cross-Entropy), which is commonly used in machine learning for classification tasks, particularly in the context of multi-class or binary classification with probabilistic outputs. The equation is written in LaTeX-style mathematical notation and reads: \[ \mathrm{CE} = -\frac{1}{N} \sum_{i} \alpha_i \sum_{j \in \{0,1\}} y_{ij} \log p_{ij} \]This formula represents a weighted cross-entropy loss function. Here, \( N \) denotes the total number of samples in the dataset. The outer summation is over individual samples \( i \), and each sample contributes to the loss scaled by a weight factor \( \alpha_i \), where \( \alpha_i \in [0,1] \). This weighting allows for adjusting the importance of different samples, typically to address class imbalance. The inner summation is over the possible class labels \( j \in \{0,1\} \), indicating a binary classification setting. For each sample \( i \) and class \( j \), \( y_{ij} \) is the true label (usually 0 or 1 in one-hot encoding), and \( p_{ij} \) is the predicted probability assigned by the model to class \( j \) for sample \( i \). The logarithmic term \( \log p_{ij} \) penalizes low-confidence correct predictions more heavily. The negative sign ensures that minimizing the CE loss corresponds to maximizing the likelihood of correct predictions. The context provided explains that this formulation allows for unequal treatment of samples via \( \alpha_i \), which can be set based on inverse class frequency or tuned via hyperparameters such as \( K \) in the expression \( \log \left( \frac{n - n_t}{n_t} + K \right) \), where \( n_t \) is the count of samples in class \( t \) and \( n \) is the total number of training samples. This adjustment reduces the influence of majority classes and increases that of minority classes, helping mitigate bias in imbalanced datasets."</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Total Number of Samples (N)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">N represents the total number of samples in the dataset. In the context of cross-entropy loss and other classification objectives, it serves as a normalization factor to average the loss across all data points, ensuring scale invariance with respect to dataset size.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;Sample Weight Coefficient (α_i)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The weight coefficient $\alpha_i$ is associated with the i-th sample and modulates its contribution to the overall loss in weighted cross-entropy formulations. It is often calculated using the inverse frequency of the sample’s class (e.g., $\log(\frac{n - n_t}{n_t} + K)$, where $n_t$ is the count of class $t$, $n$ is the total number of samples, and $K$ is a tunable hyperparameter). This weighting scheme assigns higher importance to minority-class samples to counteract data imbalance.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;True Label Indicator (y_ij)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The true label value $y_{ij}$ for the i-th sample and j-th class is a binary indicator ($\in \{0,1\}$) used in binary or one-hot encoded classification settings. In binary classification, it denotes whether sample $i$ belongs to class $j$, and only one of $y_{i0}$ or $y_{i1}$ equals 1 while the other is 0.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Predicted Class Probability (p_ij)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The predicted probability $p_{ij}$ represents the model's estimated likelihood that the i-th sample belongs to the j-th class. These probabilities are typically derived from a softmax or sigmoid output layer and satisfy $p_{i0} + p_{i1} = 1$ in binary classification. They are central to loss computation in both cross-entropy and Dice-based objectives.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Natural Logarithm Function (log)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The natural logarithm function $\log$ is applied to predicted probabilities $p_{ij}$ in the cross-entropy loss to penalize incorrect predictions more severely. As $p_{ij}$ approaches 0 for a true positive class, $\log p_{ij}$ tends toward negative infinity, resulting in a large penalty. This property makes cross-entropy sensitive to confident but wrong predictions.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Summation Operator (Σ)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The summation symbol $\Sigma$ aggregates contributions across all samples ($i$) and classes ($j \in \{0,1\}$) in the cross-entropy loss computation. It ensures that the total loss reflects performance over the entire dataset and both classes in binary classification.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Binary Class Index Set (j ∈ {0,1})&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The index set $j \in \{0,1\}$ specifies that the summation in the cross-entropy formula is performed over two mutually exclusive classes, characteristic of binary classification tasks. Each sample has exactly one true class, and the model outputs a probability distribution over these two classes.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="Cross Entropy Loss">
  <data key="d0">IMG_ENTITY</data>
  <data key="d1">The cross-entropy (CE) loss function used in binary classification tasks, as represented by the mathematical formula CE = -1/N ∑∑ y_ij log p_ij. This formula computes the average cross-entropy loss over N training instances, where y_ij is the true label and p_ij is the predicted probability for class j ∈ {0,1}. The image corresponds to the formal definition of this loss function, which is discussed in the text as a standard training objective in data-imbalanced NLP tasks.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;IMAGE_2&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical formula representing the cross-entropy (CE) loss function used in binary classification tasks. The formula is written in LaTeX-style notation and reads: CE = -\frac{1}{N} \sum_{i} \sum_{j \in \{0,1\}} y_{ij} \log p_{ij}. This equation computes the average cross-entropy loss over N training instances. For each instance i, the sum is taken over the two possible class labels j ∈ {0,1}, where y_ij represents the true label (either 0 or 1) and p_ij denotes the predicted probability for class j. The logarithm of the predicted probability is weighted by the true label, ensuring that only the log-probability of the correct class contributes to the loss. The negative sign ensures the loss is positive, and dividing by N normalizes the total loss across all samples. The context explains that this formulation applies to binary classification with one-hot encoded labels and can be extended to multi-class settings. It also mentions that each training instance contributes equally to the final objective, and strategies like class weighting or resampling are used when unequal treatment of instances is desired."</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;Sample Index (i)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">The index variable i denotes an individual sample in the dataset, ranging from 1 to N. It is used in summations over the dataset to compute aggregate metrics such as loss functions, where each sample contributes to the overall objective based on its true label and predicted probabilities.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;Class Label Index (j)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">The index variable j represents class labels in a classification task. In binary classification, j takes values in the set {0, 1}, where 0 typically denotes the negative class and 1 the positive class. This index is used in inner summations over classes when computing per-sample losses like cross-entropy.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;True Label (y_ij)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">y_ij denotes the ground-truth label for sample i and class j. In binary classification, y_ij is 1 if sample i belongs to class j, and 0 otherwise. These binary indicators are essential in computing supervised learning objectives such as cross-entropy loss and Dice-based losses, as they define the target distribution the model aims to approximate.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;Predicted Probability (p_ij)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">p_ij is the predicted probability that sample i belongs to class j, typically generated by a softmax or sigmoid activation function in neural networks. These probabilities must satisfy p_i0 + p_i1 = 1 in binary classification and are used alongside true labels y_ij to compute various loss functions including cross-entropy, Dice loss, and Tversky loss.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;Natural Logarithm (log)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">The natural logarithm function, denoted as log, is applied to predicted probabilities p_ij in the cross-entropy loss formulation. It penalizes incorrect predictions more severely—the closer p_ij is to 0 when y_ij = 1, the larger the penalty—thus encouraging the model to assign higher probabilities to correct classes.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;Summation Over Samples (∑_i)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">The outer summation over i aggregates contributions from all N samples in the dataset. In loss functions like cross-entropy, this ensures that the total loss reflects performance across the entire dataset, with each sample contributing equally unless modified by weighting schemes.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;Summation Over Classes (∑_j)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">The inner summation over j iterates through all possible class labels (e.g., j ∈ {0, 1} in binary classification) for each sample i. This allows loss functions to account for multi-class predictions by summing the contribution of each class based on its true label and predicted probability.</data>
  <data key="d2">examples/example_working/images/image_2.jpg</data>
</node>
<node id="&quot;IMAGE_1&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image is a table labeled 'Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.' The table presents statistical information about the number of negative (# neg), positive (# pos) examples, and their respective ratios for various natural language processing (NLP) tasks. It consists of five rows and four columns. The first column lists the tasks: CoNLL03 NER, OntoNotes5.0 NER, SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and QUOREF (Dasigi et al., 2019). The second column shows the number of negative examples: 170K for CoNLL03 NER, 1.96M for OntoNotes5.0 NER, 10.3M for SQuAD 1.1, 15.4M for SQuAD 2.0, and 6.52M for QUOREF. The third column displays the number of positive examples: 34K for CoNLL03 NER, 239K for OntoNotes5.0 NER, 175K for SQuAD 1.1, 188K for SQuAD 2.0, and 38.6K for QUOREF. The fourth column provides the ratio of negative to positive examples: 4.98 for CoNLL03 NER, 8.18 for OntoNotes5.0 NER, 55.9 for SQuAD 1.1, 82.0 for SQuAD 2.0, and 169 for QUOREF. The table highlights the increasing severity of data imbalance across these tasks, particularly in machine reading comprehension (MRC) tasks like SQuAD and QUOREF, where the ratio exceeds 50 and reaches as high as 169. This extreme imbalance reflects the nature of MRC tasks, where only a few tokens are labeled as positive (e.g., start or end of an answer span), while the vast majority are background (negative). The context emphasizes that such imbalance poses challenges in training models and motivates the use of specialized loss functions like Dice loss to improve performance."</data>
  <data key="d2">examples/example_working/images/image_1.jpg</data>
</node>
<node id="&quot;CONLL03&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">CoNLL03 is a Named Entity Recognition (NER) benchmark dataset used in the paper to evaluate the proposed dice loss method. It contains 170K negative and 34K positive instances, resulting in a ratio of 4.98.</data>
  <data key="d2">examples/example_working/images/image_1.jpg&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;ONTONOTES5.0&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">OntoNotes5.0 is a large-scale NER dataset referenced in the paper, noted for its severe data imbalance with a negative-to-positive ratio of 8.18. It contains 1.96M negative and 239K positive instances.</data>
  <data key="d2">examples/example_working/images/image_1.jpg&lt;SEP&gt;chunk-cebc63594f08dbcbe32a7d19f944ef73&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;SQUAD 1.1&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">SQuAD 1.1 is a machine reading comprehension (MRC) dataset introduced by Rajpurkar et al. (2016), used in the paper as an example of a highly imbalanced NLP task with a ratio of 55.9. It contains 10.3M negative and 175K positive instances.</data>
  <data key="d2">examples/example_working/images/image_1.jpg&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;SQUAD 2.0&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">SQuAD 2.0 is an MRC dataset introduced by Rajpurkar et al. (2018), exhibiting extreme data imbalance with a negative-to-positive ratio of 82.0. It contains 15.4M negative and 188K positive instances.</data>
  <data key="d2">examples/example_working/images/image_1.jpg&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;QUOREF&quot;">
  <data key="d0">EVENT</data>
  <data key="d1">QUOREF is an MRC dataset introduced by Dasigi et al. (2019), cited in the paper as having the highest imbalance ratio of 169. It contains 6.52M negative and 38.6K positive instances, and is used for coreference resolution.</data>
  <data key="d2">examples/example_working/images/image_1.jpg&lt;SEP&gt;chunk-14fe1936981bd0314ca6d73d5425c982&lt;SEP&gt;chunk-cebc63594f08dbcbe32a7d19f944ef73&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;TASK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">Column header in Table 1 of the paper 'Dice Loss for Data-imbalanced NLP Tasks' that specifies the natural language processing task being evaluated, such as Named Entity Recognition (NER), Machine Reading Comprehension (MRC), or part-of-speech tagging. These tasks are characterized by severe data imbalance between positive and negative examples.</data>
  <data key="d2">examples/example_working/images/image_1.jpg</data>
</node>
<node id="&quot;# NEG&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">Column header in Table 1 representing the number of negative instances (e.g., background tokens or non-entity tokens) in each dataset used for data-imbalanced NLP tasks. For example, in CoNLL03 NER, there are 170K negative examples, reflecting the dominance of non-entity tokens over entity tokens.</data>
  <data key="d2">examples/example_working/images/image_1.jpg</data>
</node>
<node id="&quot;# POS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">Column header in Table 1 denoting the number of positive instances (e.g., tokens labeled as entities or relevant answer spans) in each dataset. In tasks like SQuAD 2.0, only 188K tokens are positive out of over 15 million total tokens, illustrating extreme data imbalance.</data>
  <data key="d2">examples/example_working/images/image_1.jpg</data>
</node>
<node id="&quot;RATIO&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">Column header in Table 1 indicating the ratio of negative to positive instances in each NLP dataset, highlighting the severity of data imbalance. Ratios range from approximately 5 in CoNLL03 NER to as high as 169 in QUOREF, underscoring the challenge of training models that perform well on minority (positive) classes.</data>
  <data key="d2">examples/example_working/images/image_1.jpg</data>
</node>
<node id="&quot;TABLE 1&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 1 presents statistics on the number of positive and negative examples and their ratios across various data-imbalanced NLP tasks discussed in the paper."</data>
  <data key="d2">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;TABLE 9&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 9 displays the performance of BERT models with different loss functions (CE, DL, DSC) on the Stanford Sentiment Treebank datasets (SST-2 and SST-5), measured by accuracy."&lt;SEP&gt;"Table 9 presents experimental results on the effect of DL and DSC on sentiment classification tasks, specifically involving BERT fine-tuned with cross-entropy."</data>
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d&lt;SEP&gt;chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;TABLE 10&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 10 displays the effect of hyperparameters (α and β) in the Tversky Index on performance metrics across two datasets: Chinese OntoNotes4.0 and English QuoRef MRC."</data>
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
</node>
<node id="&quot;CHINESE ONTONOTES4.0 NER DATASET&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Chinese OntoNotes4.0 is a named entity recognition dataset focused on Chinese language data, used in experiments evaluating the Tversky Index hyperparameters."</data>
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
</node>
<node id="&quot;ENGLISH QUOREF MRC DATASET&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"English QuoRef MRC is a machine reading comprehension dataset in English, used alongside the Chinese OntoNotes4.0 dataset to test hyperparameter effects in the Tversky Index."</data>
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
</node>
<node id="&quot;QINGHONG HAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 Glyce paper."&lt;SEP&gt;"Qinghong Han is a co-author of the 2019 MRC-NER paper."&lt;SEP&gt;"Qinghong Han is a researcher and co-author of a 2019 paper on a unified MRC framework for named entity recognition."&lt;SEP&gt;"Qinghong Han is acknowledged for providing comments and suggestions on the research work."</data>
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d&lt;SEP&gt;chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;WEI WU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lead author of the 2019 paper 'Glyce: Glyph-vectors for Chinese character representations' published as an arXiv preprint."&lt;SEP&gt;"Wei Wu is a researcher and co-author of the 2019 Dsreg paper."&lt;SEP&gt;"Wei Wu is acknowledged for providing comments and suggestions on the research work."</data>
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d&lt;SEP&gt;chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;JIAWEI WU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jiawei Wu is acknowledged for providing comments and suggestions on the research work."</data>
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
</node>
<node id="&quot;NATIONAL NATURAL SCIENCE FOUNDATION OF CHINA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The National Natural Science Foundation of China (NSFC) provided funding support for this research under grant numbers 61625107 and 61751209."</data>
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
</node>
<node id="&quot;JIANG ET AL., 2017&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A research group or authors who proposed learning a separate network to predict sample weights, cited in the context of addressing data imbalance."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;FAN ET AL., 2018&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A research group or authors who, alongside Jiang et al., proposed using a separate network for sample weight prediction."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;LI ET AL., 2015&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers cited for their work on background-object label imbalance in object detection."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;GIRSHICK, 2015&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ross Girshick, a researcher known for contributions to object detection, cited here regarding label imbalance issues."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;HE ET AL., 2015&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A research team cited for work on object detection and data imbalance."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;GIRSHICK ET AL., 2013&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A research group including Ross Girshick that introduced hard negative mining (HNM) in object detection."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;REN ET AL., 2015&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers cited for contributions to object detection and handling class imbalance."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;PANG ET AL., 2019&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers who proposed IoU-balanced sampling to address class imbalance in computer vision."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;CHEN ET AL., 2019&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers who designed a ranking model with average-precision loss to mitigate class imbalance."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;SUDRE ET AL., 2017&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers who addressed class imbalance in image segmentation using Generalized Dice Loss."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;SHEN ET AL., 2018&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers who studied Dice-based loss for multi-class organ segmentation using abdominal CT volumes."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;KODYM ET AL., 2018&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers who proposed batch soft Dice loss for segmentation of organs at risk in medical images."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;SHAMIR ET AL., 2019&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers who extended the classical Dice coefficient for comparing binary ground truth with probabilistic maps."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;VALVERDE ET AL., 2017&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Researchers cited regarding the challenges of selecting weighting factors in imbalanced classification tasks."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;SØRENSEN, 1948&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Thorvald Sørensen, a scientist who introduced the Sørensen–Dice coefficient for measuring set similarity."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;DICE, 1945&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lee R. Dice, a scientist who independently developed the Dice coefficient, a statistic for set similarity also known as DSC."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;HARD NEGATIVE MINING (HNM)&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A technique introduced by Girshick et al. (2013) to address class imbalance in object detection by focusing on difficult negative examples."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;IOU-BALANCED SAMPLING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A method proposed by Pang et al. (2019) to balance sampling based on Intersection over Union (IoU) to mitigate data imbalance."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;GENERALIZED DICE LOSS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A training objective proposed by Sudre et al. (2017) leveraging class re-balancing properties for unbalanced image segmentation tasks."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;BATCH SOFT DICE LOSS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A loss function introduced by Kodym et al. (2018) for training CNNs in organ-at-risk segmentation."</data>
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
</node>
<node id="&quot;BERT-TAGGER (DEVLIN ET AL., 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A named entity recognition model introduced by Devlin et al. in 2018, used as a baseline in the experiments across multiple datasets including CoNLL2003, MSRA, and OntoNotes."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;BERT-MRC (LI ET AL., 2019)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A state-of-the-art named entity recognition model proposed by Li et al. in 2019 that frames NER as a machine reading comprehension (MRC) task; serves as the backbone for the current implementation."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A Chinese-specific NER model developed by Zhang and Yang in 2018 that constructs a word-character lattice structure; used only on Chinese datasets like MSRA and Chinese OntoNotes 4.0."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;GLYCE-BERT (WU ET AL., 2019)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A BERT-based model adapted for Chinese text by Wu et al. in 2019, incorporating glyph information; evaluated on Chinese NER benchmarks."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;ELMO (PETERS ET AL., 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A pretraining-based tagging model introduced by Peters et al. in 2018, used as a baseline for comparison in named entity recognition tasks."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;CVT (CLARK ET AL., 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"A method using Cross-View Training to enhance Bi-LSTM encoder representations, proposed by Clark et al. in 2018, included among the baselines."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;DEVLIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"One of the authors of the BERT-Tagger model (2018), a foundational contributor to the BERT architecture used in NER."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;LI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lead author of the BERT-MRC model (2019), whose work forms the backbone of the current NER implementation."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;ZHANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author (with Yang) of the Lattice-LSTM model (2018) for Chinese NER."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;YANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author (with Zhang) of the Lattice-LSTM model (2018) designed for Chinese named entity recognition."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;WU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lead author of the Glyce-BERT model (2019), which integrates glyph embeddings into BERT for Chinese NER."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;PETERS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lead author of the ELMo model (2018), a contextualized word representation system used in NER baselines."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;CLARK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lead author of the CVT (Cross-View Training) approach (2018) for improving sequence modeling in NER."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;ONTONOTES 4.0&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A benchmark dataset for named entity recognition, released by Pradhan et al. in 2011, used in the evaluation of NER models."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;ONTONOTES 5.0&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"An updated version of the OntoNotes dataset, released by Pradhan et al. in 2013, serving as a standard NER evaluation benchmark."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;MSRA&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A Chinese named entity recognition dataset created by Levow in 2006, commonly used for evaluating Chinese NER systems."&lt;SEP&gt;"MSRA is a Chinese NER dataset provided by Microsoft Research Asia, used as a benchmark for evaluating named entity recognition systems."&lt;SEP&gt;"MSRA is a Chinese benchmark dataset for named entity recognition containing three entity types, sourced from news articles, with no original development set provided."&lt;SEP&gt;"MSRA is a named entity recognition dataset evaluated in the paper, where the proposed method achieved competitive or better results."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813&lt;SEP&gt;chunk-cebc63594f08dbcbe32a7d19f944ef73&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;CONLL2003&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A widely used English-language NER benchmark dataset introduced by Sang and Meulder in 2003."&lt;SEP&gt;"CoNLL2003 is a widely used Named Entity Recognition dataset, often associated with English-language text from news sources; treated here as a benchmark geo-tagged corpus."&lt;SEP&gt;"CoNLL2003 is an English benchmark dataset for named entity recognition with four entity types: Location, Organization, Person, and Miscellaneous."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813&lt;SEP&gt;chunk-cebc63594f08dbcbe32a7d19f944ef73&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;CHINESE MSRA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Refers to the Chinese variant or context of the MSRA dataset, indicating its linguistic and geographic association with China."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;CHINESE ONTONOTES 4.0&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"The Chinese-language portion of the OntoNotes 4.0 corpus, geographically and linguistically tied to Chinese-speaking regions."</data>
  <data key="d2">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
</node>
<node id="&quot;MILLETARI ET AL. (2016)&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Milletari et al. (2016) refers to a group of researchers who proposed a modification to the Dice Loss denominator using a squared form to achieve faster convergence in optimization."</data>
  <data key="d2">chunk-bff777a634a5af0182c5787f1022b29f</data>
</node>
<node id="&quot;TABLE 5&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 5 displays experimental results for the Named Entity Recognition (NER) task on English CoNLL 2003 and OntoNotes5.0 datasets."&lt;SEP&gt;"Table 5 presents experimental results on Named Entity Recognition (NER) datasets, comparing the performance of various models including DSC and BERT-MRC."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;TABLE 6&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 6 displays experimental results for the Machine Reading Comprehension (MRC) task across three datasets: SQuAD v1.1, SQuAD v2.0, and Quoref."&lt;SEP&gt;"Table 6 is a data table presenting experimental results for the Machine Reading Comprehension (MRC) task, comparing models like BERT and XLNet with and without the proposed DSC loss."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982&lt;SEP&gt;chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;PETERS ET AL. (2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Peters et al. (2018) is a research group or author team that introduced ELMo, a tagging model with pretraining used as a baseline in NER experiments."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;ZHANG AND YANG (2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Zhang and Yang (2018) developed the Lattice-LSTM model, which constructs a word-character lattice and is applied only to Chinese datasets."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;CLARK ET AL. (2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Clark et al. (2018) proposed Cross-View Training (CVT) to enhance representations in a Bi-LSTM encoder."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;DEVLIN ET AL. (2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Devlin et al. (2018) introduced BERT, used here both as BERT-Tagger for NER and as a baseline model for MRC."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;WU ET AL. (2019)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Wu et al. (2019) developed Glyce-BERT, which integrates Chinese glyph information with BERT pretraining."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;LI ET AL. (2019)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Li et al. (2019) formulated NER as a machine reading comprehension task using BERT-MRC and achieved state-of-the-art results on multiple benchmarks."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;YU ET AL. (2018B)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Yu et al. (2018b) developed QANet, an MRC model based on convolutions and self-attention mechanisms."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;YANG ET AL. (2019)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Yang et al. (2019) proposed XLNet, a generalized autoregressive pretraining method used as a strong baseline in MRC tasks."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;DASIGI ET AL. (2019)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Dasigi et al. (2019) introduced the Quoref dataset, used in machine reading comprehension evaluations."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;RAJPURKAR ET AL. (2016, 2018)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Rajpurkar et al. (2016, 2018) are the creators of the SQuAD v1.1 and SQuAD v2.0 machine reading comprehension datasets."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;SEO ET AL. (2016)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Seo et al. (2016) established standard protocols for the machine reading comprehension task, focusing on predicting start and end indexes of answer spans."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
</node>
<node id="&quot;ONTONOTES4.0&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Chinese OntoNotes4.0 is a named entity recognition dataset consisting of news-domain texts with 18 entity types, used with a data split following Wu et al. (2019)."&lt;SEP&gt;"OntoNotes4.0 is an earlier version of the OntoNotes NER benchmark dataset, used for evaluating entity recognition across diverse text genres."&lt;SEP&gt;"OntoNotes4.0 is another NER dataset included in the evaluation, showing strong performance with the proposed approach."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;SQUAD V1.1&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"SQuAD v1.1 is a machine reading comprehension dataset consisting of questions posed by crowdworkers on Wikipedia passages, used as a standard evaluation benchmark."&lt;SEP&gt;"SQuAD v1.1 is a widely used machine reading comprehension dataset containing 100K crowdsourced question-answer pairs based on Wikipedia passages."</data>
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;SQUAD V2.0&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"SQuAD v2.0 extends SQuAD v1.1 by including questions that may not have answers in the given passage, testing the ability of models to recognize unanswerable questions."&lt;SEP&gt;"SQuAD v2.0 extends SQuAD v1.1 by including unanswerable questions, increasing the challenge of the machine reading comprehension task."&lt;SEP&gt;"SQuAD v2.0 is an extended version of the SQuAD dataset that includes unanswerable questions, used in MRC evaluations."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982&lt;SEP&gt;chunk-cebc63594f08dbcbe32a7d19f944ef73&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;XIAOYA LI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 Glyce paper."&lt;SEP&gt;"Xiaoya Li is a researcher and co-author of a 2019 paper on a unified MRC framework for named entity recognition."&lt;SEP&gt;"Xiaoya Li is a researcher and co-author of the paper 'Dice Loss for Data-imbalanced NLP Tasks', affiliated with Shannon.AI."&lt;SEP&gt;"Xiaoya Li is the lead author of the 2019 unified MRC framework for named entity recognition."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;JINGRONG FENG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jingrong Feng is a co-author of the 2019 MRC-NER paper."&lt;SEP&gt;"Jingrong Feng is a researcher and co-author of a 2019 paper on a unified MRC framework for named entity recognition."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;YUXIAN MENG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 Glyce paper."&lt;SEP&gt;"Yuxian Meng is a co-author of the 2019 MRC-NER paper."&lt;SEP&gt;"Yuxian Meng is a researcher and co-author of the paper, affiliated with Shannon.AI."&lt;SEP&gt;"Yuxian Meng is a researcher who co-authored multiple papers, including one on a unified MRC framework for NER (2019) and another on Dsreg (2019)."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;FEI WU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Fei Wu is a co-author of the 2019 MRC-NER paper."&lt;SEP&gt;"Fei Wu is a researcher and co-author of a 2019 paper on a unified MRC framework for named entity recognition."&lt;SEP&gt;"Fei Wu is a researcher and co-author of the paper, affiliated with the Department of Computer Science and Technology, Zhejiang University."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;JIWEI LI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 Glyce paper."&lt;SEP&gt;"Jiwei Li is a co-author of the 2019 MRC-NER paper."&lt;SEP&gt;"Jiwei Li is a researcher and co-author of the paper, affiliated with Shannon.AI."&lt;SEP&gt;"Jiwei Li is a researcher who co-authored multiple papers, including one on a unified MRC framework for NER (2019) and another on Dsreg (2019)."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;TSUNG-YI LIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tsung-Yi Lin is a researcher and lead author of the 2017 paper on Focal Loss for dense object detection."&lt;SEP&gt;"Tsung-Yi Lin is the lead author of the 2017 Focal Loss paper for dense object detection."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;PRIYA GOYAL&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Priya Goyal is a co-author of the 2017 Focal Loss paper."&lt;SEP&gt;"Priya Goyal is a researcher and co-author of the 2017 paper on Focal Loss presented at the IEEE International Conference on Computer Vision."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ROSS GIRSHICK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ross Girshick is a researcher and co-author of the 2017 Focal Loss paper and also associated with computer vision research."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;KAIMING HE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kaiming He is a co-author of the 2015 deep residual learning paper and the 2017 Focal Loss paper."&lt;SEP&gt;"Kaiming He is a researcher who co-authored the 2017 Focal Loss paper and the 2015 Faster R-CNN paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;PIOTR DOLLAR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Piotr Dollar is a co-author of the 2017 Focal Loss paper."&lt;SEP&gt;"Piotr Dollar is a researcher and co-author of the 2017 Focal Loss paper for dense object detection."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;XUEZHE MA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Xuezhe Ma is a researcher and co-author of a 2016 paper on end-to-end sequence labeling using bi-directional LSTM-CNNs-CRF."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;EDUARD HOVY&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Eduard Hovy is a researcher and co-author of a 2016 paper on end-to-end sequence labeling."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;TOMASZ MALISIEWICZ&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tomasz Malisiewicz is a researcher and lead author of a 2011 paper on Ensemble of Exemplar-SVMs for object detection."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ABHINAV GUPTA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Abhinav Gupta is a researcher and co-author of the 2011 ICCV paper on Ensemble of Exemplar-SVMs."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ALEXEI A. EFROS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Alexei A. Efros is a researcher and co-author of the 2011 ICCV paper on Ensemble of Exemplar-SVMs."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;BRYAN MCCANN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Bryan McCann is a researcher and lead author of a 2018 paper on The Natural Language Decathlon."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;NITISH SHIRISH KESKAR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Nitish Shirish Keskar is a researcher and co-author of the 2018 Natural Language Decathlon paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;CAIMING XIONG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Caiming Xiong is a researcher and co-author of the 2018 Natural Language Decathlon paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;RICHARD SOCHER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Richard Socher is a researcher and co-author of the 2018 Natural Language Decathlon paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MUYU LI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 Glyce paper."&lt;SEP&gt;"Muyu Li is a researcher and co-author of the 2019 Dsreg paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;FAUSTO MILLETARI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Fausto Milletari is a researcher and lead author of a 2016 paper on V-Net for medical image segmentation."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;NASSIR NAVAB&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Nassir Navab is a researcher and co-author of the 2016 V-Net paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;SEYED-AHMAD AHMADI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Seyed-Ahmad Ahmadi is a researcher and co-author of the 2016 V-Net paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;DAVID NADEAU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"David Nadeau is a researcher and co-author of a 2007 survey on named entity recognition and classification."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;SATOSHI SEKINE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Satoshi Sekine is a researcher and co-author of a 2007 survey on named entity recognition and classification."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;TRI NGUYEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tri Nguyen is a researcher and lead author of the 2016 MS MARCO dataset paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MIR ROSENBERG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mir Rosenberg is a researcher and co-author of the 2016 MS MARCO paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;XIA SONG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Xia Song is a researcher and co-author of the 2016 MS MARCO paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;JIANFENG GAO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Reasonet paper."&lt;SEP&gt;"Jianfeng Gao is a researcher and co-author of the 2016 MS MARCO paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;SAURABH TIWARY&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Saurabh Tiwary is a researcher and co-author of the 2016 MS MARCO paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;RANGAN MAJUMDER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Rangan Majumder is a researcher and co-author of the 2016 MS MARCO paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;LI DENG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Li Deng is a researcher and co-author of the 2016 MS MARCO paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;JIANGMIAO PANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jiangmiao Pang is a researcher and lead author of the 2019 Libra R-CNN paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;KAI CHEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 QANet papers."&lt;SEP&gt;"Kai Chen is a researcher and co-author of the 2019 Libra R-CNN paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;JIANPING SHI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jianping Shi is a researcher and co-author of the 2019 Libra R-CNN paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;HUAJUN FENG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Huajun Feng is a researcher and co-author of the 2019 Libra R-CNN paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;WANLI OUYANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Wanli Ouyang is a researcher and co-author of the 2019 Libra R-CNN paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;DAHUA LIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Dahua Lin is a researcher and co-author of the 2019 Libra R-CNN paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MATTHEW E PETERS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Matthew E Peters is a researcher and lead author of the 2018 paper on Deep Contextualized Word Representations (ELMo)."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MARK NEUMANN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mark Neumann is a researcher and co-author of the 2018 ELMo paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MOHIT IYYER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mohit Iyyer is a researcher and co-author of the 2018 ELMo paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MATT GARDNER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Matt Gardner is a co-author of the 2019 Quoref dataset paper."&lt;SEP&gt;"Matt Gardner is a researcher and co-author of the 2018 ELMo paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;CHRISTOPHER CLARK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Christopher Clark is a researcher and co-author of the 2018 ELMo paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;KENTON LEE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kenton Lee is a co-author of the 2018 BERT paper."&lt;SEP&gt;"Kenton Lee is a researcher and co-author of the 2018 ELMo paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;LUKE ZETTLEMOYER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Luke Zettlemoyer is a researcher and co-author of the 2018 ELMo paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;SAMEER PRADHAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Sameer Pradhan is a researcher involved in multiple shared tasks on computational linguistics, including CoNLL and OntoNotes."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MITCHELL P. MARCUS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mitchell P. Marcus is an editor of the 2011 CoNLL Shared Task proceedings."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MARTHA PALMER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Martha Palmer is an editor of the 2011 CoNLL Shared Task proceedings."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;LANCE A. RAMSHAW&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lance A. Ramshaw is an editor of the 2011 CoNLL Shared Task proceedings."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;RALPH M. WEISCHEDEL&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ralph M. Weischedel is an editor of the 2011 CoNLL Shared Task proceedings."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;NIANWEN XUE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Nianwen Xue is a researcher involved in both the 2011 CoNLL Shared Task and the 2013 OntoNotes robust linguistic analysis paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ALESSANDRO MOSCHITTI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Alessandro Moschitti is a researcher and co-author of the 2013 OntoNotes paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;HWEE TOU NG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Hwee Tou Ng is a researcher and co-author of the 2013 OntoNotes paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ANDERS BJORKELUND&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Anders Bjorkelund is a researcher and co-author of the 2013 OntoNotes paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;OLGA URYUPINA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Olga Uryupina is a researcher and co-author of the 2013 OntoNotes paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;YUCHEN ZHANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Yuchen Zhang is a researcher and co-author of the 2013 OntoNotes paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ZHI ZHONG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zhi Zhong is a researcher and co-author of the 2013 OntoNotes paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;PRANAV RAJPURKAR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Pranav Rajpurkar is a researcher who co-authored the SQuAD (2016) and SQuAD unanswerable questions (2018) papers."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ROBIN JIA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Robin Jia is a researcher and co-author of the 2018 SQuAD unanswerable questions paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;PERCY LIANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Percy Liang is a researcher who co-authored both the SQuAD (2016) and SQuAD unanswerable questions (2018) papers."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;JIAN ZHANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jian Zhang is a researcher and co-author of the 2016 SQuAD paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;KONSTANTIN LOPYREV&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Konstantin Lopyrev is a researcher and co-author of the 2016 SQuAD paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;SHAOQING REN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Shaoqing Ren is a co-author of the 2015 deep residual learning paper."&lt;SEP&gt;"Shaoqing Ren is a researcher and lead author of the 2015 Faster R-CNN paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ROSS B. GIRSHICK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ross B. Girshick is a computer vision researcher and author of the Fast R-CNN paper (2015) and co-author of earlier object detection work."&lt;SEP&gt;"Ross B. Girshick is a researcher and co-author of the 2015 Faster R-CNN paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;JIAN SUN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jian Sun is a co-author of the 2015 deep residual learning paper."&lt;SEP&gt;"Jian Sun is a researcher and co-author of the 2015 Faster R-CNN paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ALAN RITTER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Alan Ritter is a researcher and lead author of a 2011 paper on Named Entity Recognition in Tweets."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;SAM CLARK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Sam Clark is a researcher and co-author of the 2011 NER in Tweets paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MAUSAM&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mausam is a researcher and co-author of the 2011 NER in Tweets paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;OREN ETZIONI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Oren Etzioni is a researcher and co-author of the 2011 NER in Tweets paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ERIK F SANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Erik F Sang is a researcher and co-author of the 2003 CoNLL-2003 shared task paper on language-independent NER."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;FIEN DE MEULDER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Fien De Meulder is a researcher and co-author of the 2003 CoNLL-2003 shared task paper."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ERIK F. TJONG KIM SANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Erik F. Tjong Kim Sang is a researcher and co-author of the 2003 CoNLL-2003 shared task paper; full name variant of Erik F Sang."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;MINJOON SEO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2016 paper 'Bidirectional attention flow for machine comprehension' published as an arXiv preprint."&lt;SEP&gt;"Minjoon Seo is a researcher and lead author of a 2016 paper on Bidirectional Attention Flow for machine comprehension."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ANIRUDDHA KEMBHAVI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Aniruddha Kembhavi is a researcher and co-author of the 2016 BiDAF paper."&lt;SEP&gt;"Co-author of the 2016 paper 'Bidirectional attention flow for machine comprehension'."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ALI FARHADI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ali Farhadi is a researcher and co-author of the 2016 BiDAF paper."&lt;SEP&gt;"Co-author of the 2016 paper 'Bidirectional attention flow for machine comprehension'."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;HANNANEH HAJISHIRZI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2016 paper 'Bidirectional attention flow for machine comprehension'."&lt;SEP&gt;"Hannaneh Hajishirzi is a researcher and co-author of the 2016 BiDAF paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The IEEE International Conference on Computer Vision (ICCV) is a major academic conference where several cited papers were presented, including the 2011 and 2017 works."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ICCV 2011&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"ICCV 2011 refers to the 2011 IEEE International Conference on Computer Vision held in Barcelona, Spain, November 6–13, 2011."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;BARCELONA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Barcelona is a city in Spain where ICCV 2011 was held."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;SPAIN&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Spain is the country where ICCV 2011 took place in Barcelona."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) is a premier conference; CVPR 2019 was held in Long Beach, CA, USA."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;CVPR 2019&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"CVPR 2019 is the 2019 edition of the IEEE Conference on Computer Vision and Pattern Recognition, held in Long Beach, CA, USA, June 16–20, 2019."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;LONG BEACH&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Long Beach is a city in California, USA, where CVPR 2019 was held."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;CA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"CA (California) is a U.S. state where CVPR 2019 took place in Long Beach."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;USA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"USA is the country where CVPR 2019 was held."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;2016 FOURTH INTERNATIONAL CONFERENCE ON 3D VISION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The 2016 Fourth International Conference on 3D Vision (3DV) is a conference where the V-Net paper was presented."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;EDMONTON&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"A city in Canada where the Seventh Conference on Natural Language Learning (CoNLL 2003) and HLT-NAACL 2003 were held."&lt;SEP&gt;"Edmonton is a city in Canada where the CoNLL 2003 workshop was held."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;CANADA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Canada is the country where NeurIPS 2010 was held."&lt;SEP&gt;"Canada is the country where the CoNLL 2003 workshop took place in Edmonton."&lt;SEP&gt;"Country where the Seventh Conference on Natural Language Learning (CoNLL 2003) and HLT-NAACL 2003 took place."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;CONLL 2003&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"CoNLL 2003 refers to the Seventh Conference on Natural Language Learning, held in Edmonton, Canada, May 31 – June 1, 2003."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;HLT-NAACL 2003&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A joint human language technology and North American chapter of the Association for Computational Linguistics conference held in 2003, with which CoNLL 2003 was held in cooperation."&lt;SEP&gt;"HLT-NAACL 2003 is the Human Language Technology–North American Chapter of the ACL conference, held in cooperation with CoNLL 2003."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;SOFIA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Sofia is the capital city of Bulgaria where the Seventeenth Conference on Computational Natural Language Learning was held in 2013."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;BULGARIA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Bulgaria is the country where the 2013 Computational Natural Language Learning conference took place in Sofia."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Association for Computational Linguistics (ACL) is a professional organization that sponsors conferences such as CoNLL and publishes proceedings in computational linguistics."&lt;SEP&gt;"The Association for Computational Linguistics (ACL) sponsored the Fifth SIGHAN Workshop on Chinese Language Processing."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357&lt;SEP&gt;chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;PROCEEDINGS OF THE FIFTEENTH CONFERENCE ON COMPUTATIONAL NATURAL LANGUAGE LEARNING: SHARED TASK&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This refers to the 2011 CoNLL Shared Task workshop organized by ACL."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;PROCEEDINGS OF THE SEVENTEENTH CONFERENCE ON COMPUTATIONAL NATURAL LANGUAGE LEARNING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This 2013 conference event included the paper on OntoNotes and was held in Sofia, Bulgaria."</data>
  <data key="d2">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
</node>
<node id="&quot;TABLE&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Table presents evaluation metrics (Precision, Recall, F1) for different models across datasets CTB5, CTB6, and UD1.4, including results for 'Joint-POS(Sig)(Shao et al., 2017)'."</data>
  <data key="d2">chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
</node>
<node id="&quot;SELF-ADJUSTING DICE LOSS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Self-adjusting Dice Loss is a proposed adaptive variant of the Dice Similarity Coefficient that introduces a decaying factor (1 - p) to reduce the influence of easy examples during training."</data>
  <data key="d2">chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
</node>
<node id="&quot;SHAO ET AL., 2017&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Shao et al. (2017) are researchers who developed the Joint-POS models (Sig and Ens variants) evaluated in the experiments."&lt;SEP&gt;"Shao et al. (2017) proposed the Joint-POS model that jointly learns Chinese word segmentation and POS tagging."&lt;SEP&gt;"Shao et al., 2017 are researchers cited for their 'Joint-POS(Sig)' model, which achieved specific performance metrics on Chinese Treebank and Universal Dependencies datasets."</data>
  <data key="d2">chunk-aa4690473ce6c28f84935bd046522165&lt;SEP&gt;chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
</node>
<node id="&quot;TABLE 7&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Table 7 is a data table showing experimental results for the Paraphrase Identification (PI) task on MRPC and QQP datasets, using BERT and XLNet as base models with various training objectives."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982</data>
</node>
<node id="&quot;BERT&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERT (Bidirectional Encoder Representations from Transformers) is a language model used in the experiments described, serving as the base architecture for various training objectives."&lt;SEP&gt;"BERT refers to the Bidirectional Encoder Representations from Transformers model introduced by Devlin et al. in 2018, used as a baseline in paraphrase identification and MRC tasks."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982&lt;SEP&gt;chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;XLNET&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"XLNet is a language model introduced by Yang et al. in 2019, used as a baseline in experiments for both MRC and PI tasks, noted for enabling learning of bidirectional contexts."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982</data>
</node>
<node id="&quot;MRPC&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"MRPC (Microsoft Research Paraphrase Corpus) is a dataset and associated evaluation task for determining if two sentences are paraphrases, used in the PI experiments."&lt;SEP&gt;"MRPC (Microsoft Research Paraphrase Corpus) is a paraphrase identification dataset of sentence pairs from online news sources, annotated for semantic equivalence with imbalanced class distribution."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;QQP&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"QQP (Quora Question Pairs) is a dataset and associated evaluation task for paraphrase identification, featuring imbalanced class distribution with 37% positive and 63% negative examples."&lt;SEP&gt;"QQP (Quora Question Pairs) is a paraphrase identification dataset consisting of over 400,000 question pairs from Quora, labeled for semantic equivalence with imbalanced classes."&lt;SEP&gt;"QQP refers to the Quora Question Pairs dataset, an evaluation benchmark used in the experiments to assess data augmentation techniques via F1-score."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08&lt;SEP&gt;chunk-14fe1936981bd0314ca6d73d5425c982&lt;SEP&gt;chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;SQUADV1.1&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SQuADv1.1 is a Machine Reading Comprehension dataset used in evaluating model performance, referenced in the experimental results with EM and F1 scores."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982</data>
</node>
<node id="&quot;DEVLIN ET AL., 2018&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Devlin et al. (2018) are the authors of BERT, which serves as the backbone model in the experiments and introduced the BERT-Tagger baseline."&lt;SEP&gt;"Devlin et al. (2018) are the authors of the BERT model; their BERT-Tagger is used as a baseline in multiple experiments across Chinese and English datasets."&lt;SEP&gt;"Devlin et al., 2018 refers to the authors of the original BERT paper, cited as the source for the BERT model used in the experiments."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982&lt;SEP&gt;chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-aa4690473ce6c28f84935bd046522165</data>
</node>
<node id="&quot;YANG ET AL., 2019&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Yang et al., 2019 refers to the authors of the XLNet paper, cited as the source for the XLNet model used in the experiments."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982</data>
</node>
<node id="&quot;DBPEDIA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"DBpedia is a knowledge base used in the data augmentation process to replace entity mentions during the creation of balanced synthetic datasets."&lt;SEP&gt;"DBpedia is a large-scale, multilingual knowledge base extracted from Wikipedia, used here as a reference for linking entity mentions."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982&lt;SEP&gt;chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;SPACY&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Spacy is a natural language processing library used to retrieve entity mentions for data augmentation in the ablation studies."</data>
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982</data>
</node>
<node id="&quot;XIAOFEI SUN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 Glyce paper."&lt;SEP&gt;"Xiaofei Sun is a researcher and co-author of the paper, affiliated with Shannon.AI."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;JUNJUN LIANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Junjun Liang is a researcher and co-author of the paper, affiliated with Shannon.AI."</data>
  <data key="d2">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;ZHEJIANG UNIVERSITY&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Zhejiang University is an academic institution where Fei Wu is affiliated, specifically through its Department of Computer Science and Technology."</data>
  <data key="d2">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;SHANNON.AI&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Shannon.AI is a research organization where several authors (Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, and Jiwei Li) are affiliated."</data>
  <data key="d2">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;CTB5&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"CTB5 (Chinese Treebank 5.0) is a Chinese dataset used for part-of-speech tagging and parsing, containing text extracted from newswire sources including Xinhua, the Information Services Department of HKSAR, and Sinorama Magazine."&lt;SEP&gt;"CTB5 is a part-of-speech tagging benchmark on which the authors achieved state-of-the-art (SOTA) results using their proposed dice loss."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;CTB6&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"CTB6 (Chinese Treebank 6.0) is an extended version of CTB5, containing a larger collection of words, characters, and sentences for linguistic annotation tasks."&lt;SEP&gt;"CTB6 is a part-of-speech tagging benchmark where the authors reported SOTA performance with their method."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;UD1.4&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"UD1.4 refers to Universal Dependencies version 1.4, a POS tagging dataset on which the authors achieved SOTA results."&lt;SEP&gt;"UD1.4 refers to Universal Dependencies version 1.4, a dataset used for Chinese part-of-speech tagging evaluation."&lt;SEP&gt;"UD1.4 refers to version 1.4 of Universal Dependencies, a framework for consistent grammatical annotation across languages, used here specifically for Chinese part-of-speech tagging."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-a8b34fbfcac06ce762af2dfc253ccd08&lt;SEP&gt;chunk-bbb1108944d391ef267e2d5021e6fafd</data>
</node>
<node id="&quot;SEVENTH CONFERENCE ON NATURAL LANGUAGE LEARNING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"An academic conference (CoNLL 2003) held in Edmonton, Canada from May 31 to June 1, 2003, featuring research on natural language learning."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;REUBEN R. SHAMIR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2019 paper 'Continuous dice coefficient: a method for evaluating probabilistic segmentations' published in CoRR."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;YUVAL DUCHIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 paper on continuous dice coefficient for segmentation evaluation."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;JINYOUNG KIM&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 paper on continuous dice coefficient for segmentation evaluation."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;GUILLERMO SAPIRO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 paper on continuous dice coefficient for segmentation evaluation."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;NOAM HAREL&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 paper on continuous dice coefficient for segmentation evaluation."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;YAN SHAO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2017 paper on character-based joint segmentation and POS tagging for Chinese using bidirectional RNN-CRF."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;CHRISTIAN HARDMEIER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Chinese segmentation and POS tagging paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;JORG TIEDEMANN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Chinese segmentation and POS tagging paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;JOAKIM NIVRE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Chinese segmentation and POS tagging paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;CHEN SHEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2018 paper on the influence of Dice loss in multi-class organ segmentation using 3D fully convolutional networks."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;HOLGER R. ROTH&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 organ segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;HIROHISA ODA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 organ segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;MASAHIRO ODA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 organ segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;YUICHIRO HAYASHI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 organ segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;KAZUNARI MISAWA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 organ segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;KENSAKU MORI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 organ segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;YELONG SHEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2017 paper 'Reasonet: Learning to stop reading in machine comprehension' presented at KDD 2017."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;PO-SEN HUANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Reasonet paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;WEIZHU CHEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Reasonet paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"A major data mining conference in 2017 where the Reasonet paper was presented."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;ACM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Association for Computing Machinery, the publisher and organizer of the KDD conference."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;TH A SORENSEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of a 1948 paper on plant sociology and species similarity, introducing what later became known as the Sørensen index."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;CAROLE H. SUDRE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2017 paper on Generalised Dice overlap as a deep learning loss function for unbalanced segmentations."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;WENQI LI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Generalised Dice overlap paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;TOM VERCAUTEREN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Generalised Dice overlap paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;SEBASTIEN OURSELIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Generalised Dice overlap paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;M. JORGE CARDOSO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 Generalised Dice overlap paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;DEEP LEARNING IN MEDICAL IMAGE ANALYSIS AND MULTIMODAL LEARNING FOR CLINICAL DECISION SUPPORT&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Third International Workshop (DLMIA 2017) held in conjunction with MICCAI 2017 in Quebec City, QC, Canada."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;MICCAI 2017&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The 2017 Medical Image Computing and Computer Assisted Intervention conference, with which DLMIA and ML-CDS workshops were held."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;QUEBEC CITY&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"City in Quebec, Canada, where DLMIA 2017 and ML-CDS 2017 workshops were held in September 2017."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;QC&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Abbreviation for Quebec, a province in Canada; used here to denote the location of the DLMIA 2017 workshop."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;AMOS TVERSKY&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 1977 Psychological Review paper 'Features of similarity', a foundational work in cognitive psychology."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;SERGI VALVERDE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lead author of the 2017 NeuroImage paper on improving multiple sclerosis lesion segmentation using cascaded 3D CNNs."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;MARIANO CABEZAS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;ELOY ROURA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;SANDRA GONZALEZ-VILLA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;DEBORAH PARETO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;JOAN C VILANOVA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;LLUIS RAMIO-TORRENTA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;ALEX ROVIRA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;ARNAU OLIVER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;XAVIER LLADO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2017 multiple sclerosis lesion segmentation paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;SHUOHANG WANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2016 paper 'Machine comprehension using match-LSTM and answer pointer' published as an arXiv preprint."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;JING JIANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2016 match-LSTM machine comprehension paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;ZHIGUO WANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2016 paper 'Multi-perspective context matching for machine comprehension' published as an arXiv preprint."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;HAITAO MI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2016 multi-perspective context matching paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;WAEL HAMZA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2016 multi-perspective context matching paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;RADU FLORIAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2016 multi-perspective context matching paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;JIE MEI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 Glyce paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;PING NIE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 Glyce paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;NAIWEN XUE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2005 paper on the Penn Chinese Treebank, published in Natural Language Engineering."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;FEI XIA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2005 Penn Chinese Treebank paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;FUDONG CHOIU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2005 Penn Chinese Treebank paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;MARTA PALMER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2005 Penn Chinese Treebank paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;ZHILIN YANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lead author of the 2019 XLNet paper published in CoRR."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;ZIHANG DAI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 XLNet paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;YIMING YANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 XLNet paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;JAIME G. CARBONELL&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 XLNet paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;RUSLAN SALAKHUTDINOV&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 XLNet paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;QUOC V. LE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2019 XLNet paper and other machine comprehension papers including QANet."&lt;SEP&gt;"Quoc V. Le is a co-author of the 2018 paper on semi-supervised sequence modeling with cross-view training."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;ADAMS WEI YU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lead author of the 2018 QANet papers on reading comprehension using convolution and self-attention."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;DAVID DOHAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 QANet papers."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;MINH-THANG LUONG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 QANet papers."&lt;SEP&gt;"Minh-Thang Luong is a co-author of the 2018 paper on semi-supervised sequence modeling with cross-view training presented at EMNLP."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;RUI ZHAO&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 QANet papers."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;MOHAMMAD NOROUZI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 QANet papers."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;6TH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"ICLR 2018, held in Vancouver, BC, Canada from April 30 to May 3, 2018, where the QANet paper was presented."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;VANCOUVER&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"City in British Columbia, Canada, where ICLR 2018 was held."&lt;SEP&gt;"Vancouver is a city in British Columbia, Canada, where NeurIPS 2010 was held."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8&lt;SEP&gt;chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;BC&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"British Columbia, a province in Canada; location of ICLR 2018."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;YUE ZHANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Author of the 2018 paper 'Chinese NER using lattice LSTM' published as an arXiv preprint."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;JIE YANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Co-author of the 2018 Chinese NER using lattice LSTM paper."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;CHINESE TREEBANK 5.0/6.0&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Benchmark datasets used for part-of-speech tagging and syntactic parsing of Chinese text."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;UD 1.4&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Universal Dependencies version 1.4, a benchmark dataset used for part-of-speech tagging and syntactic parsing across languages."</data>
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
</node>
<node id="&quot;TABLE 3&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 3 presents experimental results for Chinese POS (Part-of-Speech) tagging datasets, specifically CTB5, CTB6, and UD1.4, showcasing performance metrics of various models."&lt;SEP&gt;"Table 3 presents experimental results on Chinese POS tagging datasets, showing that the proposed DSC loss achieves state-of-the-art performance."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-aa4690473ce6c28f84935bd046522165</data>
</node>
<node id="&quot;TABLE 4&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 4 presents experimental results for English POS tagging datasets, including evaluations on English WSJ and English Tweets corpora."&lt;SEP&gt;"Table 4 presents experimental results for English POS tagging datasets, though specific data is not detailed in the provided text."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-aa4690473ce6c28f84935bd046522165</data>
</node>
<node id="&quot;ZHANG AND YANG, 2018&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zhang and Yang (2018) are researchers who proposed the Lattice-LSTM model for Chinese POS tagging, cited in the results."&lt;SEP&gt;"Zhang and Yang (2018) developed the Lattice-LSTM model using a word-character lattice network for Chinese NLP tasks."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2&lt;SEP&gt;chunk-aa4690473ce6c28f84935bd046522165</data>
</node>
<node id="&quot;BOHNET ET AL., 2018&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Bohnet et al. (2018) are researchers who developed the Meta BiLSTM model evaluated on the English WSJ dataset."</data>
  <data key="d2">chunk-aa4690473ce6c28f84935bd046522165</data>
</node>
<node id="&quot;GODIN, 2019&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Godin (2019) is a researcher associated with the FastText+CNN+CRF model evaluated on English Tweets."</data>
  <data key="d2">chunk-aa4690473ce6c28f84935bd046522165</data>
</node>
<node id="&quot;SPACY1&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Spacy1 refers to the spaCy natural language processing library, used in the text for named entity recognition and linking mentions to DBpedia entities."</data>
  <data key="d2">chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;STANFORD SENTIMENT TREEBANK (SST)&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Stanford Sentiment Treebank is a benchmark dataset for sentiment analysis, comprising SST-2 and SST-5 variants, used to evaluate model performance on accuracy-oriented tasks."</data>
  <data key="d2">chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;TABLE 8&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Table 8 presents experimental results comparing different data augmentation strategies for the Quora Question Pairs (QQP) dataset in terms of F1-score across multiple BERT-based model variants."</data>
  <data key="d2">chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;SST-2&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SST-2 is a binary sentiment classification task derived from the Stanford Sentiment Treebank, used to test model accuracy under different training objectives."</data>
  <data key="d2">chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;SST-5&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"SST-5 is a fine-grained five-class sentiment classification task from the Stanford Sentiment Treebank, evaluated for accuracy in the described experiments."</data>
  <data key="d2">chunk-a3a8758b1de02b56fa79cb146de63691</data>
</node>
<node id="&quot;MRC&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"MRC (Machine Reading Comprehension) is a natural language processing task formalized as predicting starting and ending indexes based on a query and context, characterized by extreme data imbalance with very few positive tokens."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;SØRENSEN–DICE COEFFICIENT&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Sørensen–Dice coefficient, or dice loss, is a training objective proposed to address data imbalance in NLP tasks by equally weighting false positives and false negatives, serving as the harmonic mean of precision and recall."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;DATA RESAMPLING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Data Resampling refers to a set of techniques—including importance sampling, boosting, hard example mining, and oversampling—used to address class imbalance by adjusting example weights or modifying data distributions."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;SELF-PACED LEARNING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Self-paced learning is a training approach where example weights and model parameters are jointly optimized to prioritize easier examples early in training, gradually incorporating harder ones."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;OBJECT DETECTION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Object detection is a computer vision task noted for severe background-object label imbalance, which has motivated extensive research into handling data imbalance."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;SECTION 2&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Section 2 of the paper presents related work on handling data imbalance, including data resampling and approaches from computer vision."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;SECTION 3&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Section 3 describes the proposed loss functions, including dice loss and Tversky index, designed to mitigate data imbalance issues in NLP."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;SECTION 4&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Section 4 presents experimental results demonstrating performance improvements on data-imbalanced NLP tasks using the proposed methods."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;SECTION 5&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Section 5 contains ablation studies evaluating the contribution of individual components of the proposed approach."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;SECTION 6&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"Section 6 provides a brief conclusion summarizing the paper’s contributions and findings."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;KAHN AND MARSHALL&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kahn and Marshall (1953) pioneered importance sampling, a method for assigning weights to samples to alter data distribution during training."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;LIN ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lin et al. (2017) introduced focal loss, a soft weighting scheme that emphasizes harder examples during training, originally in computer vision but influential in NLP."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;KUMAR ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kumar et al. (2010) developed self-paced learning, a method that jointly optimizes model parameters and example weights to learn from easy to hard examples."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;CHAWLA ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Chawla et al. (2002) contributed to data imbalance solutions through oversampling techniques to balance class distributions."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;GIRSHICK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Girshick (2015) is cited in the context of object detection research addressing background-object label imbalance."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;HE ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"He et al. (2015) are referenced for their work on object detection and handling label imbalance in computer vision."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;REN ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ren et al. are mentioned in relation to object detection research tackling data imbalance challenges."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;MALISIEWICZ ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Malisiewicz et al. (2011) proposed hard example mining, which downsamples the majority class and focuses on difficult examples."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;JIANG ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jiang et al. (2017) proposed learning a separate network to predict sample weights for handling data imbalance."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;FAN ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Fan et al. (2018) also worked on predicting sample weights via a separate network to address imbalanced training data."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;SORENSEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Sorensen (1948) introduced the Sørensen–Dice coefficient, foundational to dice loss used in imbalanced classification."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;TVERSKY&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tversky (1977) proposed the Tversky index, a generalization of dice loss that allows flexible trade-offs between precision and recall."</data>
  <data key="d2">chunk-76ac26952e67515817b1d024e77de8e0</data>
</node>
<node id="&quot;XINHUA&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Xinhua is a Chinese news agency whose articles (from 1994–1998) are included in the CTB5 dataset."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;INFORMATION SERVICES DEPARTMENT OF HKSAR&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"The Information Services Department of the Hong Kong Special Administrative Region (HKSAR) contributed 55 news articles from 1997 to the CTB5 dataset."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;SINORAMA MAGAZINE&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Sinorama Magazine is a publication that contributed 132 articles (from 1996–1998 and 2000–2001) to the CTB5 dataset."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;MA AND HOVY&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ma and Hovy are researchers cited for their data processing protocols used in the CoNLL2003 NER experiments."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;WU ET AL.&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Wu et al. are researchers whose data split methodology for Chinese OntoNotes4.0 was adopted in this work."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;ARXIV:1805.02023&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"arXiv:1805.02023 is a 2018 preprint paper titled 'Chinese NER using Lattice LSTM,' describing experiments on multiple NLP benchmarks."</data>
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
</node>
<node id="&quot;CHINESE TREEBANK&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Chinese Treebank (CTB) 5.0/6.0 is a widely used Chinese part-of-speech tagging dataset referenced in the experiments."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;WALL STREET JOURNAL&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Wall Street Journal (WSJ) is an English dataset used for part-of-speech tagging experiments."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;RITTER ET AL. (2011) DATASET&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"This is an English POS tagging dataset introduced by Ritter et al. in 2011, used as a benchmark in the study."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;ENGLISH CONLL 2003&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"English CoNLL 2003 is a standard named entity recognition dataset used for evaluating NER models."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;ENGLISH ONTONOTES5.0&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"English OntoNotes5.0 is another NER evaluation dataset referenced in Table 5."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;LI ET AL., 2019&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Li et al. (2019) introduced the BERT-MRC approach, adapted here for NER tasks."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;PETERS ET AL., 2018&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Peters et al. (2018) developed ELMo, used as a baseline NER model in Table 5."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;CLARK ET AL., 2018&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Clark et al. (2018) proposed the CVT method, used as a baseline in both POS and NER evaluations."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;BERT-TAGGER&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERT-Tagger is a baseline model treating POS tagging as a sequence labeling task using BERT as backbone."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;JOINT-POS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Joint-POS is a baseline model that jointly handles Chinese word segmentation and POS tagging."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;LATTICE-LSTM&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Lattice-LSTM is a neural architecture combining character and word-level information via a lattice structure."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;BERT-MRC&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"BERT-MRC is a machine reading comprehension-based approach adapted for NER, proposed by Li et al."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;ELMO&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"ELMo is a contextualized word representation model used as a baseline for NER."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;CVT&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"CVT (Cross-View Training) is a semi-supervised learning method used as a baseline in the experiments."</data>
  <data key="d2">chunk-a2add12ad84f03be40432d1e511e52e2</data>
</node>
<node id="&quot;IEEE TRANSACTIONS ON NEURAL NETWORKS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"IEEE Transactions on Neural Networks is a scholarly journal that published the paper 'Ramoboost: Ranked minority oversampling in boosting.'"</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;KEVIN CLARK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kevin Clark is a co-author of the 2018 paper on semi-supervised sequence modeling with cross-view training presented at EMNLP."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;CHRISTOPHER D. MANNING&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Christopher D. Manning is a co-author of the 2018 paper on semi-supervised sequence modeling with cross-view training and is a prominent researcher in natural language processing."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;PROCEEDINGS OF THE 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This is an academic conference event held in Brussels, Belgium from October 31 to November 4, 2018, where research papers in NLP were presented."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;BRUSSELS&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Brussels is the capital city of Belgium and the location where the 2018 Conference on Empirical Methods in Natural Language Processing was held."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;BELGIUM&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Belgium is a European country where both the 2018 EMNLP conference and Ghent University (affiliation of Frederic Godin) are located."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;PRADEEP DASIGI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Pradeep Dasigi is a co-author of the 2019 paper introducing the Quoref reading comprehension dataset."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;NELSON F LIU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Nelson F Liu is a co-author of the 2019 Quoref dataset paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;ANA MARASOVIC&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ana Marasovic is a co-author of the 2019 Quoref dataset paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;NOAH A SMITH&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Noah A Smith is a co-author of the 2019 Quoref dataset paper and a well-known computational linguist."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;JACOB DEVLIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jacob Devlin is the lead author of the 2018 BERT paper, a foundational work in language understanding."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;MING-WEI CHANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Ming-Wei Chang is a co-author of the 2018 BERT paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;KRISTINA TOUTANOVA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kristina Toutanova is a co-author of the 2018 BERT paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;LEE R DICE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lee R Dice is the author of a 1945 paper on ecological association measures, known for the Dice coefficient."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;WILLIAM B. DOLAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"William B. Dolan is a co-author of a 2005 paper on automatically constructing a paraphrase corpus."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;CHRIS BROCKETT&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Chris Brockett is a co-author of a 2005 paper on paraphrase corpus construction."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;YANG FAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Yang Fan is the lead author of the 2018 'Learning to teach' paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;FEI TIAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Fei Tian is a co-author of the 2018 'Learning to teach' paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;TAO QIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tao Qin is a co-author of the 2018 'Learning to teach' paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;XIUPING LI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Xiuping Li is a co-author of the 2018 'Learning to teach' paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;TIE-YAN LIU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tie-Yan Liu is a co-author of the 2018 'Learning to teach' paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;JEFF DONAHUE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jeff Donahue is a co-author of the 2013 paper on rich feature hierarchies for object detection."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;TREVOR DARRELL&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Trevor Darrell is a co-author of the 2013 paper on rich feature hierarchies for object detection."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;JITENDRA MALIK&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jitendra Malik is a co-author of the 2013 paper on rich feature hierarchies for object detection."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;XIANGYU ZHANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Xiangyu Zhang is a co-author of the 2015 deep residual learning paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;LU JIANG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Lu Jiang is the lead author of the 2017 Mentornet paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;ZHENGYUAN ZHOU&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Zhengyuan Zhou is a co-author of the 2017 Mentornet paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;THOMAS LEUNG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Thomas Leung is a co-author of the 2017 Mentornet paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;LI-JIA LI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Li-Jia Li is a co-author of the 2017 Mentornet paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;LI FEI-FEI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Li Fei-Fei is a co-author of the 2017 Mentornet paper and a leading figure in computer vision."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;ICML&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"ICML (International Conference on Machine Learning) is a premier machine learning conference where papers like Mentornet and importance sampling were presented."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;H. KAHN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"H. Kahn is a co-author of a 1953 paper on Monte Carlo methods."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;A. W. MARSHALL&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"A. W. Marshall is a co-author of a 1953 paper on Monte Carlo methods."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;ANIL KANDURI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Anil Kanduri is the lead author of the 2018 adboost paper on thermal-aware performance boosting."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;MOHAMMAD HASHEM HAGHBAYAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Mohammad Hashem Haghbayan is a co-author of the 2018 adboost paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;AMIR M. RAHMANI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Amir M. Rahmani is a co-author of the 2018 adboost paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;MUHAMMAD SHAFIQUE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Muhammad Shafique is a co-author of the 2018 adboost paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;AXEL JANTSCH&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Axel Jantsch is a co-author of the 2018 adboost paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;PASI LILJEBERG&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Pasi Liljeberg is a co-author of the 2018 adboost paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;IEEE TRANS. COMPUTERS&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"IEEE Transactions on Computers is a journal that published the 2018 adboost paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;ANGELOS KATHAROPOULOS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Angelos Katharopoulos is the lead author of the 2018 ICML paper on importance sampling in deep learning."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;FRANÇOIS FLEURET&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"François Fleuret is a co-author of the 2018 ICML paper on importance sampling."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;TOMÁŠ KOČISKÝ&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Tomáš Kočiský is the lead author of the 2018 NarrativeQA reading comprehension challenge paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;JONATHAN SCHWARZ&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Jonathan Schwarz is a co-author of the 2018 NarrativeQA paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;PHIL BLUNSOM&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Phil Blunsom is a co-author of the 2018 NarrativeQA paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;CHRIS DYER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Chris Dyer is a co-author of both the 2018 NarrativeQA paper and the 2016 NER architecture paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;KARL MORITZ HERMANN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Karl Moritz Hermann is a co-author of the 2018 NarrativeQA paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;GÁBOR MELIS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Gábor Melis is a co-author of the 2018 NarrativeQA paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;EDWARD GREFENSTETTE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Edward Grefenstette is a co-author of the 2018 NarrativeQA paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;OLDRICH KODYM&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Oldrich Kodym is the lead author of the 2018 GCPR paper on head and neck organ segmentation."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;MICHAL SPANEL&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Michal Spanel is a co-author of the 2018 GCPR paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;ADAM HEROUT&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Adam Herout is a co-author of the 2018 GCPR paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;PATTERN RECOGNITION - 40TH GERMAN CONFERENCE, GCPR 2018&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"GCPR 2018 is a German conference on pattern recognition held in Stuttgart, Germany from October 9–12, 2018."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;STUTTGART&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Stuttgart is a city in Germany where the GCPR 2018 conference was held."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;GERMANY&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Germany is the country where the GCPR 2018 conference took place."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;M. PAWAN KUMAR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"M. Pawan Kumar is the lead author of the 2010 self-paced learning paper presented at NeurIPS."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;BENJAMIN PACKER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Benjamin Packer is a co-author of the 2010 self-paced learning paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;DAPHNE KOLLER&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Daphne Koller is a co-author of the 2010 self-paced learning paper and a renowned AI researcher."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 23&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"NeurIPS 2010 (formerly NIPS) was held December 6–9, 2010 in Vancouver, British Columbia, Canada."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;BRITISH COLUMBIA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"British Columbia is a province in Canada where Vancouver is located and where NeurIPS 2010 took place."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;GUILLAUME LAMPLE&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Guillaume Lample is the lead author of the 2016 paper on neural architectures for named entity recognition."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;MIGUEL BALLESTEROS&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Miguel Ballesteros is a co-author of the 2016 NER paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;SANDEEP SUBRAMANIAN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Sandeep Subramanian is a co-author of the 2016 NER paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;KAZUYA KAWAKAMI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Kazuya Kawakami is a co-author of the 2016 NER paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;GINA-ANNE LEVOW&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Gina-Anne Levow is the author of a 2006 paper on Chinese language processing presented at the Fifth SIGHAN Workshop."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;PROCEEDINGS OF THE FIFTH SIGHAN WORKSHOP ON CHINESE LANGUAGE PROCESSING&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"The Fifth SIGHAN Workshop was held in 2006 in Sydney, Australia, focusing on Chinese language processing tasks."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;SYDNEY&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Sydney is a city in Australia where the Fifth SIGHAN Workshop was held."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;AUSTRALIA&quot;">
  <data key="d0">"GEO"</data>
  <data key="d1">"Australia is the country where the Fifth SIGHAN Workshop took place."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;H. LI&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"H. Li is the lead author of the 2015 CVPR paper on CNN cascade for face detection."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;Z. LIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Z. Lin is a co-author of the 2015 CVPR face detection paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;X. SHEN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"X. Shen is a co-author of the 2015 CVPR face detection paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;J. BRANDT&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"J. Brandt is a co-author of the 2015 CVPR face detection paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;G. HUA&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"G. Hua is a co-author of the 2015 CVPR face detection paper."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"CVPR 2015 is a major computer vision conference where multiple cited papers were presented."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER&quot;">
  <data key="d0">"EVENT"</data>
  <data key="d1">"This refers to an IEEE computer vision or AI conference (likely CVPR) where the Focal Loss paper was presented."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;FREDERIC GODIN&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"Frederic Godin is the author of a 2019 Ph.D. thesis at Ghent University, Belgium, on neural networks for NLP."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;GHENT UNIVERSITY&quot;">
  <data key="d0">"ORGANIZATION"</data>
  <data key="d1">"Ghent University is a Belgian university where Frederic Godin completed his Ph.D. thesis in 2019."</data>
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
</node>
<node id="&quot;BERT + CE&quot;">
  <data key="d2">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
  <data key="d1">"Table 9 references 'BERT + CE' as a method involving fine-tuning BERT with cross-entropy, though 'BERT + CE' itself is not an entity type listed; thus, no explicit relationship is formed per allowed types."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;DATA IMBALANCE ISSUE IN COMPUTER VISION&quot;">
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d1">"Chen et al. (2019) contributed a novel ranking model to address the data imbalance issue in computer vision tasks."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ABDOMINAL CT VOLUMES&quot;">
  <data key="d2">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d1">"Shen et al. (2018) used a dataset of abdominal CT volumes in their study on Dice-based loss; 'Abdominal CT Volumes' represents a geo/medical imaging context, interpreted as a geo-related data source."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;QANET&quot;">
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d1">"Yu et al. (2018b) developed the QANet model, whose results are reported in Table 6 for MRC benchmarks."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;MACHINE READING COMPREHENSION&quot;">
  <data key="d2">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d1">"Seo et al. (2016) defined standard protocols for the MRC task followed in this study."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;PI&quot;">
  <data key="d2">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d1">"Table 7 presents the experimental results specifically for the Paraphrase Identification (PI) task."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d2">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d1">"Shannon.AI is the primary research organization behind the development and publication of the proposed dice loss methodology."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;PENN CHINESE TREEBANK&quot;">
  <data key="d2">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d1">"Naiwen Xue is the lead author of the paper describing the Penn Chinese Treebank."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;UNIVERSAL DEPENDENCIES&quot;">
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d1">"UD1.4 is a specific version of the Universal Dependencies framework used for Chinese POS tagging."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;CHINESE ONTONOTES4.0&quot;">
  <data key="d2">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d1">"The data split for Chinese OntoNotes4.0 follows the approach used by Wu et al. (2019)."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ARXIV PREPRINT ARXIV:1908.05803&quot;">
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d1">"Pradeep Dasigi is the lead author of the Quoref dataset paper posted on arXiv."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ARXIV PREPRINT ARXIV:1810.04805&quot;">
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d1">"Jacob Devlin is the lead author of the BERT paper posted on arXiv."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)&quot;">
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d1">"Ross B. Girshick presented the Fast R-CNN paper at ICCV 2015."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)&quot;">
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d1">"Kaiming He presented the ResNet paper at CVPR 2016."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;ARXIV PREPRINT ARXIV:1603.01360&quot;">
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d1">"Guillaume Lample is the lead author of the NER architecture paper on arXiv."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;CORR, ABS/1910.11476&quot;">
  <data key="d2">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d1">"Xiaoya Li is the lead author of the MRC-NER paper posted on CoRR (arXiv)."</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<edge source="&quot;FIGURE 1&quot;" target="&quot;IMAGE_14&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_14" is the image of "FIGURE 1".</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;∇ FL(Γ=1)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The blue curve is part of the graph, showing the derivative behavior of Focal Loss."</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;∇ DL(Γ=1)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The orange curve is part of the graph, showing the derivative behavior of Dice Loss."</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;∇ TL(Β=0.5)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The yellow curve is part of the graph, showing the derivative behavior of Tversky Loss."</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;∇ DSC&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The purple curve is part of the graph, showing the derivative behavior of Dice Similarity Coefficient."</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;Legend&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The legend is located within the graph to explain the meaning of each colored curve."</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;X-Axis&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The X-axis forms the horizontal baseline of the graph, defining the input variable."</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;Y-Axis&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Y-axis forms the vertical baseline of the graph, defining the output variable."</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Figure 1 illustrates the derivative behavior of DSC compared to other loss functions, showing how DSC stops providing gradient once p &gt; 0.5."</data>
  <data key="d5">chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FIGURE 1&quot;" target="&quot;TABLE 2&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Figure 1 visually explains concepts summarized in Table 2 regarding loss function derivatives."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;∇ FL(Γ=1)&quot;" target="&quot;IMAGE_14&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"∇ FL(γ=1)是从image_14中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;∇ DL(Γ=1)&quot;" target="&quot;IMAGE_14&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"∇ DL(γ=1)是从image_14中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;∇ TL(Β=0.5)&quot;" target="&quot;IMAGE_14&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"∇ TL(β=0.5)是从image_14中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;∇ DSC&quot;" target="&quot;IMAGE_14&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"∇ DSC是从image_14中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Legend&quot;" target="&quot;IMAGE_14&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Legend是从image_14中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;X-Axis&quot;" target="&quot;IMAGE_14&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"X-axis是从image_14中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Y-Axis&quot;" target="&quot;IMAGE_14&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Y-axis是从image_14中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_14.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_13&quot;" target="&quot;F1 SCORE / DICE COEFFICIENT&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_13" is the image of "DICE COEFFICIENT".</data>
  <data key="d5">examples/example_working/images/image_13.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_13&quot;" target="&quot;Numerical constant multiplier (2) in the Dice coefficient formula&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"2是从image_13中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_13.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_13&quot;" target="&quot;Indicator function I(p_i1 &gt; 0.5)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"I(p_i1 &gt; 0.5)是从image_13中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_13.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_13&quot;" target="&quot;True label Y_i1&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_i1是从image_13中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_13.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;F1 SCORE / DICE COEFFICIENT&quot;" target="&quot;Numerical constant multiplier (2) in the Dice coefficient formula&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The F1 score is scaled by multiplying with the constant 2."</data>
  <data key="d5">examples/example_working/images/image_13.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;F1 SCORE / DICE COEFFICIENT&quot;" target="&quot;Indicator function I(p_i1 &gt; 0.5)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The numerator and denominator of the F1 score both depend on the indicator function I(p_i1 &gt; 0.5), which determines whether the prediction is considered positive."</data>
  <data key="d5">examples/example_working/images/image_13.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;F1 SCORE / DICE COEFFICIENT&quot;" target="&quot;True label Y_i1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The true label y_i1 is used in both the numerator and denominator of the F1 score calculation, directly influencing its value."</data>
  <data key="d5">examples/example_working/images/image_13.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;F1 SCORE / DICE COEFFICIENT&quot;" target="&quot;p_{i1}&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The DL metric depends on p_i1 through its appearance in both the numerator and denominator of the fraction."</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;F1 SCORE / DICE COEFFICIENT&quot;" target="&quot;y_{i1}&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The DL metric depends on y_i1 through its presence in both the numerator and denominator of the fraction."</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;F1 SCORE / DICE COEFFICIENT&quot;" target="&quot;IMAGE_10&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_10" is the image of "DICE LOSS (DL)".</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;F1 SCORE / DICE COEFFICIENT&quot;" target="&quot;γ (gamma)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The DL metric includes γ as a constant additive term in both the numerator and denominator, affecting the overall value."</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;F1 SCORE / DICE COEFFICIENT&quot;" target="&quot;Σ (summation operator)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The DL metric uses summation (Σ) operations to aggregate values across all indices i for both p_i1 and y_i1."</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Indicator function I(p_i1 &gt; 0.5)&quot;" target="&quot;True label Y_i1&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The indicator function and the true label are compared to evaluate precision and recall components of the F1 score."</data>
  <data key="d5">examples/example_working/images/image_13.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;TVERSKY LOSS (TL)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"N是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;i&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"i是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;p_{i1}&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"p_i1是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;y_{i1}&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_i1是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;\gamma&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"γ是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;\alpha&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"α是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;\beta&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"β是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;p_{i0}&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"p_i0是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_12&quot;" target="&quot;y_{i0}&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_i0是从image_12中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;i&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The summation in the definition of TL iterates over each index i from 1 to N, indicating that i defines the scope of the computation."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;p_{i1}&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The probability p_i1 influences the numerator and denominator of each term in the summation contributing to TL."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;y_{i1}&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The label y_i1 directly affects the numerator and denominator of each term in the summation contributing to TL."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;\gamma&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The constant γ is added to both numerator and denominator to stabilize the computation and prevent division by zero."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;\alpha&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The hyperparameter α scales the influence of p_i0*y_i0 in the denominator, thereby modulating how much weight is given to incorrect predictions."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;\beta&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The hyperparameter β scales the influence of p_i0*y_i1 in the denominator, affecting the penalty for misclassification."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;p_{i0}&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The probability p_i0 contributes to the denominator through interactions with y_i0 and y_i1, influencing the overall value of each term."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;y_{i0}&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The label y_i0 interacts with p_i0 in the denominator, affecting the magnitude of the term in the summation."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;TVERSKY LOSS (TL)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The TL metric is computed as an average over N terms, making N a fundamental parameter in determining its value."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;TVERSKY INDEX (TI)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Tversky Loss is directly derived from the Tversky Index by transforming it into a differentiable loss function for optimization."</data>
  <data key="d5">chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;γ (smoothing factor)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"TL generalizes DL by incorporating additional parameters α and β to balance false positives and false negatives."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;Loss Functions in Machine Learning&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"TL is a generalized form of loss function that extends the concept of DL with tunable parameters."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"TL是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY LOSS (TL)&quot;" target="&quot;Loss Function Formula (One Sample x_i)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The TL formula expands on DL with extra parameters α and β, still adhering to the same per-sample computational pattern."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;p_{i1}&quot;" target="&quot;y_{i1}&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"p_i1 and y_i1 are paired values representing predicted and true labels for the same pixel x_i."</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;p_{i1}&quot;" target="&quot;IMAGE_10&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"p_i1是从image_10中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;p_{i1}&quot;" target="&quot;γ (gamma)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"γ is added to p_i1 in both numerator and denominator to avoid division by zero."</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;p_{i1}&quot;" target="&quot;IMAGE_8&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"p_i1是从image_8中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;p_{i1}&quot;" target="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"DSC uses p_i1, the predicted value, as part of its numerator and denominator to compute similarity."</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;y_{i1}&quot;" target="&quot;p_{i0}&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The probability p_i0 and label y_i1 are combined in the denominator, representing the penalty for assigning high confidence to the wrong class."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;y_{i1}&quot;" target="&quot;IMAGE_10&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_i1是从image_10中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;y_{i1}&quot;" target="&quot;γ (gamma)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"γ is added to y_i1 in both numerator and denominator to avoid division by zero."</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;y_{i1}&quot;" target="&quot;IMAGE_8&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_i1是从image_8中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;y_{i1}&quot;" target="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"DSC uses y_i1, the true label, as part of its numerator and denominator to compute similarity."</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;p_{i0}&quot;" target="&quot;y_{i0}&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The probability p_i0 and label y_i0 are combined in the denominator, representing the confidence in predicting the negative class."</data>
  <data key="d5">examples/example_working/images/image_12.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_11&quot;" target="&quot;TVERSKY INDEX (TI)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"B \ A是从image_11中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_11.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_11&quot;" target="&quot;α (alpha)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"α是从image_11中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_11.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_11&quot;" target="&quot;β (beta)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"β是从image_11中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_11.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_11&quot;" target="&quot;Tversky Index (TI) Formula&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Formula是从image_11中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_11.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY INDEX (TI)&quot;" target="&quot;α (alpha)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The coefficient α scales the influence of A \ B on the value of the measure TI."</data>
  <data key="d5">examples/example_working/images/image_11.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY INDEX (TI)&quot;" target="&quot;β (beta)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The coefficient β scales the influence of B \ A on the value of the measure TI."</data>
  <data key="d5">examples/example_working/images/image_11.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY INDEX (TI)&quot;" target="&quot;Tversky Index (TI) Formula&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"TI is defined by the Formula, which specifies how the sets A and B and their relationships contribute to the final value."</data>
  <data key="d5">examples/example_working/images/image_11.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY INDEX (TI)&quot;" target="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Tversky Index extends the Dice Coefficient to a more general case, functioning as an approximation of the Fβ score."</data>
  <data key="d5">chunk-bff777a634a5af0182c5787f1022b29f</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY INDEX (TI)&quot;" target="&quot;MRC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Tversky index is used as a more flexible alternative to dice loss for handling MRC's data imbalance by approximating Fβ scores."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY INDEX (TI)&quot;" target="&quot;TVERSKY&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Tversky (1977) developed the Tversky index, which extends the Dice coefficient with adjustable weights."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY INDEX (TI)&quot;" target="&quot;SECTION 3&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Section 3 describes the Tversky index as an alternative loss function for imbalanced NLP tasks."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TVERSKY INDEX (TI)&quot;" target="&quot;TVERSKY INDEX (TI)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"TI is inversely related to the size of B \ A, modulated by the coefficient β, contributing to the denominator."</data>
  <data key="d5">examples/example_working/images/image_11.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_10&quot;" target="&quot;γ (gamma)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"γ是从image_10中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_10&quot;" target="&quot;Σ (summation operator)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Σ是从image_10中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_10.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (gamma)&quot;" target="&quot;IMAGE_8&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"γ是从image_8中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (gamma)&quot;" target="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"DSC incorporates γ to ensure numerical stability when computing the similarity score."</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_9&quot;" target="&quot;N (Total Number of Samples)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"N是从image_9中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_9&quot;" target="&quot;π₁ (pi1)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"pi1是从image_9中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_9&quot;" target="&quot;yᵢ₁ (yi1)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"yi1是从image_9中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_9&quot;" target="&quot;γ (smoothing factor)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_9" is the image of "DICE LOSS (DL)".</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_9&quot;" target="&quot;Σ (Summation Operator)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Σ是从image_9中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;N (Total Number of Samples)&quot;" target="&quot;γ (smoothing factor)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"DL is computed as an average over N data points, so N directly affects the normalization of the loss value."</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;π₁ (pi1)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"DL depends on pi1 because pi1 appears in the numerator and denominator of the fraction inside the summation."</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;yᵢ₁ (yi1)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"DL depends on yi1 because it is part of the numerator in the fraction within the summation."</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;γ (smoothing factor)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"DL includes γ to ensure numerical stability by preventing division by zero when pi1 and yi1 are both zero."</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;Σ (Summation Operator)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"DL is defined using the summation operator Σ, which aggregates the individual terms across all data points."</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Dice Loss is directly derived from the Dice Coefficient, adapting it into a differentiable loss function suitable for model training."</data>
  <data key="d5">chunk-bff777a634a5af0182c5787f1022b29f</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;Loss Functions in Machine Learning&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"DL is a type of loss function focused on improving segmentation through dice-based metrics."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"DL是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;Loss Function Formula (One Sample x_i)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The DL formula uses p_i1, y_i1, and γ in a fractional form consistent with the general format provided for one sample."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;TABLE 2&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Table 2 includes the formula for Dice Loss (DL) among other loss functions, providing a concise reference for its mathematical expression."</data>
  <data key="d5">chunk-bff777a634a5af0182c5787f1022b29f</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;γ (smoothing factor)&quot;" target="&quot;MILLETARI ET AL. (2016)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Milletari et al. (2016) proposed a specific squared-form modification to the Dice Loss denominator to improve convergence speed."</data>
  <data key="d5">chunk-bff777a634a5af0182c5787f1022b29f</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;π₁ (pi1)&quot;" target="&quot;yᵢ₁ (yi1)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"pi1 and yi1 are paired in the formula, indicating their roles in comparing predicted values against actual labels."</data>
  <data key="d5">examples/example_working/images/image_9.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_8&quot;" target="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_8" is the image of "DICE COEFFICIENT".</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_8&quot;" target="&quot;x_i&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"x_i是从image_8中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;x_i&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"DSC is a function that evaluates the similarity for a specific pixel or region x_i."</data>
  <data key="d5">examples/example_working/images/image_8.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Loss Functions in Machine Learning&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"DSC is a specific loss formulation derived from the Dice coefficient, used to evaluate segmentation quality."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"DSC是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Loss Function Formula (One Sample x_i)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The DSC formula follows the same structural pattern as other losses, applying a transformation based on p_i1, y_i1, and γ."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;IMAGE_6&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_6" is the image of "DICE COEFFICIENT".</data>
  <data key="d5">examples/example_working/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Input Data Point (X_I)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The DSC function takes x_i as input to compute the similarity score."</data>
  <data key="d5">examples/example_working/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Predicted Probability for Class 1 (P_I1)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The DSC formula uses p_i1 as part of the numerator and denominator to calculate similarity."</data>
  <data key="d5">examples/example_working/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Ground Truth Label for Class 1 (Y_I1)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The DSC formula uses y_i1 as part of the numerator and denominator to calculate similarity."</data>
  <data key="d5">examples/example_working/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;IMAGE_5&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_5" is the image of "DICE COEFFICIENT".</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;True Positives (TP)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Dice Similarity Coefficient formula includes True Positives as a core component in its numerator and denominator."</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;False Negatives (FN)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Dice Similarity Coefficient formula includes False Negatives in its denominator, affecting the overall similarity score."</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;False Positives (FP)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Dice Similarity Coefficient formula includes False Positives in its denominator, influencing the final similarity value."</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Precision (PRE)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Dice Similarity Coefficient is mathematically equivalent to the F1 score, which depends on Precision."</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Recall (REC)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Dice Similarity Coefficient is mathematically equivalent to the F1 score, which depends on Recall."</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;F1 SCORE&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The Dice Similarity Coefficient (DSC) is equal to the F1 score, both representing the harmonic mean of precision and recall."</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;BERT&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"DSC is applied as a training objective in combination with BERT to improve performance in both MRC and PI tasks."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;XLNET&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"DSC is applied as a training objective in combination with XLNet to improve performance in both MRC and PI tasks."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;IMAGE_4&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_4" is the image of "DICE COEFFICIENT".</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Set A (Predicted Positive Examples)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The DSC formula computes the similarity between set A and set B."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Set B (Ground Truth Positive Examples)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The DSC formula computes the similarity between set A and set B."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;|A ∩ B| (True Positives)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The DSC formula uses the intersection size |A ∩ B| as part of its numerator."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;|A| (Total Predicted Positives)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The DSC formula uses the size of set A, |A|, in its denominator."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;|B| (Total Ground Truth Positives)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The DSC formula uses the size of set B, |B|, in its denominator."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;Constant Multiplier 2 in DSC Numerator&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The constant 2 scales the intersection term in the numerator of the DSC formula."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;SHAMIR ET AL., 2019&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Shamir et al. (2019) extended the classical Dice coefficient definition for probabilistic map comparison."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;SØRENSEN, 1948&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Sørensen (1948) co-developed the foundational Dice Coefficient for set similarity."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;DICE, 1945&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Dice (1945) independently formulated the Dice Coefficient, which bears his name."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DICE SIMILARITY COEFFICIENT (DSC)&quot;" target="&quot;SELF-ADJUSTING DICE LOSS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Self-adjusting Dice Loss modifies the standard DSC by introducing a weighting factor (1 - p) to adaptively focus on hard examples."</data>
  <data key="d5">chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Loss Functions in Machine Learning&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Loss是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Loss Functions in Machine Learning&quot;" target="&quot;Cross-Entropy Loss (CE)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"CE is a type of loss function within the broader category of loss functions."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Loss Functions in Machine Learning&quot;" target="&quot;Weighted Cross-Entropy Loss (WCE)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"WCE is a variant of the loss function family, specifically a weighted version of cross-entropy."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Loss Functions in Machine Learning&quot;" target="&quot;FOCAL LOSS (FL)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"FL is a specialized loss function designed to handle class imbalance issues in classification tasks."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CE是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Weighted Cross-Entropy Loss (WCE)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"WCE builds upon CE by introducing a weight α_i to adjust the influence of individual samples, making it suitable for imbalanced datasets."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Loss Function Formula (One Sample x_i)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The formula for CE is expressed using the summation over binary labels and log probabilities, which is part of the general structure described under Formula (one sample x_i)."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;IMAGE_3&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CE是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Total Number of Samples (N)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The cross-entropy loss is normalized by dividing by N, the total number of samples."</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Sample Weight Coefficient (α_i)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Each sample's contribution to the cross-entropy is weighted by α_i."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;True Label Indicator (y_ij)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The true labels y_ij determine the ground truth for computing the loss."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Predicted Class Probability (p_ij)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The predicted probabilities p_ij are used to compute the log-likelihood term in the loss."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Natural Logarithm Function (log)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The logarithmic function is applied to p_ij to measure prediction confidence."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Summation Operator (Σ)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Multiple summations are used to aggregate contributions across samples and classes."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Binary Class Index Set (j ∈ {0,1})&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The summation over j is constrained to binary classes, making it suitable for binary classification tasks."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;IMAGE_2&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CE是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;True Label (y_ij)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The true labels y_ij are used to compute the cross-entropy loss."</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Predicted Probability (p_ij)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The predicted probabilities p_ij are directly used in the computation of cross-entropy."</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Natural Logarithm (log)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The logarithm function is applied to p_ij to measure the uncertainty in predictions."</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Weighted Cross-Entropy Loss (WCE)&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"WCE是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Weighted Cross-Entropy Loss (WCE)&quot;" target="&quot;Loss Function Formula (One Sample x_i)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The WCE formula incorporates α_i into the CE framework, fitting within the overall formula structure for one sample."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FOCAL LOSS (FL)&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"FL是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FOCAL LOSS (FL)&quot;" target="&quot;Loss Function Formula (One Sample x_i)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The FL formula introduces a focal term (1-p_ij)^γ, modifying the standard log probability term while remaining consistent with the per-sample formula style."</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FOCAL LOSS (FL)&quot;" target="&quot;SELF-ADJUSTING DICE LOSS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The Self-adjusting Dice Loss mimics the core idea of Focal Loss by down-weighting easy examples during training, similar to how Focal Loss uses (1 - p)^γ."</data>
  <data key="d5">chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FOCAL LOSS (FL)&quot;" target="&quot;MRC&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Focal loss inspires the dynamic weighting strategy applied to MRC to reduce the dominance of easy-negative examples."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FOCAL LOSS (FL)&quot;" target="&quot;LIN ET AL.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Lin et al. (2017) originated focal loss in computer vision, which inspired the dynamic weighting approach in this NLP context."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FOCAL LOSS (FL)&quot;" target="&quot;FOCAL LOSS (FL)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Lin et al., 2017 are the original proposers of Focal Loss in the domain of object detection."</data>
  <data key="d5">chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Loss Function Formula (One Sample x_i)&quot;" target="&quot;IMAGE_7&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Formula (one sample x_i)是从image_7中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_7&quot;" target="&quot;TABLE 2&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_7" is the image of "TABLE 2".</data>
  <data key="d5">examples/example_working/images/image_7.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;Input Data Point (X_I)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"x_i是从image_6中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;Predicted Probability for Class 1 (P_I1)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"p_i1是从image_6中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;Ground Truth Label for Class 1 (Y_I1)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_i1是从image_6中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Predicted Probability for Class 1 (P_I1)&quot;" target="&quot;Ground Truth Label for Class 1 (Y_I1)&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Both p_i1 and y_i1 are paired values for the same data point i, representing predicted and actual labels respectively."</data>
  <data key="d5">examples/example_working/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_5&quot;" target="&quot;True Positives (TP)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"TP是从image_5中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_5&quot;" target="&quot;False Negatives (FN)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"FN是从image_5中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_5&quot;" target="&quot;False Positives (FP)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"FP是从image_5中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_5&quot;" target="&quot;Precision (PRE)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Pre是从image_5中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_5&quot;" target="&quot;Recall (REC)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Rec是从image_5中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_5&quot;" target="&quot;F1 SCORE&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"F1是从image_5中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Precision (PRE)&quot;" target="&quot;Recall (REC)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Precision and Recall are combined in the F1 score, reflecting their joint contribution to model evaluation."</data>
  <data key="d5">examples/example_working/images/image_5.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;Set A (Predicted Positive Examples)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"A是从image_4中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;Set B (Ground Truth Positive Examples)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"B是从image_4中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;|A ∩ B| (True Positives)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"|A ∩ B|是从image_4中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;|A| (Total Predicted Positives)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"|A|是从image_4中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;|B| (Total Ground Truth Positives)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"|B|是从image_4中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;Constant Multiplier 2 in DSC Numerator&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"2是从image_4中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Set A (Predicted Positive Examples)&quot;" target="&quot;|A ∩ B| (True Positives)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The intersection |A ∩ B| is derived from the elements common to set A."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Set A (Predicted Positive Examples)&quot;" target="&quot;|A| (Total Predicted Positives)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The cardinality |A| represents the total number of elements in set A."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Set B (Ground Truth Positive Examples)&quot;" target="&quot;|A ∩ B| (True Positives)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The intersection |A ∩ B| is derived from the elements common to set B."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Set B (Ground Truth Positive Examples)&quot;" target="&quot;|B| (Total Ground Truth Positives)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The cardinality |B| represents the total number of elements in set B."</data>
  <data key="d5">examples/example_working/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Total Number of Samples (N)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"N是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Sample Weight Coefficient (α_i)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"α_i是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;True Label Indicator (y_ij)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_ij是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Predicted Class Probability (p_ij)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"p_ij是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Natural Logarithm Function (log)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"log是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Summation Operator (Σ)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Σ是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Binary Class Index Set (j ∈ {0,1})&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"j∈{0,1}是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="Cross Entropy Loss">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_3" is the image of Cross Entropy Loss.</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Total Number of Samples (N)&quot;" target="&quot;IMAGE_2&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"N是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Total Number of Samples (N)&quot;" target="&quot;Summation Over Samples (∑_i)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The outer summation runs over all N samples."</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Sample Weight Coefficient (α_i)&quot;" target="&quot;True Label Indicator (y_ij)&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The weight α_i scales the contribution of the true label y_ij for each sample."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;True Label Indicator (y_ij)&quot;" target="&quot;Predicted Class Probability (p_ij)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The true label y_ij and predicted probability p_ij are paired in the log term to compute per-sample loss."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Predicted Class Probability (p_ij)&quot;" target="&quot;Natural Logarithm Function (log)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The logarithm is applied directly to the predicted probability p_ij to evaluate prediction accuracy."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="Cross Entropy Loss" target="&quot;IMAGE_2&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_2" is the image of Cross Entropy Loss.</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_2&quot;" target="&quot;Sample Index (i)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"i是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_2&quot;" target="&quot;Class Label Index (j)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"j是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_2&quot;" target="&quot;True Label (y_ij)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_ij是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_2&quot;" target="&quot;Predicted Probability (p_ij)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"p_ij是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_2&quot;" target="&quot;Natural Logarithm (log)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"log是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_2&quot;" target="&quot;Summation Over Samples (∑_i)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Summation over i是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_2&quot;" target="&quot;Summation Over Classes (∑_j)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Summation over j是从image_2中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Class Label Index (j)&quot;" target="&quot;Summation Over Classes (∑_j)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The inner summation iterates over the two possible class labels, j ∈ {0,1}."</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;True Label (y_ij)&quot;" target="&quot;Predicted Probability (p_ij)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The true labels y_ij and predicted probabilities p_ij are paired in the formula to evaluate prediction accuracy."</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Summation Over Samples (∑_i)&quot;" target="&quot;Summation Over Classes (∑_j)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The inner summation over j is nested within the outer summation over i, indicating per-sample evaluation across classes."</data>
  <data key="d5">examples/example_working/images/image_2.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;CONLL03&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CoNLL03 NER是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;ONTONOTES5.0&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"OntoNotes5.0 NER是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;SQUAD 1.1&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"SQuAD 1.1 (Rajpurkar et al., 2016)是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;SQUAD 2.0&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"SQuAD 2.0 (Rajpurkar et al., 2018)是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;QUOREF&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"QUOREF (Dasigi et al., 2019)是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;TASK&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Task是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;# NEG&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"# neg是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;# POS&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"# pos是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;RATIO&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"ratio是从image_1中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_1&quot;" target="&quot;TABLE 1&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_1" is the image of "TABLE 1".</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL03&quot;" target="&quot;TASK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"CoNLL03 NER is a specific task listed under the 'Task' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL03&quot;" target="&quot;# NEG&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of negative instances for CoNLL03 NER is 170K as indicated in the '# neg' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL03&quot;" target="&quot;# POS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of positive instances for CoNLL03 NER is 34K as indicated in the '# pos' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL03&quot;" target="&quot;RATIO&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ratio of negative to positive instances for CoNLL03 NER is 4.98 as indicated in the 'ratio' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL03&quot;" target="&quot;TABLE 1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Table 1 includes quantitative data about the CoNLL03 dataset, specifically its negative-positive example ratio."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL03&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The proposed method achieves competitive or better results on the CoNLL03 NER task."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES5.0&quot;" target="&quot;TASK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"OntoNotes5.0 NER is a specific task listed under the 'Task' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES5.0&quot;" target="&quot;# NEG&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of negative instances for OntoNotes5.0 NER is 1.96M as indicated in the '# neg' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES5.0&quot;" target="&quot;# POS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of positive instances for OntoNotes5.0 NER is 239K as indicated in the '# pos' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES5.0&quot;" target="&quot;RATIO&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ratio of negative to positive instances for OntoNotes5.0 NER is 8.18 as indicated in the 'ratio' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES5.0&quot;" target="&quot;TABLE 1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Table 1 provides statistics on the data imbalance in the OntoNotes5.0 dataset."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES5.0&quot;" target="&quot;TABLE 5&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 5 includes experimental results for models evaluated on the OntoNotes5.0 dataset."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES5.0&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The proposed method achieves competitive or better results on the OntoNotes5.0 NER task."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES5.0&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper employs English OntoNotes5.0 in its NER evaluation."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 1.1&quot;" target="&quot;TASK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"SQuAD 1.1 is a specific task listed under the 'Task' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 1.1&quot;" target="&quot;# NEG&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of negative instances for SQuAD 1.1 is 10.3M as indicated in the '# neg' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 1.1&quot;" target="&quot;# POS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of positive instances for SQuAD 1.1 is 175K as indicated in the '# pos' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 1.1&quot;" target="&quot;RATIO&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ratio of negative to positive instances for SQuAD 1.1 is 55.9 as indicated in the 'ratio' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 1.1&quot;" target="&quot;TABLE 1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Table 1 lists the number of negative and positive examples and the ratio for SQuAD 1.1."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 1.1&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SQuAD 1.1 is cited as an example of a highly imbalanced MRC task addressed by the proposed methodology."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 2.0&quot;" target="&quot;TASK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"SQuAD 2.0 is a specific task listed under the 'Task' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 2.0&quot;" target="&quot;# NEG&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of negative instances for SQuAD 2.0 is 15.4M as indicated in the '# neg' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 2.0&quot;" target="&quot;# POS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of positive instances for SQuAD 2.0 is 188K as indicated in the '# pos' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 2.0&quot;" target="&quot;RATIO&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ratio of negative to positive instances for SQuAD 2.0 is 82.0 as indicated in the 'ratio' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 2.0&quot;" target="&quot;TABLE 1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Table 1 includes data imbalance metrics for the SQuAD 2.0 dataset."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD 2.0&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"SQuAD 2.0 is cited as an example of a highly imbalanced MRC task addressed by the proposed methodology."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;TASK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"QUOREF is a specific task listed under the 'Task' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;# NEG&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of negative instances for QUOREF is 6.52M as indicated in the '# neg' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;# POS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The number of positive instances for QUOREF is 38.6K as indicated in the '# pos' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;RATIO&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The ratio of negative to positive instances for QUOREF is 169 as indicated in the 'ratio' column."</data>
  <data key="d5">examples/example_working/images/image_1.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;TABLE 1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Table 1 reports the extreme imbalance ratio (169) for the QUOREF dataset."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;TABLE 6&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 6 reports performance metrics on the Quoref dataset for various MRC models."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;DASIGI ET AL. (2019)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Dasigi et al. (2019) introduced the Quoref dataset, which appears in Table 6’s MRC results."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;XLNET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"XLNet is used as a baseline model evaluated on the QuoRef dataset in the MRC task."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"QUOREF is cited as an example of an extremely imbalanced MRC task relevant to the paper's focus."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOREF&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper evaluates on the Quoref dataset for coreferential reasoning in reading comprehension."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 9&quot;" target="&quot;BERT + CE&quot;">
  <data key="d3">0.0</data>
  <data key="d4">"Table 9 references 'BERT + CE' as a method involving fine-tuning BERT with cross-entropy, though 'BERT + CE' itself is not an entity type listed; thus, no explicit relationship is formed per allowed types."</data>
  <data key="d5">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 9&quot;" target="&quot;BERT&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT models with different loss functions (CE, DL, DSC) are evaluated in Table 9 on SST-2 and SST-5 for accuracy."</data>
  <data key="d5">chunk-a3a8758b1de02b56fa79cb146de63691</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 9&quot;" target="&quot;STANFORD SENTIMENT TREEBANK (SST)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Table 9 presents results from experiments on the Stanford Sentiment Treebank datasets, including SST-2 and SST-5."</data>
  <data key="d5">chunk-a3a8758b1de02b56fa79cb146de63691</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 10&quot;" target="&quot;CHINESE ONTONOTES4.0 NER DATASET&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 10 contains experimental results specifically derived from the Chinese OntoNotes4.0 NER dataset, showing F1 scores at varying α values."</data>
  <data key="d5">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 10&quot;" target="&quot;ENGLISH QUOREF MRC DATASET&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 10 includes performance results on the English QuoRef MRC dataset across different α settings in the Tversky Index."</data>
  <data key="d5">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QINGHONG HAN&quot;" target="&quot;NATIONAL NATURAL SCIENCE FOUNDATION OF CHINA&quot;">
  <data key="d3">0.0</data>
  <data key="d4">"While both are mentioned in the Acknowledgement section, there is no direct indication that Qinghong Han is affiliated with NSFC; the relationship is not clearly established."</data>
  <data key="d5">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;WEI WU&quot;" target="&quot;NATIONAL NATURAL SCIENCE FOUNDATION OF CHINA&quot;">
  <data key="d3">0.0</data>
  <data key="d4">"No explicit link is provided between Wei Wu and the NSFC beyond both being mentioned in the Acknowledgement section."</data>
  <data key="d5">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;JIAWEI WU&quot;" target="&quot;NATIONAL NATURAL SCIENCE FOUNDATION OF CHINA&quot;">
  <data key="d3">0.0</data>
  <data key="d4">"There is no clear evidence in the text linking Jiawei Wu directly to the NSFC funding body."</data>
  <data key="d5">chunk-190fb76ec24199346ebe1de20cc27c3d</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;GIRSHICK, 2015&quot;" target="&quot;GIRSHICK ET AL., 2013&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Ross Girshick is a key author in both the 2013 and 2015 works on object detection and class imbalance."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;GIRSHICK ET AL., 2013&quot;" target="&quot;HARD NEGATIVE MINING (HNM)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Girshick et al. (2013) introduced the Hard Negative Mining technique in object detection."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;PANG ET AL., 2019&quot;" target="&quot;IOU-BALANCED SAMPLING&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Pang et al. (2019) proposed the IoU-balanced sampling method."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHEN ET AL., 2019&quot;" target="&quot;DATA IMBALANCE ISSUE IN COMPUTER VISION&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Chen et al. (2019) contributed a novel ranking model to address the data imbalance issue in computer vision tasks."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SUDRE ET AL., 2017&quot;" target="&quot;GENERALIZED DICE LOSS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Sudre et al. (2017) proposed using Generalized Dice Loss for image segmentation with class imbalance."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SHEN ET AL., 2018&quot;" target="&quot;ABDOMINAL CT VOLUMES&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"Shen et al. (2018) used a dataset of abdominal CT volumes in their study on Dice-based loss; 'Abdominal CT Volumes' represents a geo/medical imaging context, interpreted as a geo-related data source."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;KODYM ET AL., 2018&quot;" target="&quot;BATCH SOFT DICE LOSS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Kodym et al. (2018) introduced the Batch Soft Dice Loss for medical image segmentation."</data>
  <data key="d5">chunk-5c1798460d3a200ab1f20bc9df026577</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT-TAGGER (DEVLIN ET AL., 2018)&quot;" target="&quot;DEVLIN&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Devlin is a key author of the BERT-Tagger model from 2018."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT-MRC (LI ET AL., 2019)&quot;" target="&quot;LI&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Li is the lead author of the BERT-MRC model published in 2019."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT-MRC (LI ET AL., 2019)&quot;" target="&quot;ONTONOTES 4.0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The BERT-MRC model is evaluated on the OntoNotes 4.0 dataset as part of the experimental setup."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT-MRC (LI ET AL., 2019)&quot;" target="&quot;MSRA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The BERT-MRC model is tested on the MSRA dataset, particularly in its Chinese form."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT-MRC (LI ET AL., 2019)&quot;" target="&quot;CONLL2003&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The BERT-MRC model is applied to the CoNLL2003 benchmark for English NER evaluation."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;" target="&quot;ZHANG&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Zhang co-developed the Lattice-LSTM model in 2018."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;" target="&quot;YANG&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Yang co-developed the Lattice-LSTM model in 2018."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;" target="&quot;CHINESE MSRA&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Lattice-LSTM is specifically designed for and evaluated on Chinese datasets like Chinese MSRA."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LATTICE-LSTM (ZHANG AND YANG, 2018)&quot;" target="&quot;CHINESE ONTONOTES 4.0&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Lattice-LSTM is applied to the Chinese portion of OntoNotes 4.0."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;GLYCE-BERT (WU ET AL., 2019)&quot;" target="&quot;WU&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Wu is the lead author of the Glyce-BERT model from 2019."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ELMO (PETERS ET AL., 2018)&quot;" target="&quot;PETERS&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Peters is the lead author of the ELMo model from 2018."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CVT (CLARK ET AL., 2018)&quot;" target="&quot;CLARK&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Clark is the lead author of the CVT method from 2018."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES 4.0&quot;" target="&quot;CHINESE ONTONOTES 4.0&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Chinese OntoNotes 4.0 is the Chinese subset of the OntoNotes 4.0 corpus."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MSRA&quot;" target="&quot;CHINESE MSRA&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Chinese MSRA refers to the Chinese-language version of the MSRA dataset, indicating a direct linguistic and contextual relationship."</data>
  <data key="d5">chunk-1cd1762b7eaa0f70fa8c3c315664c813</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MSRA&quot;" target="&quot;TABLE 5&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 5 presents model performance metrics on the MSRA NER dataset."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MSRA&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The proposed method achieves competitive or better results on the MSRA NER task."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MSRA&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The arXiv paper evaluates its NER model on the MSRA Chinese dataset."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL2003&quot;" target="&quot;TABLE 5&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 5 reports NER model performance results on the CoNLL2003 dataset."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL2003&quot;" target="&quot;MA AND HOVY&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The CoNLL2003 experiments followed data processing protocols established by Ma and Hovy (2016)."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONLL2003&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper includes CoNLL2003 as an English NER benchmark."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 5&quot;" target="&quot;ONTONOTES4.0&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 5 shows results for NER models tested on the OntoNotes4.0 dataset."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 5&quot;" target="&quot;ENGLISH CONLL 2003&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 5 presents NER results on the English CoNLL 2003 dataset."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 5&quot;" target="&quot;ENGLISH ONTONOTES5.0&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 5 includes NER performance metrics on the English OntoNotes5.0 dataset."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 6&quot;" target="&quot;SQUAD V1.1&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 6 displays EM and F1 scores for MRC models on the SQuAD v1.1 dataset."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 6&quot;" target="&quot;SQUAD V2.0&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 6 includes evaluation results for MRC models on the SQuAD v2.0 dataset."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 6&quot;" target="&quot;MRC&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 6 presents the experimental results specifically for the Machine Reading Comprehension (MRC) task."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DEVLIN ET AL. (2018)&quot;" target="&quot;BERT&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Devlin et al. (2018) introduced the BERT model, which is used as a baseline in both NER and MRC tasks."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LI ET AL. (2019)&quot;" target="&quot;BERT-MRC&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Li et al. (2019) developed the BERT-MRC model, which formulates NER as a machine reading comprehension task."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;YU ET AL. (2018B)&quot;" target="&quot;QANET&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Yu et al. (2018b) developed the QANet model, whose results are reported in Table 6 for MRC benchmarks."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;YANG ET AL. (2019)&quot;" target="&quot;XLNET&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Yang et al. (2019) proposed the XLNet model, which is evaluated in Table 6 for MRC performance."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;RAJPURKAR ET AL. (2016, 2018)&quot;" target="&quot;SQUAD V1.1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Rajpurkar et al. created the SQuAD v1.1 dataset, which is used in the MRC evaluation."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;RAJPURKAR ET AL. (2016, 2018)&quot;" target="&quot;SQUAD V2.0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Rajpurkar et al. are the authors of the SQuAD v2.0 dataset, included in the MRC experiments."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SEO ET AL. (2016)&quot;" target="&quot;MACHINE READING COMPREHENSION&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Seo et al. (2016) defined standard protocols for the MRC task followed in this study."</data>
  <data key="d5">chunk-cebc63594f08dbcbe32a7d19f944ef73</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES4.0&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The proposed method achieves competitive or better results on the OntoNotes4.0 NER task."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ONTONOTES4.0&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The arXiv paper uses Chinese OntoNotes4.0 for named entity recognition experiments."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD V1.1&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper uses SQuAD v1.1 for machine reading comprehension tasks."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD V2.0&quot;" target="&quot;XLNET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"XLNet is used as a baseline model evaluated on the SQuAD v2.0 dataset in the MRC task."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SQUAD V2.0&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper includes SQuAD v2.0 in its MRC experiments."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;XIAOYA LI&quot;" target="&quot;JINGRONG FENG&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Both are co-authors of the same 2019 paper on a unified MRC framework for NER."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;XIAOYA LI&quot;" target="&quot;SHANNON.AI&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Xiaoya Li is affiliated with Shannon.AI as indicated by the email domain and author footnote."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;XIAOYA LI&quot;" target="&quot;CORR, ABS/1910.11476&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Xiaoya Li is the lead author of the MRC-NER paper posted on CoRR (arXiv)."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;YUXIAN MENG&quot;" target="&quot;JIWEI LI&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"They co-authored two papers together in 2019: one on MRC for NER and another on Dsreg."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;YUXIAN MENG&quot;" target="&quot;SHANNON.AI&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Yuxian Meng is affiliated with Shannon.AI as indicated by the email domain and author footnote."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FEI WU&quot;" target="&quot;ZHEJIANG UNIVERSITY&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Fei Wu is affiliated with the Department of Computer Science and Technology at Zhejiang University, as shown in the author list."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;JIWEI LI&quot;" target="&quot;SHANNON.AI&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Jiwei Li is affiliated with Shannon.AI as indicated by the email domain and author footnote."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TSUNG-YI LIN&quot;" target="&quot;KAIMING HE&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Both are co-authors of the influential 2017 Focal Loss paper."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TSUNG-YI LIN&quot;" target="&quot;PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Tsung-Yi Lin presented the Focal Loss paper at this IEEE conference."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ROSS GIRSHICK&quot;" target="&quot;ROSS B. GIRSHICK&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"These likely refer to the same person, appearing with and without middle initial in different publications."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;KAIMING HE&quot;" target="&quot;SHAOQING REN&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Both co-authored the 2015 Faster R-CNN paper."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;KAIMING HE&quot;" target="&quot;2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Kaiming He presented the ResNet paper at CVPR 2016."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SAMEER PRADHAN&quot;" target="&quot;NIANWEN XUE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"They collaborated on multiple computational linguistics shared tasks, including CoNLL 2011 and OntoNotes 2013."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;PRANAV RAJPURKAR&quot;" target="&quot;PERCY LIANG&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"They co-authored both the SQuAD (2016) and its follow-up on unanswerable questions (2018)."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ROSS B. GIRSHICK&quot;" target="&quot;2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Ross B. Girshick presented the Fast R-CNN paper at ICCV 2015."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ERIK F SANG&quot;" target="&quot;ERIK F. TJONG KIM SANG&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"These are name variants of the same person, co-author of the CoNLL-2003 shared task paper."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MINJOON SEO&quot;" target="&quot;ANIRUDDHA KEMBHAVI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Co-authors on the same 2016 machine comprehension paper."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MINJOON SEO&quot;" target="&quot;ALI FARHADI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Co-authors on the same 2016 machine comprehension paper."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MINJOON SEO&quot;" target="&quot;HANNANEH HAJISHIRZI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Co-authors on the same 2016 machine comprehension paper."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ICCV 2011&quot;" target="&quot;BARCELONA&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"ICCV 2011 was held in Barcelona, Spain."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CVPR 2019&quot;" target="&quot;LONG BEACH&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CVPR 2019 took place in Long Beach, CA, USA."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;EDMONTON&quot;" target="&quot;CONLL 2003&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The CoNLL 2003 workshop was held in Edmonton, Canada."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;EDMONTON&quot;" target="&quot;SEVENTH CONFERENCE ON NATURAL LANGUAGE LEARNING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The conference was held in Edmonton, establishing a clear location-event relationship."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;EDMONTON&quot;" target="&quot;HLT-NAACL 2003&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"HLT-NAACL 2003 was co-located with CoNLL 2003 in Edmonton."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CANADA&quot;" target="&quot;SEVENTH CONFERENCE ON NATURAL LANGUAGE LEARNING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The conference took place in Canada."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CANADA&quot;" target="&quot;BRITISH COLUMBIA&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"British Columbia is a province in Canada."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SOFIA&quot;" target="&quot;PROCEEDINGS OF THE SEVENTEENTH CONFERENCE ON COMPUTATIONAL NATURAL LANGUAGE LEARNING&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The 2013 conference was held in Sofia, Bulgaria."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;PROCEEDINGS OF THE FIFTEENTH CONFERENCE ON COMPUTATIONAL NATURAL LANGUAGE LEARNING: SHARED TASK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"ACL sponsored and published the proceedings of this shared task."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;PROCEEDINGS OF THE SEVENTEENTH CONFERENCE ON COMPUTATIONAL NATURAL LANGUAGE LEARNING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"ACL is the organizing body behind this conference event."</data>
  <data key="d5">chunk-37d30b73fdd9ecef9f5313b837ffe1db</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ASSOCIATION FOR COMPUTATIONAL LINGUISTICS&quot;" target="&quot;PROCEEDINGS OF THE FIFTH SIGHAN WORKSHOP ON CHINESE LANGUAGE PROCESSING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"ACL sponsored the SIGHAN Workshop."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE&quot;" target="&quot;SHAO ET AL., 2017&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Table includes performance results for the model proposed by Shao et al., 2017, specifically 'Joint-POS(Sig)'.</data>
  <data key="d5">chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SHAO ET AL., 2017&quot;" target="&quot;TABLE 3&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Joint-POS models by Shao et al. (2017) are reported in Table 3 as part of the experimental results for Chinese POS datasets."</data>
  <data key="d5">chunk-aa4690473ce6c28f84935bd046522165</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SHAO ET AL., 2017&quot;" target="&quot;JOINT-POS&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Shao et al. (2017) are the creators of the Joint-POS model."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 7&quot;" target="&quot;PI&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 7 presents the experimental results specifically for the Paraphrase Identification (PI) task."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;MRPC&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT is evaluated on the MRPC dataset as part of the PI task experiments."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;QQP&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"BERT is evaluated on the QQP dataset as part of the PI task experiments."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;DEVLIN ET AL., 2018&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Devlin et al., 2018 are the creators of the BERT model cited in the text."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BERT&quot;" target="&quot;TABLE 8&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"BERT and its variants (e.g., BERT+FL, BERT+DL) are evaluated in Table 8 using different data augmentation methods on the QQP dataset."</data>
  <data key="d5">chunk-a3a8758b1de02b56fa79cb146de63691</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;XLNET&quot;" target="&quot;SQUADV1.1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"XLNet is used as a baseline model evaluated on the SQuADv1.1 dataset in the MRC task."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;XLNET&quot;" target="&quot;YANG ET AL., 2019&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Yang et al., 2019 are the creators of the XLNet model cited in the text."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MRPC&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper conducts paraphrase identification experiments using MRPC."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QQP&quot;" target="&quot;DBPEDIA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"DBpedia is used to augment the QQP dataset by replacing entity mentions during synthetic data creation."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QQP&quot;" target="&quot;SPACY&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Spacy is used to extract entity mentions from the QQP dataset for data augmentation purposes."</data>
  <data key="d5">chunk-14fe1936981bd0314ca6d73d5425c982</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QQP&quot;" target="&quot;TABLE 8&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Table 8 specifically reports results for data augmentation experiments conducted on the QQP dataset."</data>
  <data key="d5">chunk-a3a8758b1de02b56fa79cb146de63691</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QQP&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper uses QQP as a paraphrase identification benchmark."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DEVLIN ET AL., 2018&quot;" target="&quot;TABLE 3&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The BERT-Tagger by Devlin et al. (2018) appears in Table 3 as a baseline model for Chinese POS tagging experiments."</data>
  <data key="d5">chunk-aa4690473ce6c28f84935bd046522165</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DEVLIN ET AL., 2018&quot;" target="&quot;TABLE 4&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The BERT-Tagger and its variants by Devlin et al. (2018) are central to the results presented in Table 4 for English POS datasets."</data>
  <data key="d5">chunk-aa4690473ce6c28f84935bd046522165</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DEVLIN ET AL., 2018&quot;" target="&quot;BERT-TAGGER&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Devlin et al. (2018) introduced the BERT-Tagger baseline used throughout the experiments."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DBPEDIA&quot;" target="&quot;SPACY1&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Spacy1 is used to retrieve entity mentions and link them to corresponding entities in DBpedia for data augmentation."</data>
  <data key="d5">chunk-a3a8758b1de02b56fa79cb146de63691</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;XIAOFEI SUN&quot;" target="&quot;SHANNON.AI&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Xiaofei Sun is affiliated with Shannon.AI as indicated by the email domain and author footnote."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;JUNJUN LIANG&quot;" target="&quot;SHANNON.AI&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Junjun Liang is affiliated with Shannon.AI as indicated by the email domain and author footnote."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ZHEJIANG UNIVERSITY&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Zhejiang University contributed to the research through Fei Wu’s involvement in the paper."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SHANNON.AI&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Shannon.AI is the primary research organization behind the development and publication of the proposed dice loss methodology."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CTB5&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The paper reports achieving state-of-the-art results on CTB5 using the proposed dice loss."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CTB5&quot;" target="&quot;XINHUA&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"CTB5 includes 698 articles sourced from Xinhua news agency."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CTB5&quot;" target="&quot;INFORMATION SERVICES DEPARTMENT OF HKSAR&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"CTB5 contains 55 articles from the Information Services Department of HKSAR."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CTB5&quot;" target="&quot;SINORAMA MAGAZINE&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"CTB5 incorporates 132 articles from Sinorama Magazine."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CTB5&quot;" target="&quot;CTB6&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"CTB6 is an extension of the CTB5 dataset, indicating a direct lineage and expansion."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CTB5&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper uses CTB5 as one of its benchmark datasets for linguistic experiments."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CTB6&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The paper reports achieving state-of-the-art results on CTB6 using the proposed dice loss."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CTB6&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper includes CTB6 in its experimental evaluation."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;UD1.4&quot;" target="&quot;DICE LOSS FOR DATA-IMBALANCED NLP TASKS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The paper reports achieving state-of-the-art results on UD1.4 using the proposed dice loss."</data>
  <data key="d5">chunk-bbb1108944d391ef267e2d5021e6fafd</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;UD1.4&quot;" target="&quot;UNIVERSAL DEPENDENCIES&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"UD1.4 is a specific version of the Universal Dependencies framework used for Chinese POS tagging."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;UD1.4&quot;" target="&quot;ARXIV:1805.02023&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The arXiv paper utilizes UD1.4 for Chinese part-of-speech tagging tasks."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;UD1.4&quot;" target="&quot;TABLE 3&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 3 includes performance results on the UD1.4 dataset for Chinese POS tagging."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;YELONG SHEN&quot;" target="&quot;PO-SEN HUANG&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Co-authors on the 2017 Reasonet paper presented at KDD."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;YELONG SHEN&quot;" target="&quot;23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Yelong Shen et al. presented their paper at this conference."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING&quot;" target="&quot;ACM&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"ACM organized and published proceedings of the KDD conference."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DEEP LEARNING IN MEDICAL IMAGE ANALYSIS AND MULTIMODAL LEARNING FOR CLINICAL DECISION SUPPORT&quot;" target="&quot;MICCAI 2017&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The DLMIA 2017 workshop was held in conjunction with MICCAI 2017."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DEEP LEARNING IN MEDICAL IMAGE ANALYSIS AND MULTIMODAL LEARNING FOR CLINICAL DECISION SUPPORT&quot;" target="&quot;QUEBEC CITY&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The workshop was held in Quebec City."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;NAIWEN XUE&quot;" target="&quot;PENN CHINESE TREEBANK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Naiwen Xue is the lead author of the paper describing the Penn Chinese Treebank."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;NAIWEN XUE&quot;" target="&quot;CHINESE TREEBANK 5.0/6.0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Chinese Treebank datasets are associated with the work of Naiwen Xue and colleagues."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;QUOC V. LE&quot;" target="&quot;PROCEEDINGS OF THE 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Quoc V. Le co-authored a paper presented at this conference."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ADAMS WEI YU&quot;" target="&quot;6TH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Adams Wei Yu et al. presented QANet at ICLR 2018."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MINH-THANG LUONG&quot;" target="&quot;PROCEEDINGS OF THE 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Minh-Thang Luong co-authored a paper presented at this conference."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;6TH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS&quot;" target="&quot;VANCOUVER&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"ICLR 2018 was held in Vancouver."</data>
  <data key="d5">chunk-ce2ef87d827aee2de8ab2cb4492541c8</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;VANCOUVER&quot;" target="&quot;ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 23&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"NeurIPS 2010 was held in Vancouver."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;VANCOUVER&quot;" target="&quot;BRITISH COLUMBIA&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Vancouver is located in British Columbia."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 3&quot;" target="&quot;ZHANG AND YANG, 2018&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Lattice-LSTM model by Zhang and Yang (2018) is included in Table 3’s results for Chinese POS tagging."</data>
  <data key="d5">chunk-aa4690473ce6c28f84935bd046522165</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 3&quot;" target="&quot;CHINESE TREEBANK&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Table 3 reports experimental results specifically on the Chinese Treebank datasets (CTB5 and CTB6)."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 4&quot;" target="&quot;BOHNET ET AL., 2018&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The Meta BiLSTM model by Bohnet et al. (2018) is listed in Table 4 under results for the English WSJ dataset."</data>
  <data key="d5">chunk-aa4690473ce6c28f84935bd046522165</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 4&quot;" target="&quot;GODIN, 2019&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The FastText+CNN+CRF model by Godin (2019) is referenced in Table 4 in the context of English Tweets POS tagging results."</data>
  <data key="d5">chunk-aa4690473ce6c28f84935bd046522165</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 4&quot;" target="&quot;WALL STREET JOURNAL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Table 4 contains results for English POS tagging, including on the Wall Street Journal dataset."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;TABLE 4&quot;" target="&quot;RITTER ET AL. (2011) DATASET&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Table 4 includes evaluation results on the Ritter et al. (2011) English POS dataset."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ZHANG AND YANG, 2018&quot;" target="&quot;LATTICE-LSTM&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Zhang and Yang (2018) developed the Lattice-LSTM architecture."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;STANFORD SENTIMENT TREEBANK (SST)&quot;" target="&quot;SST-2&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"SST-2 is a subset of the Stanford Sentiment Treebank used for binary sentiment classification."</data>
  <data key="d5">chunk-a3a8758b1de02b56fa79cb146de63691</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;STANFORD SENTIMENT TREEBANK (SST)&quot;" target="&quot;SST-5&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"SST-5 is a subset of the Stanford Sentiment Treebank used for fine-grained five-class sentiment classification."</data>
  <data key="d5">chunk-a3a8758b1de02b56fa79cb146de63691</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MRC&quot;" target="&quot;DATA RESAMPLING&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"MRC suffers from severe data imbalance, which motivates the application of data resampling techniques to improve model performance."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MRC&quot;" target="&quot;SØRENSEN–DICE COEFFICIENT&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The Sørensen–Dice coefficient is proposed specifically to address the training-test discrepancy in MRC caused by extreme label imbalance."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;MRC&quot;" target="&quot;SECTION 4&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Section 4 presents experimental results on MRC and other NLP tasks using the proposed methods."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SØRENSEN–DICE COEFFICIENT&quot;" target="&quot;SORENSEN&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Sorensen (1948) originally introduced the Sørensen–Dice coefficient, forming the theoretical basis for dice loss."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SØRENSEN–DICE COEFFICIENT&quot;" target="&quot;SECTION 3&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Section 3 details the use of Sørensen–Dice coefficient as a proposed loss function."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DATA RESAMPLING&quot;" target="&quot;KAHN AND MARSHALL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Kahn and Marshall (1953) laid early groundwork for data resampling via importance sampling."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DATA RESAMPLING&quot;" target="&quot;CHAWLA ET AL.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Chawla et al. (2002) contributed oversampling methods to data resampling for class imbalance."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DATA RESAMPLING&quot;" target="&quot;MALISIEWICZ ET AL.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Malisiewicz et al. (2011) advanced data resampling through hard example mining."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DATA RESAMPLING&quot;" target="&quot;SECTION 2&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Section 2 reviews data resampling as a key category of related work for handling imbalance."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SELF-PACED LEARNING&quot;" target="&quot;KUMAR ET AL.&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Kumar et al. (2010) introduced self-paced learning as a dynamic data resampling strategy."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;OBJECT DETECTION&quot;" target="&quot;GIRSHICK&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Girshick (2015) conducted influential work on object detection where label imbalance is a core challenge."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;OBJECT DETECTION&quot;" target="&quot;HE ET AL.&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"He et al. (2015) addressed data imbalance in object detection, contributing to the field’s understanding of the issue."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;OBJECT DETECTION&quot;" target="&quot;REN ET AL.&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"Ren et al. are associated with object detection research that tackles background-object imbalance."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;OBJECT DETECTION&quot;" target="&quot;SECTION 2&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Section 2 discusses object detection as a domain where data imbalance has been extensively studied."</data>
  <data key="d5">chunk-76ac26952e67515817b1d024e77de8e0</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;WU ET AL.&quot;" target="&quot;CHINESE ONTONOTES4.0&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The data split for Chinese OntoNotes4.0 follows the approach used by Wu et al. (2019)."</data>
  <data key="d5">chunk-a8b34fbfcac06ce762af2dfc253ccd08</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LI ET AL., 2019&quot;" target="&quot;BERT-MRC&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Li et al. (2019) proposed the BERT-MRC framework adapted for NER."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;PETERS ET AL., 2018&quot;" target="&quot;ELMO&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Peters et al. (2018) developed the ELMo model used as a baseline."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CLARK ET AL., 2018&quot;" target="&quot;CVT&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Clark et al. (2018) introduced the CVT method used as a baseline."</data>
  <data key="d5">chunk-a2add12ad84f03be40432d1e511e52e2</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;KEVIN CLARK&quot;" target="&quot;PROCEEDINGS OF THE 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Kevin Clark co-authored a paper presented at this conference."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CHRISTOPHER D. MANNING&quot;" target="&quot;PROCEEDINGS OF THE 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Christopher D. Manning co-authored a paper presented at this conference."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;PROCEEDINGS OF THE 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING&quot;" target="&quot;BRUSSELS&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The conference was held in Brussels."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BRUSSELS&quot;" target="&quot;BELGIUM&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Brussels is the capital city of Belgium."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;BELGIUM&quot;" target="&quot;GHENT UNIVERSITY&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Ghent University is located in Belgium."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;PRADEEP DASIGI&quot;" target="&quot;ARXIV PREPRINT ARXIV:1908.05803&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Pradeep Dasigi is the lead author of the Quoref dataset paper posted on arXiv."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;JACOB DEVLIN&quot;" target="&quot;ARXIV PREPRINT ARXIV:1810.04805&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Jacob Devlin is the lead author of the BERT paper posted on arXiv."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LU JIANG&quot;" target="&quot;ICML&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Lu Jiang presented the Mentornet paper at ICML."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;ICML&quot;" target="&quot;ANGELOS KATHAROPOULOS&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Angelos Katharopoulos presented the importance sampling paper at ICML."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;OLDRICH KODYM&quot;" target="&quot;PATTERN RECOGNITION - 40TH GERMAN CONFERENCE, GCPR 2018&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Oldrich Kodym presented a paper at GCPR 2018."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;PATTERN RECOGNITION - 40TH GERMAN CONFERENCE, GCPR 2018&quot;" target="&quot;STUTTGART&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The GCPR 2018 conference was held in Stuttgart."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;STUTTGART&quot;" target="&quot;GERMANY&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Stuttgart is a city in Germany."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;M. PAWAN KUMAR&quot;" target="&quot;ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 23&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"M. Pawan Kumar presented a paper at NeurIPS 2010."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;GUILLAUME LAMPLE&quot;" target="&quot;ARXIV PREPRINT ARXIV:1603.01360&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Guillaume Lample is the lead author of the NER architecture paper on arXiv."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;GINA-ANNE LEVOW&quot;" target="&quot;PROCEEDINGS OF THE FIFTH SIGHAN WORKSHOP ON CHINESE LANGUAGE PROCESSING&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Gina-Anne Levow presented a paper at the Fifth SIGHAN Workshop."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;PROCEEDINGS OF THE FIFTH SIGHAN WORKSHOP ON CHINESE LANGUAGE PROCESSING&quot;" target="&quot;SYDNEY&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"The workshop was held in Sydney."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SYDNEY&quot;" target="&quot;AUSTRALIA&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Sydney is a city in Australia."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;H. LI&quot;" target="&quot;2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"H. Li presented a face detection paper at CVPR 2015."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;FREDERIC GODIN&quot;" target="&quot;GHENT UNIVERSITY&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Frederic Godin completed his Ph.D. at Ghent University."</data>
  <data key="d5">chunk-6e391a0a4509f4436d350797fb934357</data>
  <data key="d6">1</data>
</edge>
</graph></graphml>