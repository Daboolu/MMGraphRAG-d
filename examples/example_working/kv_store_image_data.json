{
  "image_1": {
    "image_id": 1,
    "image_path": "examples/example_working/images/image_1.jpg",
    "caption": [
      "Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks. "
    ],
    "footnote": [],
    "context": "With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP.  1 Introduction  Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class $O$ . Specifically, the number of tokens with tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is more severe for MRC tasks (Rajpurkar et al., 2016; Nguyen et al., 2016; Rajpurkar et al., 2018; Kocisk ˇ y et al. \\` , 2018; Dasigi et al., 2019) with the value of negative-positive ratio being 50-200, which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context, and given a chunk of text of an arbitrary length, only two tokens are positive (or of interest) with all the rest being background. ",
    "chunk_order_index": 0,
    "chunk_id": "chunk-bbb1108944d391ef267e2d5021e6fafd",
    "description": "The image is a table labeled 'Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.' The table presents statistical information about the number of negative (# neg), positive (# pos) examples, and their respective ratios for various natural language processing (NLP) tasks. It consists of five rows and four columns. The first column lists the tasks: CoNLL03 NER, OntoNotes5.0 NER, SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and QUOREF (Dasigi et al., 2019). The second column shows the number of negative examples: 170K for CoNLL03 NER, 1.96M for OntoNotes5.0 NER, 10.3M for SQuAD 1.1, 15.4M for SQuAD 2.0, and 6.52M for QUOREF. The third column displays the number of positive examples: 34K for CoNLL03 NER, 239K for OntoNotes5.0 NER, 175K for SQuAD 1.1, 188K for SQuAD 2.0, and 38.6K for QUOREF. The fourth column provides the ratio of negative to positive examples: 4.98 for CoNLL03 NER, 8.18 for OntoNotes5.0 NER, 55.9 for SQuAD 1.1, 82.0 for SQuAD 2.0, and 169 for QUOREF. The table highlights the increasing severity of data imbalance across these tasks, particularly in machine reading comprehension (MRC) tasks like SQuAD and QUOREF, where the ratio exceeds 50 and reaches as high as 169. This extreme imbalance reflects the nature of MRC tasks, where only a few tokens are labeled as positive (e.g., start or end of an answer span), while the vast majority are background (negative). The context emphasizes that such imbalance poses challenges in training models and motivates the use of specialized loss functions like Dice loss to improve performance.",
    "segmentation": false
  },
  "image_2": {
    "image_id": 2,
    "image_path": "examples/example_working/images/image_2.jpg",
    "caption": [],
    "footnote": [],
    "context": "For illustration purposes, we use the binary classification task to demonstrate how different losses work. The mechanism can be easily extended to multi-class classification. Let $X$ denote a set of training instances and each instance $x _ { i } \\in X$ is associated with a golden binary label $y _ { i } = [ y _ { i 0 } , y _ { i 1 } ]$ denoting the ground-truth class $x _ { i }$ belongs to, and $p _ { i } = [ p _ { i 0 } , p _ { i 1 } ]$ is the predicted probabilities of the two classes respectively, where $y _ { i 0 } , y _ { i 1 } \\in$ $\\{ 0 , 1 \\} , p _ { i 0 } , p _ { i 1 } \\in [ 0 , 1 ]$ and $p _ { i 1 } + p _ { i 0 } = 1$ .  3.2 Cross Entropy Loss  The vanilla cross entropy (CE) loss is given by:  As can be seen from Eq.1, each $x _ { i }$ contributes equally to the final objective. Two strategies are normally used to address the the case where we wish that not all $x _ { i }$ are treated equally: associating different classes with different weighting factor $\\alpha$ or resampling the datasets. For the former, Eq.1 is adjusted as follows: ",
    "chunk_order_index": 2,
    "chunk_id": "chunk-5c1798460d3a200ab1f20bc9df026577",
    "description": "The image displays a mathematical formula representing the cross-entropy (CE) loss function used in binary classification tasks. The formula is written in LaTeX-style notation and reads: CE = -\\frac{1}{N} \\sum_{i} \\sum_{j \\in \\{0,1\\}} y_{ij} \\log p_{ij}. This equation computes the average cross-entropy loss over N training instances. For each instance i, the sum is taken over the two possible class labels j ∈ {0,1}, where y_ij represents the true label (either 0 or 1) and p_ij denotes the predicted probability for class j. The logarithm of the predicted probability is weighted by the true label, ensuring that only the log-probability of the correct class contributes to the loss. The negative sign ensures the loss is positive, and dividing by N normalizes the total loss across all samples. The context explains that this formulation applies to binary classification with one-hot encoded labels and can be extended to multi-class settings. It also mentions that each training instance contributes equally to the final objective, and strategies like class weighting or resampling are used when unequal treatment of instances is desired.",
    "segmentation": false
  },
  "image_3": {
    "image_id": 3,
    "image_path": "examples/example_working/images/image_3.jpg",
    "caption": [],
    "footnote": [],
    "context": "As can be seen from Eq.1, each $x _ { i }$ contributes equally to the final objective. Two strategies are normally used to address the the case where we wish that not all $x _ { i }$ are treated equally: associating different classes with different weighting factor $\\alpha$ or resampling the datasets. For the former, Eq.1 is adjusted as follows:  where $\\alpha _ { i } \\in [ 0 , 1 ]$ may be set by the inverse class frequency or treated as a hyperparameter to set by cross validation. In this work, we use $\\textstyle \\log ( { \\frac { n - n _ { t } } { n _ { t } } } + K )$ to calculate the coefficient $\\alpha$ , where $n _ { t }$ is the number of samples with class $t$ and $n$ is the total number of samples in the training set. $K$ is a hyperparameter to tune. Intuitively, this equation assigns less weight to the majority class and more weight to the minority class. The data resampling strategy constructs a new dataset by sampling training examples from the original dataset based on human-designed criteria, e.g. extracting equal training samples from each class. Both strategies are equivalent to changing the data distribution during training and thus are of the same nature. Empirically, these two methods are not widely used due to the trickiness of selecting $\\alpha$ especially for multi-class classification tasks and that inappropriate selection can easily bias towards rare classes (Valverde et al., 2017). ",
    "chunk_order_index": 2,
    "chunk_id": "chunk-5c1798460d3a200ab1f20bc9df026577",
    "description": "The image displays a mathematical equation labeled as 'CE' (Cross-Entropy), which is commonly used in machine learning for classification tasks, particularly in the context of multi-class or binary classification with probabilistic outputs. The equation is written in LaTeX-style mathematical notation and reads: \n\n\\[ \\mathrm{CE} = -\\frac{1}{N} \\sum_{i} \\alpha_i \\sum_{j \\in \\{0,1\\}} y_{ij} \\log p_{ij} \\]\n\nThis formula represents a weighted cross-entropy loss function. Here, \\( N \\) denotes the total number of samples in the dataset. The outer summation is over individual samples \\( i \\), and each sample contributes to the loss scaled by a weight factor \\( \\alpha_i \\), where \\( \\alpha_i \\in [0,1] \\). This weighting allows for adjusting the importance of different samples, typically to address class imbalance. The inner summation is over the possible class labels \\( j \\in \\{0,1\\} \\), indicating a binary classification setting. For each sample \\( i \\) and class \\( j \\), \\( y_{ij} \\) is the true label (usually 0 or 1 in one-hot encoding), and \\( p_{ij} \\) is the predicted probability assigned by the model to class \\( j \\) for sample \\( i \\). The logarithmic term \\( \\log p_{ij} \\) penalizes low-confidence correct predictions more heavily. The negative sign ensures that minimizing the CE loss corresponds to maximizing the likelihood of correct predictions. The context provided explains that this formulation allows for unequal treatment of samples via \\( \\alpha_i \\), which can be set based on inverse class frequency or tuned via hyperparameters such as \\( K \\) in the expression \\( \\log \\left( \\frac{n - n_t}{n_t} + K \\right) \\), where \\( n_t \\) is the count of samples in class \\( t \\) and \\( n \\) is the total number of training samples. This adjustment reduces the influence of majority classes and increases that of minority classes, helping mitigate bias in imbalanced datasets.",
    "segmentation": false
  },
  "image_4": {
    "image_id": 4,
    "image_path": "examples/example_working/images/image_4.jpg",
    "caption": [],
    "footnote": [],
    "context": "Sørensen–Dice coefficient (Sorensen, 1948; Dice, 1945), dice coefficient (DSC) for short, is an F1- oriented statistic used to gauge the similarity of two sets. Given two sets $A$ and $B$ , the vanilla dice coefficient between them is given as follows:  In our case, $A$ is the set that contains all positive examples predicted by a specific model, and $B$ is the set of all golden positive examples in the dataset. When applied to boolean data with the definition of true positive (TP), false positive (FP), and false negative (FN), it can be then written as follows: ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-bff777a634a5af0182c5787f1022b29f",
    "description": "The image displays a mathematical formula representing the Sørensen–Dice coefficient (DSC), a statistical measure used to quantify the similarity between two sets. The formula is written in LaTeX-style mathematical notation and reads: DSC(A, B) = 2|A ∩ B| / (|A| + |B|). Here, A and B are sets, |A ∩ B| denotes the cardinality (number of elements) in the intersection of sets A and B, and |A| and |B| represent the cardinalities of sets A and B, respectively. The coefficient ranges from 0 to 1, where 1 indicates perfect overlap between the two sets and 0 indicates no overlap. In the context provided, set A corresponds to the set of positive examples predicted by a model, and set B corresponds to the set of true positive examples in the dataset (ground truth). This formulation is commonly used in evaluation metrics for segmentation tasks, particularly in medical imaging or natural language processing, where it serves as an F1-oriented metric. The expression is presented in a clean, centered format using standard mathematical typography, with clear use of absolute value bars, set notation, and division symbols.",
    "segmentation": false
  },
  "image_5": {
    "image_id": 5,
    "image_path": "examples/example_working/images/image_5.jpg",
    "caption": [],
    "footnote": [],
    "context": "In our case, $A$ is the set that contains all positive examples predicted by a specific model, and $B$ is the set of all golden positive examples in the dataset. When applied to boolean data with the definition of true positive (TP), false positive (FP), and false negative (FN), it can be then written as follows:  For an individual example $x _ { i }$ , its corresponding dice coefficient is given as follows:  $$\n\\mathrm { D S C } ( x _ { i } ) = \\frac { 2 p _ { i 1 } y _ { i 1 } } { p _ { i 1 } + y _ { i 1 } }\n$$",
    "chunk_order_index": 3,
    "chunk_id": "chunk-bff777a634a5af0182c5787f1022b29f",
    "description": "The image displays a mathematical formula for the Dice Similarity Coefficient (DSC), commonly used in evaluating the performance of binary classification models, particularly in image segmentation tasks. The equation is presented in three equivalent forms. The first form expresses DSC as: DSC = (2TP) / (2TP + FN + FP), where TP denotes true positives, FN denotes false negatives, and FP denotes false positives. The second form rewrites this using precision (Pre) and recall (Rec): DSC = (2 × Pre × Rec) / (Pre + Rec). The third form shows that DSC is mathematically equivalent to the F1 score: DSC = F1. The formula is derived from set theory, with A representing predicted positive examples and B representing actual (golden) positive examples. The context explains that for a single example xi, the Dice coefficient can be written as DSC(xi) = (2pi1yi1) / (pi1 + yi1), where pi1 and yi1 are binary indicators for prediction and ground truth, respectively. The image contains no graphical elements, colors, or visual objects beyond the black text on a white background, formatted in standard mathematical notation.",
    "segmentation": false
  },
  "image_6": {
    "image_id": 6,
    "image_path": "examples/example_working/images/image_6.jpg",
    "caption": [],
    "footnote": [],
    "context": "$$\n{ \\begin{array} { r l } & { { \\mathrm { D S C } } = { \\frac { \\mathrm { 2 T P } } { \\mathrm { 2 T P } + { \\mathrm { F N } } + { \\mathrm { F P } } } } = { \\frac { \\mathrm { 2 } { \\frac { \\mathrm { T P } } { \\mathrm { T P } + { \\mathrm { F N } } } } { \\frac { \\mathrm { T P } } { \\mathrm { T P } + { \\mathrm { F P } } } } } { { \\frac { \\mathrm { T P } } { \\mathrm { T P } + { \\mathrm { F N } } } } + { \\frac { \\mathrm { T P } } { \\mathrm { T P } + { \\mathrm { F P } } } } } } } \\\\ & { \\qquad = { \\frac { \\mathrm { 2 P r e ~ \\times ~ R e c } } { \\mathrm { P r e + R e c } } } = F 1 } \\end{array} }\n$$ For an individual example $x _ { i }$ , its corresponding dice coefficient is given as follows:  As can be seen, a negative example $( y _ { i 1 } = 0$ ) does not contribute to the objective. For smoothing purposes, it is common to add a $\\gamma$ factor to both the nominator and the denominator, making the form to be as follows (we simply set $\\gamma = 1$ in the rest of ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-bff777a634a5af0182c5787f1022b29f",
    "description": "The image displays a mathematical formula for the Dice Similarity Coefficient (DSC) applied to an individual example $x_i$. The equation is presented in a clean, typeset format typical of academic or technical documents. The formula is written as: $$ \\mathrm{DSC}(x_i) = \\frac{2p_{i1}y_{i1}}{p_{i1} + y_{i1}} $$ where $p_{i1}$ represents the predicted probability of class 1 for example $x_i$, and $y_{i1}$ is the true label (0 or 1) for that example. This formulation indicates that the DSC is computed based on the product of the predicted and actual values, scaled by twice the numerator divided by their sum. The context explains that when $y_{i1} = 0$ (a negative example), the term does not contribute to the objective function, meaning only positive examples influence the loss. For numerical stability, a smoothing factor $\\gamma$ is typically added to both the numerator and denominator, though it is set to 1 in the described case. The surrounding text provides additional insight into the use of this metric in machine learning, particularly in binary classification tasks where precision and recall are balanced via the DSC, which is mathematically equivalent to the F1-score in certain contexts.",
    "segmentation": false
  },
  "image_7": {
    "image_id": 7,
    "image_path": "examples/example_working/images/image_7.jpg",
    "caption": [],
    "footnote": [],
    "context": "As can be seen, a negative example $( y _ { i 1 } = 0$ ) does not contribute to the objective. For smoothing purposes, it is common to add a $\\gamma$ factor to both the nominator and the denominator, making the form to be as follows (we simply set $\\gamma = 1$ in the rest of  467  Table 2: Different losses and their formulas. We add $+ 1$ to DL, TL and DSC so that they are positive. ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-bff777a634a5af0182c5787f1022b29f",
    "description": "The image is a table labeled 'Table 2: Different losses and their formulas.' It presents a list of six different loss functions used in machine learning, particularly in classification tasks, along with their corresponding mathematical formulas for a single sample \\( x_i \\). The table has two columns: the first column lists the abbreviations for the loss functions (CE, WCE, DL, TL, DSC, FL), and the second column provides the respective formulas. Each formula involves variables such as \\( y_{ij} \\) (true label), \\( p_{ij} \\) (predicted probability), \\( \\alpha \\), \\( \\beta \\), and \\( \\gamma \\) (hyperparameters or smoothing factors). Specific details include:\n\n- **CE (Cross-Entropy)**: \\( -\\sum_{j\\in\\{0,1\\}} y_{ij} \\log p_{ij} \\)\n- **WCE (Weighted Cross-Entropy)**: \\( -\\alpha_i \\sum_{j\\in\\{0,1\\}} y_{ij} \\log p_{ij} \\)\n- **DL (Dice Loss)**: \\( 1 - \\frac{2p_{i1}y_{i1} + \\gamma}{p_{i1}^2 + y_{i1}^2 + \\gamma} \\)\n- **TL (Tversky Loss)**: \\( 1 - \\frac{p_{i1}y_{i1} + \\gamma}{p_{i1}y_{i1} + \\alpha p_{i1}y_{i0} + \\beta p_{i0}y_{i1} + \\gamma} \\)\n- **DSC (Dice Similarity Coefficient Loss)**: \\( 1 - \\frac{2(1-p_{i1})p_{i1}y_{i1} + \\gamma}{(1-p_{i1})p_{i1} + y_{i1} + \\gamma} \\)\n- **FL (Focal Loss)**: \\( -\\alpha_i \\sum_{j\\in\\{0,1\\}} (1 - p_{ij})^\\gamma \\log p_{ij} \\)\n\nThe context notes that a negative example (\\( y_{i1} = 0 \\)) does not contribute to the objective function. For numerical stability, a smoothing factor \\( \\gamma \\) is added to both numerator and denominator in some formulas; it is set to 1 in subsequent discussions. Additionally, a '+1' is added to DL, TL, and DSC to ensure they remain positive. The table is formatted in black text on a white background, using standard mathematical notation, and is aligned in a clean, structured layout suitable for academic documentation.",
    "segmentation": false
  },
  "image_8": {
    "image_id": 8,
    "image_path": "examples/example_working/images/image_8.jpg",
    "caption": [],
    "footnote": [],
    "context": "Table 2: Different losses and their formulas. We add $+ 1$ to DL, TL and DSC so that they are positive.  this paper):  As can be seen, negative examples whose DSC is $\\frac { \\gamma } { p _ { i 1 } + \\gamma }$ , also contribute to the training. Additionally, Milletari et al. (2016) proposed to change the denominator to the square form for faster convergence, which leads to the following dice loss (DL): ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-bff777a634a5af0182c5787f1022b29f",
    "description": "The image displays a mathematical formula representing the Dice Similarity Coefficient (DSC) for a given sample \\( x_i \\). The formula is written in LaTeX-style mathematical notation and reads: \n\n\\[ \\text{DSC}(x_i) = \\frac{2p_{i1}y_{i1} + \\gamma}{p_{i1} + y_{i1} + \\gamma} \\]\n\nHere, \\( p_{i1} \\) represents the predicted probability of the positive class for sample \\( x_i \\), \\( y_{i1} \\) is the true label (typically binary, with 1 indicating the positive class), and \\( \\gamma \\) is a small smoothing constant added to prevent division by zero and improve numerical stability. This formulation is commonly used in medical image segmentation tasks where class imbalance is prevalent. The numerator includes twice the product of the prediction and ground truth, emphasizing agreement between them, while the denominator sums all relevant values, ensuring normalization. The context provided indicates that this DSC is part of a broader discussion on loss functions, particularly in relation to Dice Loss (DL), and notes that adding a constant (like +1) ensures positivity in certain loss formulations. Additionally, it references Milletari et al. (2016), who proposed modifying the denominator to a squared form for faster convergence, though this specific variant uses a linear denominator. The image contains no visual elements beyond the mathematical expression; it is purely symbolic and analytical.",
    "segmentation": false
  },
  "image_9": {
    "image_id": 9,
    "image_path": "examples/example_working/images/image_9.jpg",
    "caption": [],
    "footnote": [],
    "context": "As can be seen, negative examples whose DSC is $\\frac { \\gamma } { p _ { i 1 } + \\gamma }$ , also contribute to the training. Additionally, Milletari et al. (2016) proposed to change the denominator to the square form for faster convergence, which leads to the following dice loss (DL):  Another version of $\\mathrm { D L }$ is to directly compute setlevel dice coefficient instead of the sum of individual dice coefficient, which is easier for optimization: ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-bff777a634a5af0182c5787f1022b29f",
    "description": "The image displays a mathematical formula representing the Dice Loss (DL) used in machine learning, particularly in segmentation tasks. The formula is written in LaTeX-style mathematical notation and reads: \n\n\\[ \\mathrm{DL} = \\frac{1}{N} \\sum_{i} \\left[ 1 - \\frac{2p_{i1}y_{i1} + \\gamma}{p_{i1}^2 + y_{i1}^2 + \\gamma} \\right] \\]\n\nThis equation computes the average loss across N samples, where each term in the summation corresponds to a single sample i. The variables involved are:\n- \\( p_{i1} \\): predicted probability for class 1 of sample i,\n- \\( y_{i1} \\): true label (binary) for class 1 of sample i,\n- \\( \\gamma \\): a small positive constant (smoothing factor) added to avoid division by zero and stabilize training.\n\nThe numerator contains the product of predictions and labels scaled by 2, plus the smoothing term \\( \\gamma \\), while the denominator uses the sum of squares of both prediction and label values, also including \\( \\gamma \\). This formulation is a modified version of the standard Dice coefficient loss, designed to improve convergence speed by using squared terms in the denominator, as suggested by Milletari et al. (2016). The context indicates that this form of DL allows negative examples (where \\( y_{i1} = 0 \\)) to contribute meaningfully to training, with their DSC contribution being \\( \\frac{\\gamma}{p_{i1} + \\gamma} \\). The overall structure of the formula emphasizes its use in optimizing models for binary segmentation tasks, such as medical image analysis or object detection, where balanced performance between foreground and background is critical.",
    "segmentation": false
  },
  "image_10": {
    "image_id": 10,
    "image_path": "examples/example_working/images/image_10.jpg",
    "caption": [],
    "footnote": [],
    "context": "Another version of $\\mathrm { D L }$ is to directly compute setlevel dice coefficient instead of the sum of individual dice coefficient, which is easier for optimization:  Tversky index (TI), which can be thought as the approximation of the $F _ { \\beta }$ score, extends dice coefficient to a more general case. Given two sets $A$ and $B$ , tversky index is computed as follows: ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-bff777a634a5af0182c5787f1022b29f",
    "description": "The image displays a mathematical formula representing the Dice Loss (DL) in the context of set-level computation, commonly used in machine learning tasks such as segmentation or classification. The formula is written in LaTeX-style mathematical notation and reads: \n\n$$\n\\mathrm{DL} = 1 - \\frac{2 \\sum_i p_{i1} y_{i1} + \\gamma}{\\sum_i p_{i1}^2 + \\sum_i y_{i1}^2 + \\gamma}\n$$\n\nHere, $\\mathrm{DL}$ denotes the Dice Loss, which measures the dissimilarity between predicted probabilities $p_{i1}$ and true labels $y_{i1}$ across all instances $i$. The numerator includes twice the sum of element-wise products of predictions and labels, regularized by a small constant $\\gamma$ to prevent division by zero. The denominator consists of the sum of squared predictions and squared true labels, also regularized by $\\gamma$. This formulation is designed to improve optimization stability by computing a set-level Dice coefficient rather than summing individual ones. The context indicates that this version of DL is related to the Tversky index (TI), a generalization of the Dice coefficient that approximates the $F_\\beta$ score. The equation is presented in clean, black serif font on a white background, typical of academic or technical documentation.",
    "segmentation": false
  },
  "image_11": {
    "image_id": 11,
    "image_path": "examples/example_working/images/image_11.jpg",
    "caption": [],
    "footnote": [],
    "context": "Tversky index (TI), which can be thought as the approximation of the $F _ { \\beta }$ score, extends dice coefficient to a more general case. Given two sets $A$ and $B$ , tversky index is computed as follows:  Tversky index offers the flexibility in controlling the tradeoff between false-negatives and falsepositives. It degenerates to DSC if $\\alpha = \\beta = 0 . 5$ . The Tversky loss (TL) is thus given as follows: ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb",
    "description": "The image displays a mathematical formula for the Tversky Index (TI), which is used to measure the similarity between two sets, A and B. The formula is presented in a clean, centered LaTeX-style typeset format. It reads: \n\nTI = \\frac{|A \\cap B|}{|A \\cap B| + \\alpha |A \\setminus B| + \\beta |B \\setminus A|}\n\nThe numerator represents the size of the intersection of sets A and B, denoted as |A ∩ B|. The denominator includes the same intersection term plus weighted terms for the set differences: α times the size of A minus B (|A \\ B|), and β times the size of B minus A (|B \\ A|). The parameters α and β are weighting factors that allow control over the penalty for false positives and false negatives, respectively. When α = β = 0.5, the Tversky Index reduces to the Dice Similarity Coefficient (DSC). This formula is commonly used in machine learning, particularly in segmentation tasks, where it serves as a loss function or evaluation metric. The context explains that this index generalizes the Dice coefficient and offers flexibility in balancing false positives and false negatives, making it suitable for scenarios requiring nuanced similarity measures.",
    "segmentation": false
  },
  "image_12": {
    "image_id": 12,
    "image_path": "examples/example_working/images/image_12.jpg",
    "caption": [],
    "footnote": [],
    "context": "Tversky index offers the flexibility in controlling the tradeoff between false-negatives and falsepositives. It degenerates to DSC if $\\alpha = \\beta = 0 . 5$ . The Tversky loss (TL) is thus given as follows:  3.4 Self-adjusting Dice Loss  Consider a simple case where the dataset consists of only one example $x _ { i }$ , which is classified as positive as long as $p _ { i 1 }$ is larger than 0.5. The computation of $F 1$ score is actually as follows: ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb",
    "description": "The image displays a mathematical formula representing the Tversky Loss (TL), which is a loss function used in machine learning, particularly in segmentation tasks. The formula is written in LaTeX-style mathematical notation and reads: TL = \\frac{1}{N} \\sum_{i} \\left[ 1 - \\frac{p_{i1} y_{i1} + \\gamma}{p_{i1} y_{i1} + \\alpha \\, p_{i1} y_{i0} + \\beta \\, p_{i0} y_{i1} + \\gamma} \\right]. In this equation, TL denotes the Tversky Loss, N is the total number of samples, and the summation is over individual samples i. The terms p_{i1} and p_{i0} represent the predicted probabilities for class 1 and class 0, respectively, for sample i. Similarly, y_{i1} and y_{i0} are the true labels for class 1 and class 0, respectively. The parameters \\alpha and \\beta control the trade-off between false positives and false negatives, with \\gamma being a small constant added to avoid division by zero. When \\alpha = \\beta = 0.5, the Tversky loss reduces to the Dice Similarity Coefficient (DSC). The context indicates that this loss function is part of a discussion on self-adjusting Dice loss, where the behavior of the model under specific conditions (e.g., binary classification) is analyzed. The formula is presented in a clean, black-on-white format typical of academic or technical documents.",
    "segmentation": false
  },
  "image_13": {
    "image_id": 13,
    "image_path": "examples/example_working/images/image_13.jpg",
    "caption": [],
    "footnote": [],
    "context": "Consider a simple case where the dataset consists of only one example $x _ { i }$ , which is classified as positive as long as $p _ { i 1 }$ is larger than 0.5. The computation of $F 1$ score is actually as follows:   Comparing Eq.5 with Eq.11, we can see that Eq.5 is actually a soft form of $F 1$ , using a continuous $p$ rather than the binary $\\mathbb { I } ( p _ { i 1 } > 0 . 5 )$ . This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easy-negative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hardnegative examples and positive ones, which has a huge negative effect on the final F1 performance. ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb",
    "description": "The image displays a mathematical formula representing a modified version of the F1 score, specifically tailored for binary classification tasks involving probabilistic predictions. The formula is written as: \n\nF1(x_i) = 2 \\frac{\\mathbb{I}(p_{i1} > 0.5)y_{i1}}{\\mathbb{I}(p_{i1} > 0.5) + y_{i1}}\n\nHere, x_i denotes an input example, p_{i1} represents the predicted probability that the example belongs to class 1 (positive class), and y_{i1} is the true label (binary: 0 or 1). The indicator function \\mathbb{I}(p_{i1} > 0.5) evaluates to 1 if the predicted probability exceeds 0.5, and 0 otherwise. This formulation computes the F1 score based on hard decisions derived from thresholding probabilities at 0.5, rather than using continuous probabilities directly. The context explains that this form contrasts with a 'soft' F1 variant (Eq.5) which uses continuous probabilities, and highlights the potential issue when training data contains many easy-negative examples—these can dominate training due to their low prediction probabilities being easily pushed to zero, while hard-negative examples remain indistinguishable from positives, degrading final F1 performance. The equation is presented in standard LaTeX-style mathematical notation, centered and clearly formatted, with no additional visual elements such as graphs, tables, or diagrams.",
    "segmentation": false
  },
  "image_14": {
    "image_id": 14,
    "image_path": "examples/example_working/images/image_14.jpg",
    "caption": [],
    "footnote": [],
    "context": "$$\n\\operatorname { F 1 } ( x _ { i } ) = 2 { \\frac { \\mathbb { I } ( p _ { i 1 } > 0 . 5 ) y _ { i 1 } } { \\mathbb { I } ( p _ { i 1 } > 0 . 5 ) + y _ { i 1 } } }\n$$ Comparing Eq.5 with Eq.11, we can see that Eq.5 is actually a soft form of $F 1$ , using a continuous $p$ rather than the binary $\\mathbb { I } ( p _ { i 1 } > 0 . 5 )$ . This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easy-negative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hardnegative examples and positive ones, which has a huge negative effect on the final F1 performance. ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb",
    "description": "The image is a line graph that plots the derivatives of four different loss functions with respect to the probability of the ground-truth label, denoted as \\( \\bar{p}_i \\), which ranges from 0 to 1 on the x-axis. The y-axis represents the 'Derivatives' and spans from -2 to 2. Four distinct curves are shown, each corresponding to a different loss function, as indicated in the legend: blue for ∇ FL(γ=1) (Focal Loss with γ=1), orange for ∇ DL(γ=1) (Dice Loss with γ=1), yellow for ∇ TL(β=0.5) (Tversky Loss with β=0.5), and purple for ∇ DSC (Dice Similarity Coefficient).\n\n- The blue curve (∇ FL) starts at approximately -2 when \\( \\bar{p}_i = 0 \\), remains flat near this value until around \\( \\bar{p}_i = 0.5 \\), then sharply increases to reach about 0.5 at \\( \\bar{p}_i = 1 \\).\n- The orange curve (∇ DL) begins at -1 when \\( \\bar{p}_i = 0 \\), gradually rises, crosses zero around \\( \\bar{p}_i = 0.7 \\), and reaches approximately 0.5 at \\( \\bar{p}_i = 1 \\).\n- The yellow curve (∇ TL) starts at -2 when \\( \\bar{p}_i = 0 \\), increases slowly, crosses zero near \\( \\bar{p}_i = 0.6 \\), and reaches about 0.4 at \\( \\bar{p}_i = 1 \\).\n- The purple curve (∇ DSC) begins at -1 when \\( \\bar{p}_i = 0 \\), steadily increases, crosses zero around \\( \\bar{p}_i = 0.6 \\), and reaches approximately 1 at \\( \\bar{p}_i = 1 \\).\n\nAll curves exhibit smooth, continuous behavior, reflecting the gradient responses of their respective loss functions across varying confidence levels of the predicted probabilities. The graph highlights how each loss function responds differently to prediction confidence, particularly emphasizing the sensitivity of Focal Loss (blue) to low-confidence predictions and its sharp increase in derivative after \\( \\bar{p}_i = 0.5 \\). This visual comparison aids in understanding the optimization dynamics of these losses during training, especially in imbalanced datasets where easy-negative examples may dominate.",
    "segmentation": false
  },
  "image_15": {
    "image_id": 15,
    "image_path": "examples/example_working/images/image_15.jpg",
    "caption": [],
    "footnote": [],
    "context": "To address this issue, we propose to multiply the soft probability $p$ with a decaying factor $( 1 - p )$ , changing Eq.11 to the following adaptive variant of DSC:  One can think $\\left( 1 - p _ { i 1 } \\right)$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p _ { i 1 }$ to $( 1 - p _ { i 1 } ) p _ { i 1 }$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $( 1 - p _ { i 1 } ) p _ { i 1 }$ makes the model attach significantly less focus to them. ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb",
    "description": "The image displays a mathematical equation representing an adaptive variant of the Dice Similarity Coefficient (DSC), used in machine learning, particularly in tasks involving segmentation or classification with probabilistic outputs. The equation is written as: DSC(x_i) = \\frac{2(1 - p_{i1})p_{i1} \\cdot y_{i1} + \\gamma}{(1 - p_{i1})p_{i1} + y_{i1} + \\gamma}. Here, x_i represents an input example, p_{i1} is the predicted probability for class 1 (or positive class), y_{i1} is the corresponding ground truth label (typically binary: 0 or 1), and \\gamma is a small smoothing constant to avoid division by zero. The term (1 - p_{i1})p_{i1} acts as a dynamic weighting factor that diminishes the influence of examples where the prediction is confident (i.e., when p_{i1} approaches 0 or 1), thereby focusing more on hard-to-classify examples during training. This modification aims to improve model performance by adaptively adjusting the loss contribution based on prediction confidence. The equation is presented in standard mathematical notation using LaTeX-style formatting, with clear superscripts, subscripts, fractions, and symbols. The context provided explains that this formulation replaces a previous version (Eq.11) to make the DSC more adaptive by incorporating a decaying factor, emphasizing its use in training models where balancing focus between easy and difficult examples is crucial.",
    "segmentation": false
  },
  "image_16": {
    "image_id": 16,
    "image_path": "examples/example_working/images/image_16.jpg",
    "caption": [],
    "footnote": [
      "Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4. "
    ],
    "context": "A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $( 1 - p ) ^ { \\gamma }$ factor, leading the final loss to be $- ( 1 - p ) ^ { \\gamma } \\log p$ .    4.1 Part-of-Speech Tagging  In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible. ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb",
    "description": "The image is a table labeled 'Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4.' The table compares the performance of various models on three Chinese part-of-speech (POS) tagging datasets: CTB5, CTB6, and UD1.4. Each dataset has three evaluation metrics: Precision (Prec.), Recall (Rec.), and F1-score (F1). The rows represent different models, with their respective citations provided in parentheses. The models listed are: Joint-POS(Sig)(Shao et al., 2017), Joint-POS(Ens)(Shao et al., 2017), Lattice-LSTM(Zhang and Yang, 2018), BERT-Tagger(Devlin et al., 2018), BERT+FL, BERT+DL, and BERT+DSC. For each model, the table reports the Prec., Rec., and F1 values across the three datasets. Notably, some entries are marked with a dash (-), indicating missing or unavailable data. The BERT-based models show incremental improvements over baseline models. Specifically, BERT+FL improves upon BERT-Tagger by +0.70 in F1 on CTB5, +0.67 on CTB6, and +2.02 on UD1.4. BERT+DL shows further gains: +1.75 on CTB5, +0.32 on CTB6, and +2.15 on UD1.4. The best-performing model, BERT+DSC, achieves an F1 score of 97.92 on CTB5 (+1.86 improvement), 96.57 on CTB6 (+1.80), and 96.98 on UD1.4 (+2.19). These values are highlighted in bold to emphasize their superiority. The table uses consistent formatting with clear column headers and aligned numerical values. The overall layout is clean and structured, facilitating comparison across models and datasets.",
    "segmentation": false
  },
  "image_17": {
    "image_id": 17,
    "image_path": "examples/example_working/images/image_17.jpg",
    "caption": [
      "Table 4: Experimental results for English POS datasets. "
    ],
    "footnote": [],
    "context": "A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $( 1 - p ) ^ { \\gamma }$ factor, leading the final loss to be $- ( 1 - p ) ^ { \\gamma } \\log p$ .    4.1 Part-of-Speech Tagging  In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible. ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-9b0d43ab3776b1b7dc9bc607fe6b98fb",
    "description": "The image is a table labeled 'Table 4: Experimental results for English POS datasets,' which presents performance metrics for various models on two distinct English part-of-speech (POS) tagging datasets: English WSJ and English Tweets. The table is divided into two main sections, each corresponding to one dataset.\n\nIn the first section, titled 'English WSJ,' the rows list different models, including Meta BiLSTM (Bohnet et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-Tagger+FL, BERT-Tagger+DL, and BERT-Tagger+DSC. For each model, three columns report the evaluation metrics: Precision (Prec.), Recall (Rec.), and F1 score. The values are as follows:\n- Meta BiLSTM: Prec. = -, Rec. = -, F1 = 98.23\n- BERT-Tagger: Prec. = 99.21, Rec. = 98.36, F1 = 98.86\n- BERT-Tagger+FL: Prec. = 98.36, Rec. = 98.97, F1 = 98.88 (+0.02)\n- BERT-Tagger+DL: Prec. = 99.34, Rec. = 98.22, F1 = 98.91 (+0.05)\n- BERT-Tagger+DSC: Prec. = 99.41, Rec. = 98.93, F1 = 99.38 (+0.52)\n\nThe second section, titled 'English Tweets,' includes models such as FastText+CNN+CRF (Godin, 2019), BERT-Tagger (Devlin et al., 2018), BERT-Tagger+FL, BERT-Tagger+DL, and BERT-Tagger+DSC. The same three metrics are reported:\n- FastText+CNN+CRF: Prec. = -, Rec. = -, F1 = 91.78\n- BERT-Tagger: Prec. = 92.33, Rec. = 91.98, F1 = 92.34\n- BERT-Tagger+FL: Prec. = 91.24, Rec. = 93.22, F1 = 92.47 (+0.13)\n- BERT-Tagger+DL: Prec. = 91.44, Rec. = 92.88, F1 = 92.52 (+0.18)\n- BERT-Tagger+DSC: Prec. = 92.87, Rec. = 93.54, F1 = 92.58 (+0.24)\n\nThe table uses bold formatting to highlight the highest F1 scores in each dataset section. Notably, BERT-Tagger+DSC achieves the highest F1 scores in both datasets: 99.38 on English WSJ and 92.58 on English Tweets. Incremental improvements over the base BERT-Tagger are indicated in parentheses below the F1 scores. The context suggests that these models incorporate different loss functions—focal loss (FL), dice loss (DL), and DSC (Dice Soft Cross-Entropy)—to improve training efficiency and performance, with DSC showing the most significant gains, particularly on the English WSJ dataset.",
    "segmentation": false
  },
  "image_18": {
    "image_id": 18,
    "image_path": "examples/example_working/images/image_18.jpg",
    "caption": [
      "Table 5: Experimental results for NER task. "
    ],
    "footnote": [],
    "context": "Results Table 3 presents the experimental results on Chinese datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+ 1 . 8 6$ in terms of F1 score on CTB5, $+ 1 . 8 0$ on CTB6 and $+ 2 . 1 9$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue.  Table 4 presents the experimental results for English datasets.   4.2 Named Entity Recognition  Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. ",
    "chunk_order_index": 6,
    "chunk_id": "chunk-a2add12ad84f03be40432d1e511e52e2",
    "description": "The image is a structured table labeled 'Table 5: Experimental results for NER task,' presenting comparative performance metrics of various named entity recognition (NER) models across four different datasets: English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA, and Chinese OntoNotes 4.0. The table is divided into four main sections, each corresponding to one dataset. Within each section, the rows list different models, and the columns report their performance in terms of Precision (Prec.), Recall (Rec.), and F1 score. The models include ELMo (Peters et al., 2018), CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), and its variants with additional loss functions: BERT-MRC+FL (Focal Loss), BERT-MRC+DL (Dice Loss), and BERT-MRC+DSC (Dice-Similarity Coefficient Loss). For each model, the exact numerical values are provided. Notably, in the English CoNLL 2003 dataset, BERT-MRC achieves an F1 score of 93.04, while BERT-MRC+DSC improves it to 93.33 (+0.29). In English OntoNotes 5.0, BERT-MRC+DSC reaches an F1 of 92.07 (+0.96) compared to the base BERT-MRC. On Chinese MSRA, BERT-MRC+DSC achieves 96.72 (+0.97), and on Chinese OntoNotes 4.0, it scores 84.47 (+2.36), significantly outperforming the baseline BERT-MRC. The table highlights that the DSC loss consistently provides substantial gains over other losses like FL and DL, especially in Chinese datasets. The context emphasizes that these results demonstrate state-of-the-art (SOTA) performance, particularly due to the effectiveness of the DSC loss in handling data imbalance issues.",
    "segmentation": false
  },
  "image_19": {
    "image_id": 19,
    "image_path": "examples/example_working/images/image_19.jpg",
    "caption": [
      "Table 6: Experimental results for MRC task. "
    ],
    "footnote": [],
    "context": "• QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction.   \n• XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that    enables learning bidirectional contexts.  Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+ 1 . 2 5$ in terms of F1 score and $+ 0 . 8 4$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+ 1 . 4 6$ on EM and $+ 1 . 4 1$ on F1. ",
    "chunk_order_index": 8,
    "chunk_id": "chunk-cebc63594f08dbcbe32a7d19f944ef73",
    "description": "Image description generation timed out.",
    "segmentation": false
  },
  "image_20": {
    "image_id": 20,
    "image_path": "examples/example_working/images/image_20.jpg",
    "caption": [
      "Table 7: Experimental results for PI task. "
    ],
    "footnote": [],
    "context": "• QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction.   \n• XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that    enables learning bidirectional contexts.  Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+ 1 . 2 5$ in terms of F1 score and $+ 0 . 8 4$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+ 1 . 4 6$ on EM and $+ 1 . 4 1$ on F1. ",
    "chunk_order_index": 8,
    "chunk_id": "chunk-cebc63594f08dbcbe32a7d19f944ef73",
    "description": "The image is a table labeled 'Table 7: Experimental results for PI task.' It presents performance comparisons of various models on two natural language understanding benchmarks: MRPC (Microsoft Research Paraphrase Corpus) and QQP (Quora Question Pairs). The table has three columns: 'Model', 'MRPC F1', and 'QQP F1'. Each row corresponds to a model configuration, with the base models being BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019), followed by their variants enhanced with different techniques: FL (Focal Loss), DL (Denoising Loss), and DSC (Dual Soft Constraint loss).\n\nThe first section lists BERT-based models. BERT achieves an F1 score of 88.0 on MRPC and 91.3 on QQP. Adding FL improves the scores to 88.43 (+0.43) and 91.86 (+0.56), respectively. BERT+DL yields 88.71 (+0.71) and 91.92 (+0.62). The best-performing BERT variant is BERT+DSC, achieving 88.92 (+0.92) on MRPC and 92.11 (+0.81) on QQP.\n\nThe second section presents XLNet-based models. XLNet alone achieves 89.2 on MRPC and 91.8 on QQP. With FL, it improves to 89.25 (+0.05) and 92.31 (+0.51). XLNet+DL reaches 89.33 (+0.13) and 92.39 (+0.59). The top-performing model in this group is XLNet+DSC, scoring 89.78 (+0.58) on MRPC and 92.60 (+0.79) on QQP.\n\nAll values are presented in black text on a white background, with clear horizontal lines separating rows. The table uses bold formatting for the highest scores in each column (BERT+DSC and XLNet+DSC). The improvement values in parentheses indicate absolute gains over the base model. The overall trend shows that the DSC method consistently provides the largest performance boost across both datasets and base models, suggesting its effectiveness in enhancing model performance on paraphrase identification tasks.",
    "segmentation": false
  },
  "image_21": {
    "image_id": 21,
    "image_path": "examples/example_working/images/image_21.jpg",
    "caption": [
      "Table 8: The effect of different data augmentation ways for QQP in terms of F1-score. "
    ],
    "footnote": [],
    "context": "• Negative augmentation ( $\\cdot +$ negative) We created a more imbalanced dataset. The size of the newly constructed training set and the data augmented technique are exactly the same as +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21 \\%$ being positive and $79 \\%$ being negative.  1https://github.com/explosion/spaCy    • Negative downsampling (- negative)  We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50 \\%$ being positive and $50 \\%$ being negative. ",
    "chunk_order_index": 10,
    "chunk_id": "chunk-a3a8758b1de02b56fa79cb146de63691",
    "description": "The image is a table labeled 'Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.' It presents comparative performance results (F1-scores) of various models under different data augmentation strategies. The table has five columns: 'original', '+ positive', '+ negative', '- negative', and '+ positive & negative'. There are four rows corresponding to different model configurations: BERT, BERT+FL, BERT+DL, and BERT+DSC. Each cell contains an F1-score value, with some entries including a parenthetical value indicating the change relative to the original score.\n\nThe values are as follows:\n- For BERT: original = 91.3; + positive = 92.27; + negative = 90.08; - negative = 89.73; + positive & negative = 93.14.\n- For BERT+FL: original = 91.86 (+0.56); + positive = 92.64 (+0.37); + negative = 90.61 (+0.53); - negative = 90.79 (+1.06); + positive & negative = 93.45 (+0.31).\n- For BERT+DL: original = 91.92 (+0.62); + positive = 92.87 (+0.60); + negative = 90.22 (+0.14); - negative = 90.49 (+0.76); + positive & negative = 93.52 (+0.38).\n- For BERT+DSC: original = 92.11 (+0.81); + positive = 92.92 (+0.65); + negative = 90.78 (+0.70); - negative = 90.80 (+1.07); + positive & negative = 93.63 (+0.49).\n\nThe table shows that adding both positive and negative augmentations ('+ positive & negative') consistently yields the highest F1-scores across all models, with improvements ranging from +0.31 to +0.49 compared to their respective original baselines. In contrast, removing negative examples ('- negative') leads to lower performance than the original, especially for BERT (89.73). Adding only negative examples ('+ negative') generally decreases performance compared to the original, while adding only positive examples ('+ positive') improves scores slightly. The context explains that '+ negative' creates an imbalanced dataset with 21% positive and 79% negative examples, while '- negative' down-samples negative examples to achieve a balanced 50-50 split.",
    "segmentation": false
  },
  "image_22": {
    "image_id": 22,
    "image_path": "examples/example_working/images/image_22.jpg",
    "caption": [],
    "footnote": [
      "Table 9: The effect of DL and DSC on sentiment classification tasks. BERT $\\mathrm { + C E }$ refers to fine-tuning BERT and setting cross-entropy as the training objective. "
    ],
    "context": "We argue that the cross-entropy objective is actually accuracy-oriented, whereas the proposed losses perform as a soft version of F1 score. To explore the effect of the dice loss on accuracyoriented tasks such as text classification, we conduct experiments on the Stanford Sentiment Treebank (SST) datasets including SST-2 and SST-5. We fine-tuned $\\mathrm { B E R T _ { L a r g e } }$ with different training objectives. Experimental results for SST are shown in Table 9. For SST-5, BERT with CE achieves 55.57 in terms of accuracy, while DL and DSC perform slightly worse (54.63 and 55.19, respectively). Similar phenomenon is observed for SST-2. These results verify that the proposed dice loss is not accuracy-oriented, and should not be used for accuracy-oriented tasks.   5.3 Hyper-parameters in Tversky Index  As mentioned in Section 3.3, Tversky index (TI) offers the flexibility in controlling the tradeoff between false-negatives and false-positives. In this subsection, we explore the effect of hyperparameters (i.e., $\\alpha$ and $\\beta$ ) in TI to test how they manipulate the tradeoff. We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset. Experimental results are shown in Table 10. The highest F1 on Chinese OntoNotes4.0 is 84.67 when $\\alpha$ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\\alpha$ is set to 0.4. In addition, we can observe that the performance varies a lot as $\\alpha$ changes in distinct datasets, which shows that the hyperparameters $\\alpha , \\beta$ acturally play an important role in TI. ",
    "chunk_order_index": 10,
    "chunk_id": "chunk-a3a8758b1de02b56fa79cb146de63691",
    "description": "The image is a table labeled 'Table 9: The effect of DL and DSC on sentiment classification tasks.' It presents experimental results comparing the performance of different training objectives when fine-tuning BERT on two versions of the Stanford Sentiment Treebank (SST) datasets: SST-2 and SST-5. The table has three columns: 'Model', 'SST-2 Acc', and 'SST-5 Acc'. The rows list three models: 'BERT+CE', 'BERT+DL', and 'BERT+DSC'. For SST-2, the accuracy values are as follows: BERT+CE achieves 94.90, BERT+DL achieves 94.37, and BERT+DSC achieves 94.84. For SST-5, the accuracy values are: BERT+CE achieves 55.57, BERT+DL achieves 54.63, and BERT+DSC achieves 55.19. The highest accuracy in both SST-2 and SST-5 is achieved by BERT+CE, with 94.90 and 55.57 respectively. The other two models (BERT+DL and BERT+DSC) show slightly lower performance, indicating that the proposed dice-based losses (DL and DSC) perform worse than the cross-entropy (CE) objective in accuracy-oriented tasks. This supports the claim in the context that dice loss is not accuracy-oriented and should be avoided for tasks where accuracy is the primary metric. The table uses bold formatting to emphasize the best-performing value in each column (94.90 and 55.57).",
    "segmentation": false
  },
  "image_23": {
    "image_id": 23,
    "image_path": "examples/example_working/images/image_23.jpg",
    "caption": [],
    "footnote": [],
    "context": "In this paper, we propose the dice-based loss to narrow down the gap between training objective and evaluation metrics (F1 score). Experimental results show that the proposed loss function help   Table 10: The effect of hyperparameters in Tversky Index. We set $\\beta = 1 - \\alpha$ and thus we only list $\\alpha$ here. ",
    "chunk_order_index": 11,
    "chunk_id": "chunk-190fb76ec24199346ebe1de20cc27c3d",
    "description": "The image is a table labeled 'Table 10: The effect of hyperparameters in Tversky Index.' It presents numerical results comparing the performance of two datasets—'Chinese Onto4.0' and 'English QuoRef'—under varying values of the hyperparameter α, where β is set to 1 − α. The table has three columns: the first column lists values of α ranging from 0.1 to 0.9 in increments of 0.1. The second column shows corresponding performance scores for 'Chinese Onto4.0,' and the third column displays scores for 'English QuoRef.' The exact values are as follows:\n\n- For α = 0.1: Chinese Onto4.0 = 80.13, English QuoRef = 63.23\n- For α = 0.2: Chinese Onto4.0 = 81.17, English QuoRef = 63.45\n- For α = 0.3: Chinese Onto4.0 = 84.22, English QuoRef = 65.88\n- For α = 0.4: Chinese Onto4.0 = 84.52, English QuoRef = 68.44 (highlighted in bold)\n- For α = 0.5: Chinese Onto4.0 = 84.47, English QuoRef = 67.52\n- For α = 0.6: Chinese Onto4.0 = 84.67 (highlighted in bold), English QuoRef = 66.35\n- For α = 0.7: Chinese Onto4.0 = 81.81, English QuoRef = 65.09\n- For α = 0.8: Chinese Onto4.0 = 80.97, English QuoRef = 64.13\n- For α = 0.9: Chinese Onto4.0 = 80.21, English QuoRef = 64.84\n\nThe highest score for Chinese Onto4.0 is 84.67 at α = 0.6, while the highest score for English QuoRef is 68.44 at α = 0.4. Both peak values are emphasized with bold formatting. The trend indicates that performance improves for both datasets up to α = 0.4–0.6 before declining. The context suggests this table evaluates the impact of the Tversky Index hyperparameter α on model performance, particularly in relation to a dice-based loss function designed to align training objectives with F1-score evaluation metrics.",
    "segmentation": false
  }
}