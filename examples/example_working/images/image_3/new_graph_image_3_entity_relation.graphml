<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d6" for="edge" attr.name="order" attr.type="long"/>
<key id="d5" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d4" for="edge" attr.name="description" attr.type="string"/>
<key id="d3" for="edge" attr.name="weight" attr.type="double"/>
<key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;IMAGE_3&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image displays a mathematical equation labeled as 'CE' (Cross-Entropy), which is commonly used in machine learning for classification tasks, particularly in the context of multi-class or binary classification with probabilistic outputs. The equation is written in LaTeX-style mathematical notation and reads: \[ \mathrm{CE} = -\frac{1}{N} \sum_{i} \alpha_i \sum_{j \in \{0,1\}} y_{ij} \log p_{ij} \]This formula represents a weighted cross-entropy loss function. Here, \( N \) denotes the total number of samples in the dataset. The outer summation is over individual samples \( i \), and each sample contributes to the loss scaled by a weight factor \( \alpha_i \), where \( \alpha_i \in [0,1] \). This weighting allows for adjusting the importance of different samples, typically to address class imbalance. The inner summation is over the possible class labels \( j \in \{0,1\} \), indicating a binary classification setting. For each sample \( i \) and class \( j \), \( y_{ij} \) is the true label (usually 0 or 1 in one-hot encoding), and \( p_{ij} \) is the predicted probability assigned by the model to class \( j \) for sample \( i \). The logarithmic term \( \log p_{ij} \) penalizes low-confidence correct predictions more heavily. The negative sign ensures that minimizing the CE loss corresponds to maximizing the likelihood of correct predictions. The context provided explains that this formulation allows for unequal treatment of samples via \( \alpha_i \), which can be set based on inverse class frequency or tuned via hyperparameters such as \( K \) in the expression \( \log \left( \frac{n - n_t}{n_t} + K \right) \), where \( n_t \) is the count of samples in class \( t \) and \( n \) is the total number of training samples. This adjustment reduces the influence of majority classes and increases that of minority classes, helping mitigate bias in imbalanced datasets."</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Cross-Entropy Loss (CE)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">Cross-entropy loss (CE) is a standard objective function used in classification tasks, particularly in data-imbalanced NLP problems. It is defined as $\mathrm{CE} = -\frac{1}{N} \sum_i \sum_{j \in \{0, 1\}} y_{ij} \log p_{ij}$, where each sample contributes equally to the total loss. However, CE fails to address data imbalance issues such as training-test discrepancy and the overwhelming influence of easy-negative examples. Weighted variants (WCE) introduce sample-specific coefficients $\alpha_i$ to mitigate class imbalance, often based on inverse class frequency.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Total Number of Samples (N)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">N represents the total number of samples in the dataset and serves as a normalization factor in the cross-entropy loss formula. It ensures that the average loss per sample is computed, making the objective function scale-invariant to dataset size.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Sample Weight Coefficient (α_i)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The weight coefficient $\alpha_i$ is associated with the i-th sample and modulates its contribution to the overall loss in weighted cross-entropy formulations. It is often calculated using the inverse frequency of the sample’s class (e.g., $\log(\frac{n - n_t}{n_t} + K)$, where $n_t$ is the count of class $t$, $n$ is the total number of samples, and $K$ is a tunable hyperparameter). This weighting scheme assigns higher importance to minority-class samples to counteract data imbalance.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;True Label Indicator (y_ij)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The true label value $y_{ij}$ for the i-th sample and j-th class is a binary indicator ($\in \{0,1\}$) used in binary or one-hot encoded classification settings. In binary classification, it denotes whether sample $i$ belongs to class $j$, and only one of $y_{i0}$ or $y_{i1}$ equals 1 while the other is 0.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Predicted Class Probability (p_ij)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The predicted probability $p_{ij}$ represents the model's estimated likelihood that the i-th sample belongs to the j-th class. These probabilities are typically derived from a softmax or sigmoid output layer and satisfy $p_{i0} + p_{i1} = 1$ in binary classification. They are central to loss computation in both cross-entropy and Dice-based objectives.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Natural Logarithm Function (log)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The natural logarithm function $\log$ is applied to predicted probabilities $p_{ij}$ in the cross-entropy loss to penalize incorrect predictions more severely. As $p_{ij}$ approaches 0 for a true positive class, $\log p_{ij}$ tends toward negative infinity, resulting in a large penalty. This property makes cross-entropy sensitive to confident but wrong predictions.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Summation Operator (Σ)&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The summation symbol $\Sigma$ aggregates contributions across all samples ($i$) and classes ($j \in \{0,1\}$) in the cross-entropy loss computation. It ensures that the total loss reflects performance over the entire dataset and both classes in binary classification.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="&quot;Binary Class Index Set (j ∈ {0,1})&quot;">
  <data key="d0">"OBJECT"</data>
  <data key="d1">The index set $j \in \{0,1\}$ specifies that the summation in the cross-entropy formula is performed over two mutually exclusive classes, characteristic of binary classification tasks. Each sample has exactly one true class, and the model outputs a probability distribution over these two classes.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<node id="Cross Entropy Loss">
  <data key="d0">IMG_ENTITY</data>
  <data key="d1">The cross-entropy loss (CE) is a mathematical objective function used in machine learning, particularly for binary classification tasks. The image displays the formula for a weighted version of CE, where each sample's contribution to the loss is scaled by a weight α_i, allowing for handling class imbalance. This formulation is central to training models in imbalanced datasets, as it adjusts the influence of majority and minority classes during optimization. The text elaborates on its limitations in addressing data imbalance and introduces alternative losses like Dice and Tversky indices to overcome these issues.</data>
  <data key="d2">examples/example_working/images/image_3.jpg</data>
</node>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Cross-Entropy Loss (CE)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CE是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Total Number of Samples (N)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"N是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Sample Weight Coefficient (α_i)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"α_i是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;True Label Indicator (y_ij)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"y_ij是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Predicted Class Probability (p_ij)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"p_ij是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Natural Logarithm Function (log)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"log是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Summation Operator (Σ)&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Σ是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="&quot;Binary Class Index Set (j ∈ {0,1})&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"j∈{0,1}是从image_3中提取的实体。"</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_3&quot;" target="Cross Entropy Loss">
  <data key="d3">10.0</data>
  <data key="d4">"IMAGE_3" is the image of Cross Entropy Loss.</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Total Number of Samples (N)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The cross-entropy loss is normalized by dividing by N, the total number of samples."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Sample Weight Coefficient (α_i)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"Each sample's contribution to the cross-entropy is weighted by α_i."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;True Label Indicator (y_ij)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The true labels y_ij determine the ground truth for computing the loss."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Predicted Class Probability (p_ij)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The predicted probabilities p_ij are used to compute the log-likelihood term in the loss."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Natural Logarithm Function (log)&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The logarithmic function is applied to p_ij to measure prediction confidence."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Summation Operator (Σ)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"Multiple summations are used to aggregate contributions across samples and classes."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Cross-Entropy Loss (CE)&quot;" target="&quot;Binary Class Index Set (j ∈ {0,1})&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The summation over j is constrained to binary classes, making it suitable for binary classification tasks."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Sample Weight Coefficient (α_i)&quot;" target="&quot;True Label Indicator (y_ij)&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The weight α_i scales the contribution of the true label y_ij for each sample."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;True Label Indicator (y_ij)&quot;" target="&quot;Predicted Class Probability (p_ij)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The true label y_ij and predicted probability p_ij are paired in the log term to compute per-sample loss."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;Predicted Class Probability (p_ij)&quot;" target="&quot;Natural Logarithm Function (log)&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The logarithm is applied directly to the predicted probability p_ij to evaluate prediction accuracy."</data>
  <data key="d5">examples/example_working/images/image_3.jpg</data>
  <data key="d6">1</data>
</edge>
</graph></graphml>